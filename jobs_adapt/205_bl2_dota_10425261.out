============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
[2025-03-10 19:50:53,373] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INIT slurm
| distributed init (rank 0): env://, gpu 0 | args.distributed=True
Distributed process initialized successfully.
	Rank 0, World Size: 1, Device: 0
Namespace(batch_size=56, epochs=50, update_freq=1, save_ckpt_freq=1, model='rein_vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.2, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, rein_link_token_to_query=False, rein_token_length=100, rein_fuse_method='simple_concat', loss='crossentropy', opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.01, layer_decay=0.6, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=1, aa='rand-m6-n3-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=1, test_num_crop=1, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='logs/pretrained/VideoMAE2_distill/videomae_vits_k710_distill_from_giant.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, final_reduction='fc_norm', freeze_layers=None, data_path='/gpfs/work3/0/tese0625/RiskNetData/DoTA_refined', eval_data_path=None, nb_classes=2, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=1, sampling_rate_val=2, view_fps=10, data_set='DoTA', output_dir='/home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3', log_dir='/home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3', device='cuda', seed=42, resume='', auto_resume=True, nb_samples_per_epoch=50000, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=12, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, eval_option='', deepspeed=False, deepspeed_config='/home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')


===
[train] | COUNT safe: 176952
COUNT risk: 108831
===
Number of the class = 2
Sampler_train = <utils.ShortDistributedSampler object at 0x14a3b4ced460>


===
[validation] | COUNT safe: 37774
COUNT risk: 23322
===
Number of the class = 2
dset lengths: train 285783, val 61096
==== Rein: link token: False, fuse method: simple_concat
Patch size = (16, 16)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/rein_run_frame_finetuning.py:406: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.finetune, map_location='cpu')
Load ckpt from logs/pretrained/VideoMAE2_distill/videomae_vits_k710_distill_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of ReinVisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias', 'reins.scale', 'reins.learnable_tokens_a', 'reins.learnable_tokens_b', 'reins.mlp_token2feat.weight', 'reins.mlp_token2feat.bias', 'reins.mlp_delta_f.weight', 'reins.mlp_delta_f.bias', 'reins.transform.weight', 'reins.transform.bias', 'reins.merge.weight', 'reins.merge.bias']
Model = ReinVisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.05454545468091965)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.072727270424366)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090908616781235)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10909091681241989)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.12727272510528564)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1454545557498932)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16363637149333954)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.1818181872367859)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.20000000298023224)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=2, bias=True)
  (reins): LoRAReins(
    (mlp_token2feat): Linear(in_features=384, out_features=384, bias=True)
    (mlp_delta_f): Linear(in_features=384, out_features=384, bias=True)
    (transform): Linear(in_features=384, out_features=384, bias=True)
    (merge): Linear(in_features=1152, out_features=384, bias=True)
  )
)
number of params: 22859907
LR = 0.00218750
Batch size = 56
Update frequent = 1
Number of training examples = 285783
Number of training training per epoch = 892
Assigned values = [0.0013060694015999993, 0.002176782335999999, 0.0036279705599999985, 0.006046617599999997, 0.010077695999999997, 0.016796159999999994, 0.027993599999999993, 0.04665599999999999, 0.07775999999999998, 0.1296, 0.21599999999999997, 0.36, 0.6, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.0013060694015999993
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.0013060694015999993
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.002176782335999999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.002176782335999999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.0036279705599999985
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.0036279705599999985
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.006046617599999997
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.006046617599999997
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.010077695999999997
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.010077695999999997
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.016796159999999994
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.016796159999999994
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.027993599999999993
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.027993599999999993
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.04665599999999999
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.04665599999999999
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.07775999999999998
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.07775999999999998
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.1296
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.1296
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.21599999999999997
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.21599999999999997
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.36
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.36
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.6
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.6
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias",
      "reins.mlp_token2feat.bias",
      "reins.mlp_delta_f.bias",
      "reins.transform.bias",
      "reins.merge.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight",
      "reins.scale",
      "reins.learnable_tokens_a",
      "reins.learnable_tokens_b",
      "reins.mlp_token2feat.weight",
      "reins.mlp_delta_f.weight",
      "reins.transform.weight",
      "reins.merge.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-03-10 19:51:47,030] [INFO] [logging.py:129:log_dist] [Rank -1] DeepSpeed info: version=0.15.3, git-hash=unknown, git-branch=unknown
[2025-03-10 19:51:47,030] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-03-10 19:51:47,030] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1
[2025-03-10 19:51:47,097] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /gpfs/home1/sorlova/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /gpfs/home1/sorlova/.cache/torch_extensions/py312_cu121/fused_adam/build.ninja...
/home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 3.5393710136413574 seconds
[2025-03-10 19:51:50,649] [INFO] [logging.py:129:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-03-10 19:51:50,649] [INFO] [logging.py:129:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 19:51:50,653] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 19:51:50,653] [INFO] [logging.py:129:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-03-10 19:51:50,678] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed Final Optimizer = FP16_Optimizer
[2025-03-10 19:51:50,678] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-03-10 19:51:50,678] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-03-10 19:51:50,678] [INFO] [logging.py:129:log_dist] [Rank 0] step=0, skipped=0, lr=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-10 19:51:50,679] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-03-10 19:51:50,679] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-03-10 19:51:50,679] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 19:51:50,679] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-03-10 19:51:50,679] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14a3b41d7980>
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   fp16_enabled ................. True
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-03-10 19:51:50,680] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 128
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   loss_scale ................... 0
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   optimizer_name ............... adam
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 0.01, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   steps_per_print .............. 1000
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   train_batch_size ............. 56
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  56
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   world_size ................... 1
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2025-03-10 19:51:50,681] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 19:51:50,682] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2025-03-10 19:51:50,682] [INFO] [config.py:989:print_user_config]   json = {
    "train_batch_size": 56, 
    "train_micro_batch_size_per_gpu": 56, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.01, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 4460
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = CrossEntropyLoss()
Start training for 50 epochs
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.18 GB
------------CPU Memory Usage: 48.53 GB / 503.51 GB
Epoch: [0]  [  0/893]  eta: 4:13:36  lr: 0.000000  min_lr: 0.000000  loss: 0.6934 (0.6934)  class_acc: 0.4821 (0.4821)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 17.0398  data: 11.8178  max mem: 31080
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 57.69 GB / 503.51 GB
Epoch: [0]  [ 10/893]  eta: 0:44:41  lr: 0.000005  min_lr: 0.000000  loss: 0.6929 (0.6930)  class_acc: 0.6071 (0.5812)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 3.0371  data: 1.0747  max mem: 31080
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 66.87 GB / 503.51 GB
Epoch: [0]  [ 20/893]  eta: 0:34:44  lr: 0.000010  min_lr: 0.000000  loss: 0.6929 (0.6927)  class_acc: 0.6250 (0.5961)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6556  data: 0.0004  max mem: 31080
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 77.17 GB / 503.51 GB
Epoch: [0]  [ 30/893]  eta: 0:31:04  lr: 0.000015  min_lr: 0.000000  loss: 0.6914 (0.6920)  class_acc: 0.6429 (0.6077)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6789  data: 0.0005  max mem: 31080
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.83 GB / 503.51 GB
Epoch: [0]  [ 40/893]  eta: 0:28:40  lr: 0.000020  min_lr: 0.000000  loss: 0.6899 (0.6910)  class_acc: 0.6250 (0.6128)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6282  data: 0.0004  max mem: 31080
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.01 GB / 503.51 GB
Epoch: [0]  [ 50/893]  eta: 0:26:44  lr: 0.000025  min_lr: 0.000000  loss: 0.6860 (0.6897)  class_acc: 0.6250 (0.6183)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5035  data: 0.0003  max mem: 31080
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.22 GB / 503.51 GB
Epoch: [0]  [ 60/893]  eta: 0:25:20  lr: 0.000029  min_lr: 0.000000  loss: 0.6846 (0.6888)  class_acc: 0.6071 (0.6148)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4310  data: 0.0002  max mem: 31080
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.79 GB / 503.51 GB
Epoch: [0]  [ 70/893]  eta: 0:24:17  lr: 0.000034  min_lr: 0.000000  loss: 0.6792 (0.6869)  class_acc: 0.6071 (0.6202)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4342  data: 0.0003  max mem: 31080
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.48 GB / 503.51 GB
Epoch: [0]  [ 80/893]  eta: 0:23:25  lr: 0.000039  min_lr: 0.000000  loss: 0.6777 (0.6858)  class_acc: 0.6071 (0.6182)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4342  data: 0.0003  max mem: 31080
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.87 GB / 503.51 GB
Epoch: [0]  [ 90/893]  eta: 0:22:42  lr: 0.000044  min_lr: 0.000000  loss: 0.6768 (0.6842)  class_acc: 0.6071 (0.6191)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4312  data: 0.0003  max mem: 31080
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.44 GB / 503.51 GB
Epoch: [0]  [100/893]  eta: 0:22:04  lr: 0.000049  min_lr: 0.000000  loss: 0.6724 (0.6834)  class_acc: 0.6071 (0.6170)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4343  data: 0.0003  max mem: 31080
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.99 GB / 503.51 GB
Epoch: [0]  [110/893]  eta: 0:21:30  lr: 0.000054  min_lr: 0.000000  loss: 0.6724 (0.6828)  class_acc: 0.6071 (0.6145)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4281  data: 0.0003  max mem: 31080
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.64 GB / 503.51 GB
Epoch: [0]  [120/893]  eta: 0:21:00  lr: 0.000059  min_lr: 0.000000  loss: 0.6714 (0.6814)  class_acc: 0.6071 (0.6157)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4314  data: 0.0003  max mem: 31080
[2025-03-10 19:55:19,690] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 19:55:19,691] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 128 to 256
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.79 GB / 503.51 GB
Epoch: [0]  [130/893]  eta: 0:20:33  lr: 0.000064  min_lr: 0.000000  loss: 0.6611 (0.6796)  class_acc: 0.6429 (0.6179)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 1.4386  data: 0.0004  max mem: 31080
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.25 GB / 503.51 GB
Epoch: [0]  [140/893]  eta: 0:20:06  lr: 0.000069  min_lr: 0.000000  loss: 0.6641 (0.6792)  class_acc: 0.6071 (0.6160)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 1.4335  data: 0.0003  max mem: 31080
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.35 GB / 503.51 GB
Epoch: [0]  [150/893]  eta: 0:19:42  lr: 0.000074  min_lr: 0.000000  loss: 0.6738 (0.6789)  class_acc: 0.5893 (0.6144)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 1.4280  data: 0.0003  max mem: 31080
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.84 GB / 503.51 GB
Epoch: [0]  [160/893]  eta: 0:19:18  lr: 0.000078  min_lr: 0.000000  loss: 0.6699 (0.6778)  class_acc: 0.6071 (0.6144)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 1.4268  data: 0.0003  max mem: 31080
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.61 GB / 503.51 GB
Epoch: [0]  [170/893]  eta: 0:18:56  lr: 0.000083  min_lr: 0.000000  loss: 0.6606 (0.6769)  class_acc: 0.6071 (0.6143)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 1.4312  data: 0.0003  max mem: 31080
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.73 GB / 503.51 GB
Epoch: [0]  [180/893]  eta: 0:18:35  lr: 0.000088  min_lr: 0.000000  loss: 0.6577 (0.6758)  class_acc: 0.6071 (0.6146)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 1.4337  data: 0.0003  max mem: 31080
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.07 GB / 503.51 GB
Epoch: [0]  [190/893]  eta: 0:18:15  lr: 0.000093  min_lr: 0.000000  loss: 0.6494 (0.6746)  class_acc: 0.6250 (0.6152)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 1.4323  data: 0.0003  max mem: 31080
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.29 GB / 503.51 GB
Epoch: [0]  [200/893]  eta: 0:17:54  lr: 0.000098  min_lr: 0.000000  loss: 0.6494 (0.6735)  class_acc: 0.6250 (0.6156)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 1.4286  data: 0.0003  max mem: 31080
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.10 GB / 503.51 GB
Epoch: [0]  [210/893]  eta: 0:17:35  lr: 0.000103  min_lr: 0.000000  loss: 0.6528 (0.6725)  class_acc: 0.6429 (0.6163)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 1.4261  data: 0.0003  max mem: 31080
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.53 GB / 503.51 GB
Epoch: [0]  [220/893]  eta: 0:17:16  lr: 0.000108  min_lr: 0.000000  loss: 0.6528 (0.6718)  class_acc: 0.6071 (0.6158)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 1.4283  data: 0.0003  max mem: 31080
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.84 GB / 503.51 GB
Epoch: [0]  [230/893]  eta: 0:16:57  lr: 0.000113  min_lr: 0.000000  loss: 0.6592 (0.6709)  class_acc: 0.6071 (0.6158)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 1.4317  data: 0.0003  max mem: 31080
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.20 GB / 503.51 GB
Epoch: [0]  [240/893]  eta: 0:16:39  lr: 0.000118  min_lr: 0.000000  loss: 0.6592 (0.6708)  class_acc: 0.5893 (0.6144)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 1.4328  data: 0.0003  max mem: 31080
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.89 GB / 503.51 GB
Epoch: [0]  [250/893]  eta: 0:16:21  lr: 0.000123  min_lr: 0.000000  loss: 0.6509 (0.6699)  class_acc: 0.6250 (0.6153)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 1.4305  data: 0.0002  max mem: 31080
[2025-03-10 19:58:22,778] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 19:58:22,779] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 256 to 512
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.73 GB / 503.51 GB
Epoch: [0]  [260/893]  eta: 0:16:04  lr: 0.000128  min_lr: 0.000000  loss: 0.6509 (0.6691)  class_acc: 0.6071 (0.6149)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 1.4310  data: 0.0003  max mem: 31080
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.22 GB / 503.51 GB
Epoch: [0]  [270/893]  eta: 0:15:46  lr: 0.000132  min_lr: 0.000000  loss: 0.6382 (0.6676)  class_acc: 0.6071 (0.6165)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 1.4307  data: 0.0003  max mem: 31080
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 79.26 GB / 503.51 GB
Epoch: [0]  [280/893]  eta: 0:15:29  lr: 0.000137  min_lr: 0.000000  loss: 0.6211 (0.6660)  class_acc: 0.6607 (0.6176)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 1.4288  data: 0.0003  max mem: 31080
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.93 GB / 503.51 GB
Epoch: [0]  [290/893]  eta: 0:15:12  lr: 0.000142  min_lr: 0.000000  loss: 0.6353 (0.6648)  class_acc: 0.6071 (0.6181)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 1.4308  data: 0.0004  max mem: 31080
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.82 GB / 503.51 GB
Epoch: [0]  [300/893]  eta: 0:14:56  lr: 0.000147  min_lr: 0.000000  loss: 0.6357 (0.6645)  class_acc: 0.6071 (0.6176)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 1.4330  data: 0.0003  max mem: 31080
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 80.65 GB / 503.51 GB
Epoch: [0]  [310/893]  eta: 0:14:39  lr: 0.000152  min_lr: 0.000000  loss: 0.6436 (0.6634)  class_acc: 0.6250 (0.6181)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 1.4317  data: 0.0003  max mem: 31080
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.45 GB / 503.51 GB
Epoch: [0]  [320/893]  eta: 0:14:22  lr: 0.000157  min_lr: 0.000000  loss: 0.6250 (0.6623)  class_acc: 0.6607 (0.6192)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 1.4271  data: 0.0003  max mem: 31080
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.05 GB / 503.51 GB
Epoch: [0]  [330/893]  eta: 0:14:06  lr: 0.000162  min_lr: 0.000000  loss: 0.6143 (0.6607)  class_acc: 0.6786 (0.6212)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 1.4266  data: 0.0004  max mem: 31080
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.04 GB / 503.51 GB
Epoch: [0]  [340/893]  eta: 0:13:50  lr: 0.000167  min_lr: 0.000000  loss: 0.6143 (0.6596)  class_acc: 0.6786 (0.6224)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 1.4263  data: 0.0004  max mem: 31080
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.45 GB / 503.51 GB
Epoch: [0]  [350/893]  eta: 0:13:34  lr: 0.000172  min_lr: 0.000000  loss: 0.6040 (0.6571)  class_acc: 0.6786 (0.6246)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 1.4278  data: 0.0003  max mem: 31080
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.05 GB / 503.51 GB
Epoch: [0]  [360/893]  eta: 0:13:18  lr: 0.000177  min_lr: 0.000000  loss: 0.5762 (0.6553)  class_acc: 0.6964 (0.6270)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 1.4319  data: 0.0004  max mem: 31080
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.13 GB / 503.51 GB
Epoch: [0]  [370/893]  eta: 0:13:03  lr: 0.000182  min_lr: 0.000000  loss: 0.5864 (0.6541)  class_acc: 0.6964 (0.6286)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 1.4890  data: 0.0004  max mem: 31080
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.32 GB / 503.51 GB
Epoch: [0]  [380/893]  eta: 0:12:47  lr: 0.000186  min_lr: 0.000000  loss: 0.6064 (0.6528)  class_acc: 0.6607 (0.6295)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 1.4791  data: 0.0002  max mem: 31080
[2025-03-10 20:01:26,795] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:01:26,795] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 512 to 1024
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.42 GB / 503.51 GB
Epoch: [0]  [390/893]  eta: 0:12:31  lr: 0.000191  min_lr: 0.000000  loss: 0.5903 (0.6511)  class_acc: 0.7143 (0.6317)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 1.4230  data: 0.0003  max mem: 31080
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.80 GB / 503.51 GB
Epoch: [0]  [400/893]  eta: 0:12:16  lr: 0.000196  min_lr: 0.000000  loss: 0.5879 (0.6497)  class_acc: 0.7143 (0.6332)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 1.4300  data: 0.0003  max mem: 31080
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.63 GB / 503.51 GB
Epoch: [0]  [410/893]  eta: 0:12:00  lr: 0.000201  min_lr: 0.000000  loss: 0.5762 (0.6477)  class_acc: 0.6964 (0.6352)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 1.4257  data: 0.0003  max mem: 31080
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.08 GB / 503.51 GB
Epoch: [0]  [420/893]  eta: 0:11:44  lr: 0.000206  min_lr: 0.000000  loss: 0.5742 (0.6460)  class_acc: 0.6964 (0.6363)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 1.4274  data: 0.0004  max mem: 31080
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.70 GB / 503.51 GB
Epoch: [0]  [430/893]  eta: 0:11:29  lr: 0.000211  min_lr: 0.000000  loss: 0.5757 (0.6443)  class_acc: 0.6786 (0.6378)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 1.4291  data: 0.0004  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.54 GB / 503.51 GB
Epoch: [0]  [440/893]  eta: 0:11:13  lr: 0.000216  min_lr: 0.000000  loss: 0.5688 (0.6422)  class_acc: 0.7143 (0.6397)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 1.4314  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 79.23 GB / 503.51 GB
Epoch: [0]  [450/893]  eta: 0:10:58  lr: 0.000221  min_lr: 0.000000  loss: 0.5640 (0.6407)  class_acc: 0.7143 (0.6410)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 1.4343  data: 0.0004  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.77 GB / 503.51 GB
Epoch: [0]  [460/893]  eta: 0:10:42  lr: 0.000226  min_lr: 0.000000  loss: 0.5757 (0.6392)  class_acc: 0.6786 (0.6420)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 1.4316  data: 0.0005  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.36 GB / 503.51 GB
Epoch: [0]  [470/893]  eta: 0:10:27  lr: 0.000231  min_lr: 0.000000  loss: 0.5752 (0.6372)  class_acc: 0.6964 (0.6437)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 1.4339  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.41 GB / 503.51 GB
Epoch: [0]  [480/893]  eta: 0:10:12  lr: 0.000235  min_lr: 0.000000  loss: 0.5049 (0.6348)  class_acc: 0.7500 (0.6463)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 1.4301  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.78 GB / 503.51 GB
Epoch: [0]  [490/893]  eta: 0:09:57  lr: 0.000240  min_lr: 0.000000  loss: 0.5049 (0.6331)  class_acc: 0.7500 (0.6477)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 1.4219  data: 0.0004  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 79.01 GB / 503.51 GB
Epoch: [0]  [500/893]  eta: 0:09:41  lr: 0.000245  min_lr: 0.000000  loss: 0.5381 (0.6310)  class_acc: 0.7143 (0.6494)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 1.4262  data: 0.0005  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.80 GB / 503.51 GB
Epoch: [0]  [510/893]  eta: 0:09:26  lr: 0.000250  min_lr: 0.000000  loss: 0.5327 (0.6287)  class_acc: 0.7321 (0.6514)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 1.4305  data: 0.0003  max mem: 31081
[2025-03-10 20:04:29,805] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:04:29,805] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 1024 to 2048
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.82 GB / 503.51 GB
Epoch: [0]  [520/893]  eta: 0:09:11  lr: 0.000255  min_lr: 0.000000  loss: 0.4905 (0.6261)  class_acc: 0.7500 (0.6537)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 1.4315  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.95 GB / 503.51 GB
Epoch: [0]  [530/893]  eta: 0:08:56  lr: 0.000260  min_lr: 0.000000  loss: 0.5044 (0.6248)  class_acc: 0.7500 (0.6549)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 1.4326  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.52 GB / 503.51 GB
Epoch: [0]  [540/893]  eta: 0:08:41  lr: 0.000265  min_lr: 0.000000  loss: 0.5210 (0.6224)  class_acc: 0.7679 (0.6571)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 1.4341  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.83 GB / 503.51 GB
Epoch: [0]  [550/893]  eta: 0:08:26  lr: 0.000270  min_lr: 0.000000  loss: 0.5000 (0.6211)  class_acc: 0.7500 (0.6580)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 1.4325  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.91 GB / 503.51 GB
Epoch: [0]  [560/893]  eta: 0:08:11  lr: 0.000275  min_lr: 0.000000  loss: 0.4951 (0.6189)  class_acc: 0.7321 (0.6596)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 1.4317  data: 0.0002  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 79.10 GB / 503.51 GB
Epoch: [0]  [570/893]  eta: 0:07:56  lr: 0.000280  min_lr: 0.000000  loss: 0.4951 (0.6172)  class_acc: 0.7500 (0.6612)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 1.4335  data: 0.0002  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.86 GB / 503.51 GB
Epoch: [0]  [580/893]  eta: 0:07:41  lr: 0.000285  min_lr: 0.000000  loss: 0.5244 (0.6155)  class_acc: 0.7500 (0.6629)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 1.4301  data: 0.0002  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.60 GB / 503.51 GB
Epoch: [0]  [590/893]  eta: 0:07:26  lr: 0.000289  min_lr: 0.000000  loss: 0.5244 (0.6140)  class_acc: 0.7500 (0.6642)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 1.4290  data: 0.0002  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.47 GB / 503.51 GB
Epoch: [0]  [600/893]  eta: 0:07:11  lr: 0.000294  min_lr: 0.000000  loss: 0.5327 (0.6125)  class_acc: 0.7500 (0.6657)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 1.4341  data: 0.0002  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.43 GB / 503.51 GB
Epoch: [0]  [610/893]  eta: 0:06:56  lr: 0.000299  min_lr: 0.000000  loss: 0.5400 (0.6111)  class_acc: 0.7321 (0.6669)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 1.4364  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.60 GB / 503.51 GB
Epoch: [0]  [620/893]  eta: 0:06:41  lr: 0.000304  min_lr: 0.000000  loss: 0.5645 (0.6100)  class_acc: 0.7143 (0.6680)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 1.4305  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.94 GB / 503.51 GB
Epoch: [0]  [630/893]  eta: 0:06:26  lr: 0.000309  min_lr: 0.000000  loss: 0.5571 (0.6083)  class_acc: 0.7500 (0.6695)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 1.4291  data: 0.0003  max mem: 31081
[2025-03-10 20:07:33,185] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:07:33,185] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 2048 to 4096
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 79.03 GB / 503.51 GB
Epoch: [0]  [640/893]  eta: 0:06:11  lr: 0.000314  min_lr: 0.000000  loss: 0.5259 (0.6072)  class_acc: 0.7679 (0.6705)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 1.4354  data: 0.0003  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 78.62 GB / 503.51 GB
Epoch: [0]  [650/893]  eta: 0:05:57  lr: 0.000319  min_lr: 0.000000  loss: 0.5176 (0.6058)  class_acc: 0.7500 (0.6717)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 1.4325  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 79.31 GB / 503.51 GB
Epoch: [0]  [660/893]  eta: 0:05:42  lr: 0.000324  min_lr: 0.000000  loss: 0.5215 (0.6045)  class_acc: 0.7500 (0.6727)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 1.4306  data: 0.0002  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 79.53 GB / 503.51 GB
Epoch: [0]  [670/893]  eta: 0:05:28  lr: 0.000329  min_lr: 0.000000  loss: 0.5239 (0.6032)  class_acc: 0.7500 (0.6737)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 1.5546  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.10 GB / 503.51 GB
Epoch: [0]  [680/893]  eta: 0:05:13  lr: 0.000334  min_lr: 0.000000  loss: 0.4910 (0.6020)  class_acc: 0.7679 (0.6746)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 1.5577  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.70 GB / 503.51 GB
Epoch: [0]  [690/893]  eta: 0:04:58  lr: 0.000339  min_lr: 0.000000  loss: 0.4910 (0.6004)  class_acc: 0.7679 (0.6761)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 1.4334  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.30 GB / 503.51 GB
Epoch: [0]  [700/893]  eta: 0:04:43  lr: 0.000343  min_lr: 0.000000  loss: 0.4893 (0.5987)  class_acc: 0.7857 (0.6778)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 1.4305  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.65 GB / 503.51 GB
Epoch: [0]  [710/893]  eta: 0:04:28  lr: 0.000348  min_lr: 0.000000  loss: 0.4766 (0.5969)  class_acc: 0.7857 (0.6794)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 1.4347  data: 0.0003  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.15 GB / 503.51 GB
Epoch: [0]  [720/893]  eta: 0:04:14  lr: 0.000353  min_lr: 0.000000  loss: 0.4702 (0.5954)  class_acc: 0.7857 (0.6806)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 1.4347  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.64 GB / 503.51 GB
Epoch: [0]  [730/893]  eta: 0:03:59  lr: 0.000358  min_lr: 0.000000  loss: 0.4934 (0.5945)  class_acc: 0.7500 (0.6812)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 1.4320  data: 0.0004  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.88 GB / 503.51 GB
Epoch: [0]  [740/893]  eta: 0:03:44  lr: 0.000363  min_lr: 0.000000  loss: 0.5024 (0.5931)  class_acc: 0.7500 (0.6824)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 1.4249  data: 0.0004  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.59 GB / 503.51 GB
Epoch: [0]  [750/893]  eta: 0:03:29  lr: 0.000368  min_lr: 0.000000  loss: 0.5024 (0.5918)  class_acc: 0.7679 (0.6836)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 1.4249  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.36 GB / 503.51 GB
Epoch: [0]  [760/893]  eta: 0:03:15  lr: 0.000373  min_lr: 0.000000  loss: 0.5044 (0.5908)  class_acc: 0.7500 (0.6846)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 1.4301  data: 0.0003  max mem: 31081
[2025-03-10 20:10:38,657] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:10:38,657] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096 to 8192
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.83 GB / 503.51 GB
Epoch: [0]  [770/893]  eta: 0:03:00  lr: 0.000378  min_lr: 0.000000  loss: 0.5083 (0.5897)  class_acc: 0.7500 (0.6854)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 1.4181  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.23 GB / 503.51 GB
Epoch: [0]  [780/893]  eta: 0:02:45  lr: 0.000383  min_lr: 0.000000  loss: 0.5405 (0.5892)  class_acc: 0.7143 (0.6858)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 1.4225  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.09 GB / 503.51 GB
Epoch: [0]  [790/893]  eta: 0:02:30  lr: 0.000388  min_lr: 0.000001  loss: 0.5142 (0.5879)  class_acc: 0.7500 (0.6869)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 1.4341  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.08 GB / 503.51 GB
Epoch: [0]  [800/893]  eta: 0:02:16  lr: 0.000392  min_lr: 0.000001  loss: 0.4648 (0.5866)  class_acc: 0.7679 (0.6882)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 1.4271  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.37 GB / 503.51 GB
Epoch: [0]  [810/893]  eta: 0:02:01  lr: 0.000397  min_lr: 0.000001  loss: 0.4456 (0.5850)  class_acc: 0.8036 (0.6896)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 1.4293  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.15 GB / 503.51 GB
Epoch: [0]  [820/893]  eta: 0:01:46  lr: 0.000402  min_lr: 0.000001  loss: 0.4421 (0.5834)  class_acc: 0.8214 (0.6910)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 1.4312  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.20 GB / 503.51 GB
Epoch: [0]  [830/893]  eta: 0:01:32  lr: 0.000407  min_lr: 0.000001  loss: 0.4602 (0.5823)  class_acc: 0.7679 (0.6919)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 1.4253  data: 0.0004  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.65 GB / 503.51 GB
Epoch: [0]  [840/893]  eta: 0:01:17  lr: 0.000412  min_lr: 0.000001  loss: 0.4897 (0.5812)  class_acc: 0.7500 (0.6929)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 1.4258  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.31 GB / 503.51 GB
Epoch: [0]  [850/893]  eta: 0:01:02  lr: 0.000417  min_lr: 0.000001  loss: 0.4897 (0.5806)  class_acc: 0.7500 (0.6938)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 1.4201  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.88 GB / 503.51 GB
Epoch: [0]  [860/893]  eta: 0:00:48  lr: 0.000422  min_lr: 0.000001  loss: 0.4832 (0.5795)  class_acc: 0.7679 (0.6946)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 1.3768  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.70 GB / 503.51 GB
Epoch: [0]  [870/893]  eta: 0:00:33  lr: 0.000427  min_lr: 0.000001  loss: 0.4768 (0.5784)  class_acc: 0.7679 (0.6956)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 1.3455  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.57 GB / 503.51 GB
Epoch: [0]  [880/893]  eta: 0:00:18  lr: 0.000432  min_lr: 0.000001  loss: 0.4949 (0.5773)  class_acc: 0.7679 (0.6964)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 1.3491  data: 0.0002  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.88 GB / 503.51 GB
Epoch: [0]  [890/893]  eta: 0:00:04  lr: 0.000437  min_lr: 0.000001  loss: 0.4966 (0.5762)  class_acc: 0.7500 (0.6973)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 1.3518  data: 0.0002  max mem: 31081
Epoch: [0]  [892/893]  eta: 0:00:01  lr: 0.000437  min_lr: 0.000001  loss: 0.5078 (0.5763)  class_acc: 0.7500 (0.6973)  loss_scale: 8192.0000 (2295.9641)  weight_decay: 0.0500 (0.0500)  time: 1.3047  data: 0.0002  max mem: 31081
Epoch: [0] Total time: 0:21:40 (1.4564 s / it)
Averaged stats: lr: 0.000437  min_lr: 0.000001  loss: 0.5078 (0.5763)  class_acc: 0.7500 (0.6973)  loss_scale: 8192.0000 (2295.9641)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:25:56  loss: 0.7187 (0.7187)  acc: 60.7143 (60.7143)  time: 12.0281  data: 10.5011  max mem: 31081
Val:  [ 10/728]  eta: 0:19:46  loss: 0.4147 (0.4611)  acc: 82.1429 (79.4372)  time: 1.6527  data: 1.0371  max mem: 31081
Val:  [ 20/728]  eta: 0:14:10  loss: 0.3953 (0.4733)  acc: 82.1429 (78.3447)  time: 0.6596  data: 0.1375  max mem: 31081
Val:  [ 30/728]  eta: 0:12:47  loss: 0.3953 (0.4941)  acc: 78.5714 (76.8433)  time: 0.7960  data: 0.2650  max mem: 31081
Val:  [ 40/728]  eta: 0:11:51  loss: 0.4029 (0.4887)  acc: 78.5714 (76.9744)  time: 0.8587  data: 0.3264  max mem: 31081
Val:  [ 50/728]  eta: 0:11:14  loss: 0.4099 (0.4875)  acc: 78.5714 (77.0775)  time: 0.8330  data: 0.3112  max mem: 31081
Val:  [ 60/728]  eta: 0:10:19  loss: 0.5007 (0.5033)  acc: 73.8095 (76.0343)  time: 0.7090  data: 0.1885  max mem: 31081
Val:  [ 70/728]  eta: 0:09:49  loss: 0.6154 (0.5043)  acc: 70.2381 (75.4863)  time: 0.6441  data: 0.1236  max mem: 31081
Val:  [ 80/728]  eta: 0:09:38  loss: 0.4645 (0.5161)  acc: 76.1905 (74.9559)  time: 0.7839  data: 0.2619  max mem: 31081
Val:  [ 90/728]  eta: 0:09:22  loss: 0.4645 (0.5302)  acc: 70.2381 (74.2805)  time: 0.8305  data: 0.3096  max mem: 31081
Val:  [100/728]  eta: 0:09:11  loss: 0.4364 (0.5201)  acc: 79.7619 (74.9175)  time: 0.8183  data: 0.2970  max mem: 31081
Val:  [110/728]  eta: 0:09:02  loss: 0.3778 (0.5167)  acc: 83.3333 (75.1931)  time: 0.8567  data: 0.3330  max mem: 31081
Val:  [120/728]  eta: 0:08:47  loss: 0.3938 (0.5112)  acc: 83.3333 (75.4427)  time: 0.8146  data: 0.2925  max mem: 31081
Val:  [130/728]  eta: 0:08:26  loss: 0.4102 (0.5151)  acc: 79.7619 (75.3544)  time: 0.6765  data: 0.1564  max mem: 31081
Val:  [140/728]  eta: 0:08:15  loss: 0.4963 (0.5215)  acc: 73.8095 (75.0338)  time: 0.6925  data: 0.1707  max mem: 31081
Val:  [150/728]  eta: 0:08:02  loss: 0.4963 (0.5240)  acc: 73.8095 (74.7556)  time: 0.7538  data: 0.2286  max mem: 31081
Val:  [160/728]  eta: 0:07:54  loss: 0.4705 (0.5250)  acc: 75.0000 (74.6894)  time: 0.7864  data: 0.2604  max mem: 31081
Val:  [170/728]  eta: 0:07:45  loss: 0.4703 (0.5221)  acc: 78.5714 (74.9582)  time: 0.8347  data: 0.3101  max mem: 31081
Val:  [180/728]  eta: 0:07:33  loss: 0.4574 (0.5236)  acc: 79.7619 (74.9605)  time: 0.7595  data: 0.2371  max mem: 31081
Val:  [190/728]  eta: 0:07:19  loss: 0.4857 (0.5202)  acc: 77.3810 (75.0623)  time: 0.6591  data: 0.1385  max mem: 31081
Val:  [200/728]  eta: 0:07:11  loss: 0.4500 (0.5163)  acc: 77.3810 (75.2665)  time: 0.7317  data: 0.2105  max mem: 31081
Val:  [210/728]  eta: 0:07:06  loss: 0.4500 (0.5175)  acc: 79.7619 (75.2877)  time: 0.8894  data: 0.3668  max mem: 31081
Val:  [220/728]  eta: 0:06:59  loss: 0.4688 (0.5148)  acc: 77.3810 (75.4902)  time: 0.8997  data: 0.3770  max mem: 31081
Val:  [230/728]  eta: 0:06:49  loss: 0.4362 (0.5118)  acc: 82.1429 (75.6751)  time: 0.8213  data: 0.2991  max mem: 31081
Val:  [240/728]  eta: 0:06:38  loss: 0.4160 (0.5084)  acc: 82.1429 (75.9287)  time: 0.7187  data: 0.1953  max mem: 31081
Val:  [250/728]  eta: 0:06:27  loss: 0.4160 (0.5071)  acc: 80.9524 (76.0434)  time: 0.6750  data: 0.1503  max mem: 31081
Val:  [260/728]  eta: 0:06:20  loss: 0.4285 (0.5054)  acc: 79.7619 (76.1950)  time: 0.7680  data: 0.2441  max mem: 31081
Val:  [270/728]  eta: 0:06:12  loss: 0.4285 (0.5047)  acc: 79.7619 (76.2432)  time: 0.8399  data: 0.3176  max mem: 31081
Val:  [280/728]  eta: 0:06:07  loss: 0.4394 (0.5029)  acc: 77.3810 (76.3303)  time: 0.9240  data: 0.4031  max mem: 31081
Val:  [290/728]  eta: 0:05:59  loss: 0.4344 (0.5017)  acc: 78.5714 (76.3500)  time: 0.9165  data: 0.3955  max mem: 31081
Val:  [300/728]  eta: 0:05:46  loss: 0.4199 (0.5010)  acc: 78.5714 (76.4120)  time: 0.6716  data: 0.1488  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.4541 (0.5016)  acc: 78.5714 (76.4202)  time: 0.6739  data: 0.1521  max mem: 31081
Val:  [320/728]  eta: 0:05:30  loss: 0.4287 (0.5007)  acc: 80.9524 (76.4427)  time: 0.8159  data: 0.2949  max mem: 31081
Val:  [330/728]  eta: 0:05:23  loss: 0.4282 (0.5006)  acc: 78.5714 (76.3703)  time: 0.8524  data: 0.3298  max mem: 31081
Val:  [340/728]  eta: 0:05:16  loss: 0.4198 (0.4973)  acc: 78.5714 (76.5815)  time: 0.9045  data: 0.3818  max mem: 31081
Val:  [350/728]  eta: 0:05:08  loss: 0.3873 (0.4954)  acc: 83.3333 (76.6857)  time: 0.8546  data: 0.3127  max mem: 31081
Val:  [360/728]  eta: 0:04:57  loss: 0.4277 (0.4936)  acc: 80.9524 (76.8269)  time: 0.6606  data: 0.1179  max mem: 31081
Val:  [370/728]  eta: 0:04:49  loss: 0.4366 (0.4949)  acc: 79.7619 (76.8034)  time: 0.6961  data: 0.1741  max mem: 31081
Val:  [380/728]  eta: 0:04:41  loss: 0.4961 (0.4965)  acc: 76.1905 (76.7498)  time: 0.8530  data: 0.3320  max mem: 31081
Val:  [390/728]  eta: 0:04:34  loss: 0.3959 (0.4931)  acc: 76.1905 (76.9182)  time: 0.8378  data: 0.3116  max mem: 31081
Val:  [400/728]  eta: 0:04:26  loss: 0.3557 (0.4915)  acc: 80.9524 (77.0277)  time: 0.8637  data: 0.3380  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.3618 (0.4903)  acc: 79.7619 (77.0797)  time: 0.8686  data: 0.3469  max mem: 31081
Val:  [420/728]  eta: 0:04:08  loss: 0.4534 (0.4903)  acc: 79.7619 (77.0982)  time: 0.6835  data: 0.1625  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4534 (0.4904)  acc: 77.3810 (77.0661)  time: 0.6922  data: 0.1698  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.4048 (0.4891)  acc: 82.1429 (77.1596)  time: 0.8267  data: 0.3046  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4274 (0.4915)  acc: 79.7619 (76.9850)  time: 0.8251  data: 0.3043  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.4863 (0.4938)  acc: 73.8095 (76.8438)  time: 0.8463  data: 0.3246  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.5120 (0.4937)  acc: 70.2381 (76.8375)  time: 0.8058  data: 0.2829  max mem: 31081
Val:  [480/728]  eta: 0:03:19  loss: 0.4719 (0.4931)  acc: 72.6190 (76.8513)  time: 0.6487  data: 0.1278  max mem: 31081
Val:  [490/728]  eta: 0:03:11  loss: 0.4908 (0.4943)  acc: 73.8095 (76.7190)  time: 0.6922  data: 0.1712  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.4841 (0.4953)  acc: 76.1905 (76.6586)  time: 0.8510  data: 0.3278  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.4527 (0.4941)  acc: 79.7619 (76.7310)  time: 0.8746  data: 0.3527  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4537 (0.4944)  acc: 76.1905 (76.6932)  time: 0.8549  data: 0.3344  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.4960 (0.4934)  acc: 75.0000 (76.7689)  time: 0.7538  data: 0.2335  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.4877 (0.4948)  acc: 76.1905 (76.6900)  time: 0.6166  data: 0.0952  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.5002 (0.4958)  acc: 76.1905 (76.6269)  time: 0.6659  data: 0.1437  max mem: 31081
Val:  [560/728]  eta: 0:02:14  loss: 0.5002 (0.4957)  acc: 73.8095 (76.6276)  time: 0.7571  data: 0.2331  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4414 (0.4985)  acc: 73.8095 (76.5241)  time: 0.8021  data: 0.2780  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4306 (0.4987)  acc: 80.9524 (76.5060)  time: 0.8356  data: 0.3131  max mem: 31081
Val:  [590/728]  eta: 0:01:50  loss: 0.4547 (0.4986)  acc: 76.1905 (76.5289)  time: 0.7390  data: 0.2167  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4547 (0.4989)  acc: 75.0000 (76.4995)  time: 0.6954  data: 0.1733  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.5463 (0.5008)  acc: 70.2381 (76.3405)  time: 0.6615  data: 0.1399  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.5480 (0.5022)  acc: 66.6667 (76.2614)  time: 0.7541  data: 0.2309  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4575 (0.5016)  acc: 77.3810 (76.3150)  time: 0.8415  data: 0.3174  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.4239 (0.5012)  acc: 79.7619 (76.3316)  time: 0.8008  data: 0.2776  max mem: 31081
Val:  [650/728]  eta: 0:01:02  loss: 0.4352 (0.5015)  acc: 75.0000 (76.2728)  time: 0.8058  data: 0.2828  max mem: 31081
Val:  [660/728]  eta: 0:00:54  loss: 0.4982 (0.5011)  acc: 73.8095 (76.3003)  time: 0.8082  data: 0.2846  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.5223 (0.5019)  acc: 77.3810 (76.2898)  time: 0.6871  data: 0.1638  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4639 (0.5011)  acc: 77.3810 (76.3286)  time: 0.6553  data: 0.1332  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.4102 (0.5006)  acc: 79.7619 (76.3541)  time: 0.8446  data: 0.3202  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4509 (0.5007)  acc: 79.7619 (76.3569)  time: 0.8636  data: 0.3358  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4844 (0.5007)  acc: 76.1905 (76.3495)  time: 0.8244  data: 0.3031  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4832 (0.5006)  acc: 76.1905 (76.3160)  time: 0.7958  data: 0.2846  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4844 (0.5015)  acc: 76.1905 (76.2734)  time: 0.7851  data: 0.2846  max mem: 31081
Val: Total time: 0:09:35 (0.7903 s / it)
* Acc@1 76.273 AP 0.7670820951461792 loss 0.502
Accuracy of the network on the 61096 val videos: 76.3%
[2025-03-10 20:23:07,985] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestauroc is about to be saved!
[2025-03-10 20:23:08,010] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestauroc/mp_rank_00_model_states.pt
[2025-03-10 20:23:08,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestauroc/mp_rank_00_model_states.pt...
[2025-03-10 20:23:08,309] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestauroc/mp_rank_00_model_states.pt.
[2025-03-10 20:23:08,309] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestauroc is ready now!
[2025-03-10 20:23:08,310] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestap is about to be saved!
[2025-03-10 20:23:08,313] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt
[2025-03-10 20:23:08,313] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt...
[2025-03-10 20:23:08,609] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt.
[2025-03-10 20:23:08,609] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestap is ready now!
[2025-03-10 20:23:08,610] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestacc is about to be saved!
[2025-03-10 20:23:08,613] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt
[2025-03-10 20:23:08,613] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt...
[2025-03-10 20:23:08,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt.
[2025-03-10 20:23:08,893] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestacc is ready now!
Max accuracy: 0.84%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.41 GB / 503.51 GB
Epoch: [1]  [  0/893]  eta: 3:26:40  lr: 0.000438  min_lr: 0.000001  loss: 0.5552 (0.5552)  class_acc: 0.6786 (0.6786)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 13.8859  data: 12.5489  max mem: 31081
[2025-03-10 20:23:29,076] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:23:29,076] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192 to 16384
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.73 GB / 503.51 GB
Epoch: [1]  [ 10/893]  eta: 0:38:32  lr: 0.000443  min_lr: 0.000001  loss: 0.5459 (0.5221)  class_acc: 0.7500 (0.7484)  loss_scale: 16384.0000 (13405.0909)  weight_decay: 0.0500 (0.0500)  time: 2.6194  data: 1.1413  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.26 GB / 503.51 GB
Epoch: [1]  [ 20/893]  eta: 0:30:28  lr: 0.000447  min_lr: 0.000001  loss: 0.4666 (0.4747)  class_acc: 0.7679 (0.7815)  loss_scale: 16384.0000 (14823.6190)  weight_decay: 0.0500 (0.0500)  time: 1.5045  data: 0.0005  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.84 GB / 503.51 GB
Epoch: [1]  [ 30/893]  eta: 0:27:21  lr: 0.000452  min_lr: 0.000001  loss: 0.4507 (0.4771)  class_acc: 0.8036 (0.7823)  loss_scale: 16384.0000 (15326.9677)  weight_decay: 0.0500 (0.0500)  time: 1.5071  data: 0.0006  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.08 GB / 503.51 GB
Epoch: [1]  [ 40/893]  eta: 0:25:39  lr: 0.000457  min_lr: 0.000001  loss: 0.4841 (0.4785)  class_acc: 0.7500 (0.7774)  loss_scale: 16384.0000 (15584.7805)  weight_decay: 0.0500 (0.0500)  time: 1.5007  data: 0.0006  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.37 GB / 503.51 GB
Epoch: [1]  [ 50/893]  eta: 0:24:29  lr: 0.000462  min_lr: 0.000001  loss: 0.4927 (0.4861)  class_acc: 0.7500 (0.7749)  loss_scale: 16384.0000 (15741.4902)  weight_decay: 0.0500 (0.0500)  time: 1.4962  data: 0.0005  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.25 GB / 503.51 GB
Epoch: [1]  [ 60/893]  eta: 0:23:34  lr: 0.000467  min_lr: 0.000001  loss: 0.4854 (0.4787)  class_acc: 0.7857 (0.7793)  loss_scale: 16384.0000 (15846.8197)  weight_decay: 0.0500 (0.0500)  time: 1.4783  data: 0.0003  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.13 GB / 503.51 GB
Epoch: [1]  [ 70/893]  eta: 0:22:49  lr: 0.000472  min_lr: 0.000001  loss: 0.4744 (0.4864)  class_acc: 0.7679 (0.7721)  loss_scale: 16384.0000 (15922.4789)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0002  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.79 GB / 503.51 GB
Epoch: [1]  [ 80/893]  eta: 0:22:12  lr: 0.000477  min_lr: 0.000001  loss: 0.4807 (0.4848)  class_acc: 0.7500 (0.7729)  loss_scale: 16384.0000 (15979.4568)  weight_decay: 0.0500 (0.0500)  time: 1.4609  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.65 GB / 503.51 GB
Epoch: [1]  [ 90/893]  eta: 0:21:40  lr: 0.000482  min_lr: 0.000001  loss: 0.4441 (0.4826)  class_acc: 0.7857 (0.7730)  loss_scale: 16384.0000 (16023.9121)  weight_decay: 0.0500 (0.0500)  time: 1.4593  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.55 GB / 503.51 GB
Epoch: [1]  [100/893]  eta: 0:21:11  lr: 0.000487  min_lr: 0.000001  loss: 0.4883 (0.4846)  class_acc: 0.7679 (0.7714)  loss_scale: 16384.0000 (16059.5644)  weight_decay: 0.0500 (0.0500)  time: 1.4597  data: 0.0003  max mem: 31081
[2025-03-10 20:26:01,466] [INFO] [logging.py:129:log_dist] [Rank 0] step=1000, skipped=0, lr=[6.400919015886969e-07, 6.400919015886969e-07, 1.0668198359811614e-06, 1.0668198359811614e-06, 1.7780330599686024e-06, 1.7780330599686024e-06, 2.963388433281004e-06, 2.963388433281004e-06, 4.9389807221350075e-06, 4.9389807221350075e-06, 8.231634536891678e-06, 8.231634536891678e-06, 1.3719390894819467e-05, 1.3719390894819467e-05, 2.2865651491365777e-05, 2.2865651491365777e-05, 3.8109419152276293e-05, 3.8109419152276293e-05, 6.351569858712717e-05, 6.351569858712717e-05, 0.00010585949764521194, 0.00010585949764521194, 0.00017643249607535326, 0.00017643249607535326, 0.00029405416012558876, 0.00029405416012558876, 0.0004900902668759813, 0.0004900902668759813], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-10 20:26:01,466] [INFO] [timer.py:264:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=61.82313994233723, CurrSamplesPerSec=61.53677585759787, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.53 GB / 503.51 GB
Epoch: [1]  [110/893]  eta: 0:20:45  lr: 0.000492  min_lr: 0.000001  loss: 0.5073 (0.4871)  class_acc: 0.7500 (0.7693)  loss_scale: 16384.0000 (16088.7928)  weight_decay: 0.0500 (0.0500)  time: 1.4609  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.31 GB / 503.51 GB
Epoch: [1]  [120/893]  eta: 0:20:21  lr: 0.000496  min_lr: 0.000001  loss: 0.4746 (0.4853)  class_acc: 0.7500 (0.7715)  loss_scale: 16384.0000 (16113.1901)  weight_decay: 0.0500 (0.0500)  time: 1.4612  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.52 GB / 503.51 GB
Epoch: [1]  [130/893]  eta: 0:19:58  lr: 0.000501  min_lr: 0.000001  loss: 0.4724 (0.4836)  class_acc: 0.8036 (0.7730)  loss_scale: 16384.0000 (16133.8626)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0003  max mem: 31081
[2025-03-10 20:26:37,966] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:26:37,966] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 16384 to 32768
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.34 GB / 503.51 GB
Epoch: [1]  [140/893]  eta: 0:19:36  lr: 0.000506  min_lr: 0.000001  loss: 0.4712 (0.4808)  class_acc: 0.8036 (0.7744)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  time: 1.4593  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.83 GB / 503.51 GB
Epoch: [1]  [150/893]  eta: 0:19:16  lr: 0.000511  min_lr: 0.000001  loss: 0.4443 (0.4823)  class_acc: 0.7857 (0.7746)  loss_scale: 32768.0000 (18228.5563)  weight_decay: 0.0500 (0.0500)  time: 1.4570  data: 0.0003  max mem: 31081
[2025-03-10 20:27:08,615] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 1045
[2025-03-10 20:27:08,615] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0
[2025-03-10 20:27:08,615] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768, reducing to 16384.0
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.37 GB / 503.51 GB
Epoch: [1]  [160/893]  eta: 0:18:56  lr: 0.000516  min_lr: 0.000001  loss: 0.5322 (0.4836)  class_acc: 0.7500 (0.7747)  loss_scale: 32768.0000 (18317.5155)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.46 GB / 503.51 GB
Epoch: [1]  [170/893]  eta: 0:18:36  lr: 0.000521  min_lr: 0.000001  loss: 0.4412 (0.4829)  class_acc: 0.7679 (0.7751)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  time: 1.4597  data: 0.0004  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.12 GB / 503.51 GB
Epoch: [1]  [180/893]  eta: 0:18:17  lr: 0.000526  min_lr: 0.000001  loss: 0.4412 (0.4819)  class_acc: 0.8036 (0.7759)  loss_scale: 16384.0000 (18103.8674)  weight_decay: 0.0500 (0.0500)  time: 1.4562  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.89 GB / 503.51 GB
Epoch: [1]  [190/893]  eta: 0:17:59  lr: 0.000531  min_lr: 0.000001  loss: 0.4690 (0.4836)  class_acc: 0.7857 (0.7744)  loss_scale: 16384.0000 (18013.8220)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0002  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.20 GB / 503.51 GB
Epoch: [1]  [200/893]  eta: 0:17:41  lr: 0.000536  min_lr: 0.000001  loss: 0.5049 (0.4852)  class_acc: 0.7500 (0.7729)  loss_scale: 16384.0000 (17932.7363)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.99 GB / 503.51 GB
Epoch: [1]  [210/893]  eta: 0:17:24  lr: 0.000541  min_lr: 0.000001  loss: 0.4854 (0.4843)  class_acc: 0.7679 (0.7740)  loss_scale: 16384.0000 (17859.3365)  weight_decay: 0.0500 (0.0500)  time: 1.4617  data: 0.0004  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.69 GB / 503.51 GB
Epoch: [1]  [220/893]  eta: 0:17:06  lr: 0.000546  min_lr: 0.000001  loss: 0.4763 (0.4840)  class_acc: 0.7857 (0.7741)  loss_scale: 16384.0000 (17792.5792)  weight_decay: 0.0500 (0.0500)  time: 1.4587  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.51 GB / 503.51 GB
Epoch: [1]  [230/893]  eta: 0:16:49  lr: 0.000550  min_lr: 0.000001  loss: 0.4553 (0.4838)  class_acc: 0.7679 (0.7740)  loss_scale: 16384.0000 (17731.6017)  weight_decay: 0.0500 (0.0500)  time: 1.4584  data: 0.0004  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.60 GB / 503.51 GB
Epoch: [1]  [240/893]  eta: 0:16:32  lr: 0.000555  min_lr: 0.000001  loss: 0.4592 (0.4824)  class_acc: 0.7679 (0.7745)  loss_scale: 16384.0000 (17675.6846)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.87 GB / 503.51 GB
Epoch: [1]  [250/893]  eta: 0:16:16  lr: 0.000560  min_lr: 0.000001  loss: 0.4592 (0.4824)  class_acc: 0.7679 (0.7743)  loss_scale: 16384.0000 (17624.2231)  weight_decay: 0.0500 (0.0500)  time: 1.4713  data: 0.0003  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.77 GB / 503.51 GB
Epoch: [1]  [260/893]  eta: 0:15:59  lr: 0.000565  min_lr: 0.000001  loss: 0.4739 (0.4824)  class_acc: 0.7857 (0.7742)  loss_scale: 16384.0000 (17576.7050)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0002  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.36 GB / 503.51 GB
Epoch: [1]  [270/893]  eta: 0:15:43  lr: 0.000570  min_lr: 0.000001  loss: 0.4702 (0.4813)  class_acc: 0.7857 (0.7747)  loss_scale: 16384.0000 (17532.6937)  weight_decay: 0.0500 (0.0500)  time: 1.4586  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.44 GB / 503.51 GB
Epoch: [1]  [280/893]  eta: 0:15:27  lr: 0.000575  min_lr: 0.000001  loss: 0.4834 (0.4826)  class_acc: 0.7500 (0.7734)  loss_scale: 16384.0000 (17491.8149)  weight_decay: 0.0500 (0.0500)  time: 1.4650  data: 0.0003  max mem: 31081
[2025-03-10 20:30:17,266] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:30:17,267] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.42 GB / 503.51 GB
Epoch: [1]  [290/893]  eta: 0:15:10  lr: 0.000580  min_lr: 0.000001  loss: 0.4834 (0.4815)  class_acc: 0.7679 (0.7741)  loss_scale: 16384.0000 (17960.4674)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.96 GB / 503.51 GB
Epoch: [1]  [300/893]  eta: 0:14:55  lr: 0.000585  min_lr: 0.000001  loss: 0.4414 (0.4803)  class_acc: 0.8036 (0.7753)  loss_scale: 32768.0000 (18452.4120)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0002  max mem: 31081
[2025-03-10 20:30:53,818] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 1199
[2025-03-10 20:30:53,818] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-03-10 20:30:53,818] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.59 GB / 503.51 GB
Epoch: [1]  [310/893]  eta: 0:14:39  lr: 0.000590  min_lr: 0.000001  loss: 0.4626 (0.4807)  class_acc: 0.7857 (0.7753)  loss_scale: 32768.0000 (18701.9936)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0002  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.04 GB / 503.51 GB
Epoch: [1]  [320/893]  eta: 0:14:23  lr: 0.000595  min_lr: 0.000001  loss: 0.4907 (0.4813)  class_acc: 0.7679 (0.7746)  loss_scale: 16384.0000 (18629.7819)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0004  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.58 GB / 503.51 GB
Epoch: [1]  [330/893]  eta: 0:14:07  lr: 0.000599  min_lr: 0.000001  loss: 0.4954 (0.4811)  class_acc: 0.7679 (0.7747)  loss_scale: 16384.0000 (18561.9335)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.25 GB / 503.51 GB
Epoch: [1]  [340/893]  eta: 0:13:51  lr: 0.000604  min_lr: 0.000001  loss: 0.4482 (0.4804)  class_acc: 0.8036 (0.7755)  loss_scale: 16384.0000 (18498.0645)  weight_decay: 0.0500 (0.0500)  time: 1.4591  data: 0.0002  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.50 GB / 503.51 GB
Epoch: [1]  [350/893]  eta: 0:13:35  lr: 0.000609  min_lr: 0.000001  loss: 0.4460 (0.4796)  class_acc: 0.8036 (0.7763)  loss_scale: 16384.0000 (18437.8348)  weight_decay: 0.0500 (0.0500)  time: 1.4611  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.35 GB / 503.51 GB
Epoch: [1]  [360/893]  eta: 0:13:20  lr: 0.000614  min_lr: 0.000001  loss: 0.4338 (0.4790)  class_acc: 0.7857 (0.7769)  loss_scale: 16384.0000 (18380.9418)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.13 GB / 503.51 GB
Epoch: [1]  [370/893]  eta: 0:13:04  lr: 0.000619  min_lr: 0.000001  loss: 0.4138 (0.4780)  class_acc: 0.8036 (0.7773)  loss_scale: 16384.0000 (18327.1159)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.55 GB / 503.51 GB
Epoch: [1]  [380/893]  eta: 0:12:49  lr: 0.000624  min_lr: 0.000001  loss: 0.4343 (0.4774)  class_acc: 0.8036 (0.7776)  loss_scale: 16384.0000 (18276.1155)  weight_decay: 0.0500 (0.0500)  time: 1.4593  data: 0.0002  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.86 GB / 503.51 GB
Epoch: [1]  [390/893]  eta: 0:12:33  lr: 0.000629  min_lr: 0.000001  loss: 0.4438 (0.4765)  class_acc: 0.8036 (0.7781)  loss_scale: 16384.0000 (18227.7238)  weight_decay: 0.0500 (0.0500)  time: 1.4592  data: 0.0002  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.91 GB / 503.51 GB
Epoch: [1]  [400/893]  eta: 0:12:18  lr: 0.000634  min_lr: 0.000001  loss: 0.4487 (0.4761)  class_acc: 0.7857 (0.7784)  loss_scale: 16384.0000 (18181.7456)  weight_decay: 0.0500 (0.0500)  time: 1.4650  data: 0.0003  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.54 GB / 503.51 GB
Epoch: [1]  [410/893]  eta: 0:12:02  lr: 0.000639  min_lr: 0.000001  loss: 0.4487 (0.4755)  class_acc: 0.7857 (0.7788)  loss_scale: 16384.0000 (18138.0049)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0004  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.16 GB / 503.51 GB
Epoch: [1]  [420/893]  eta: 0:11:47  lr: 0.000644  min_lr: 0.000001  loss: 0.4580 (0.4753)  class_acc: 0.8036 (0.7793)  loss_scale: 16384.0000 (18096.3420)  weight_decay: 0.0500 (0.0500)  time: 1.4537  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.11 GB / 503.51 GB
Epoch: [1]  [430/893]  eta: 0:11:32  lr: 0.000649  min_lr: 0.000001  loss: 0.5039 (0.4757)  class_acc: 0.7857 (0.7792)  loss_scale: 16384.0000 (18056.6125)  weight_decay: 0.0500 (0.0500)  time: 1.4533  data: 0.0003  max mem: 31081
[2025-03-10 20:34:02,215] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:34:02,216] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.61 GB / 503.51 GB
Epoch: [1]  [440/893]  eta: 0:11:16  lr: 0.000653  min_lr: 0.000001  loss: 0.4905 (0.4753)  class_acc: 0.7679 (0.7794)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0002  max mem: 31081
[2025-03-10 20:34:10,984] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 1334
[2025-03-10 20:34:10,984] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-03-10 20:34:10,984] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.11 GB / 503.51 GB
Epoch: [1]  [450/893]  eta: 0:11:01  lr: 0.000658  min_lr: 0.000001  loss: 0.4463 (0.4743)  class_acc: 0.8036 (0.7800)  loss_scale: 16384.0000 (18200.4080)  weight_decay: 0.0500 (0.0500)  time: 1.4596  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.99 GB / 503.51 GB
Epoch: [1]  [460/893]  eta: 0:10:46  lr: 0.000663  min_lr: 0.000001  loss: 0.4463 (0.4742)  class_acc: 0.7857 (0.7801)  loss_scale: 16384.0000 (18161.0065)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.04 GB / 503.51 GB
Epoch: [1]  [470/893]  eta: 0:10:30  lr: 0.000668  min_lr: 0.000001  loss: 0.4377 (0.4733)  class_acc: 0.8036 (0.7807)  loss_scale: 16384.0000 (18123.2781)  weight_decay: 0.0500 (0.0500)  time: 1.4609  data: 0.0002  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.04 GB / 503.51 GB
Epoch: [1]  [480/893]  eta: 0:10:15  lr: 0.000673  min_lr: 0.000001  loss: 0.4353 (0.4725)  class_acc: 0.8036 (0.7810)  loss_scale: 16384.0000 (18087.1185)  weight_decay: 0.0500 (0.0500)  time: 1.4594  data: 0.0002  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.00 GB / 503.51 GB
Epoch: [1]  [490/893]  eta: 0:10:00  lr: 0.000678  min_lr: 0.000001  loss: 0.4199 (0.4715)  class_acc: 0.8036 (0.7820)  loss_scale: 16384.0000 (18052.4318)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.56 GB / 503.51 GB
Epoch: [1]  [500/893]  eta: 0:09:45  lr: 0.000683  min_lr: 0.000001  loss: 0.4373 (0.4719)  class_acc: 0.8036 (0.7816)  loss_scale: 16384.0000 (18019.1297)  weight_decay: 0.0500 (0.0500)  time: 1.4688  data: 0.0004  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.29 GB / 503.51 GB
Epoch: [1]  [510/893]  eta: 0:09:30  lr: 0.000688  min_lr: 0.000001  loss: 0.4995 (0.4722)  class_acc: 0.7679 (0.7817)  loss_scale: 16384.0000 (17987.1311)  weight_decay: 0.0500 (0.0500)  time: 1.4615  data: 0.0004  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.49 GB / 503.51 GB
Epoch: [1]  [520/893]  eta: 0:09:15  lr: 0.000693  min_lr: 0.000001  loss: 0.4504 (0.4722)  class_acc: 0.7857 (0.7818)  loss_scale: 16384.0000 (17956.3608)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0004  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.42 GB / 503.51 GB
Epoch: [1]  [530/893]  eta: 0:09:00  lr: 0.000698  min_lr: 0.000001  loss: 0.4766 (0.4729)  class_acc: 0.7500 (0.7813)  loss_scale: 16384.0000 (17926.7495)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0004  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.84 GB / 503.51 GB
Epoch: [1]  [540/893]  eta: 0:08:45  lr: 0.000703  min_lr: 0.000001  loss: 0.4502 (0.4723)  class_acc: 0.7679 (0.7816)  loss_scale: 16384.0000 (17898.2329)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.46 GB / 503.51 GB
Epoch: [1]  [550/893]  eta: 0:08:30  lr: 0.000707  min_lr: 0.000001  loss: 0.4465 (0.4726)  class_acc: 0.7857 (0.7813)  loss_scale: 16384.0000 (17870.7514)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.26 GB / 503.51 GB
Epoch: [1]  [560/893]  eta: 0:08:15  lr: 0.000712  min_lr: 0.000001  loss: 0.4539 (0.4721)  class_acc: 0.7857 (0.7815)  loss_scale: 16384.0000 (17844.2496)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0002  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.69 GB / 503.51 GB
Epoch: [1]  [570/893]  eta: 0:08:00  lr: 0.000717  min_lr: 0.000001  loss: 0.4431 (0.4719)  class_acc: 0.7857 (0.7816)  loss_scale: 16384.0000 (17818.6760)  weight_decay: 0.0500 (0.0500)  time: 1.4582  data: 0.0003  max mem: 31081
[2025-03-10 20:37:19,903] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:37:19,903] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-03-10 20:37:21,366] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 1464
[2025-03-10 20:37:21,366] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-03-10 20:37:21,366] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.04 GB / 503.51 GB
Epoch: [1]  [580/893]  eta: 0:07:45  lr: 0.000722  min_lr: 0.000001  loss: 0.4570 (0.4723)  class_acc: 0.7679 (0.7813)  loss_scale: 16384.0000 (17822.1824)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0004  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.38 GB / 503.51 GB
Epoch: [1]  [590/893]  eta: 0:07:30  lr: 0.000727  min_lr: 0.000001  loss: 0.4653 (0.4725)  class_acc: 0.7679 (0.7812)  loss_scale: 16384.0000 (17797.8477)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.37 GB / 503.51 GB
Epoch: [1]  [600/893]  eta: 0:07:15  lr: 0.000732  min_lr: 0.000001  loss: 0.4758 (0.4729)  class_acc: 0.7679 (0.7808)  loss_scale: 16384.0000 (17774.3228)  weight_decay: 0.0500 (0.0500)  time: 1.4590  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.13 GB / 503.51 GB
Epoch: [1]  [610/893]  eta: 0:07:00  lr: 0.000737  min_lr: 0.000001  loss: 0.4731 (0.4730)  class_acc: 0.7679 (0.7804)  loss_scale: 16384.0000 (17751.5679)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.95 GB / 503.51 GB
Epoch: [1]  [620/893]  eta: 0:06:45  lr: 0.000742  min_lr: 0.000001  loss: 0.4731 (0.4732)  class_acc: 0.7500 (0.7802)  loss_scale: 16384.0000 (17729.5459)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.57 GB / 503.51 GB
Epoch: [1]  [630/893]  eta: 0:06:30  lr: 0.000747  min_lr: 0.000001  loss: 0.4661 (0.4732)  class_acc: 0.7679 (0.7800)  loss_scale: 16384.0000 (17708.2219)  weight_decay: 0.0500 (0.0500)  time: 1.4576  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.00 GB / 503.51 GB
Epoch: [1]  [640/893]  eta: 0:06:15  lr: 0.000752  min_lr: 0.000001  loss: 0.4463 (0.4733)  class_acc: 0.7679 (0.7798)  loss_scale: 16384.0000 (17687.5632)  weight_decay: 0.0500 (0.0500)  time: 1.4543  data: 0.0003  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.58 GB / 503.51 GB
Epoch: [1]  [650/893]  eta: 0:06:00  lr: 0.000756  min_lr: 0.000001  loss: 0.4453 (0.4729)  class_acc: 0.7679 (0.7800)  loss_scale: 16384.0000 (17667.5392)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0002  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.88 GB / 503.51 GB
Epoch: [1]  [660/893]  eta: 0:05:45  lr: 0.000761  min_lr: 0.000001  loss: 0.4578 (0.4731)  class_acc: 0.7679 (0.7799)  loss_scale: 16384.0000 (17648.1210)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0002  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.13 GB / 503.51 GB
Epoch: [1]  [670/893]  eta: 0:05:30  lr: 0.000766  min_lr: 0.000001  loss: 0.4368 (0.4725)  class_acc: 0.7857 (0.7804)  loss_scale: 16384.0000 (17629.2817)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0002  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.23 GB / 503.51 GB
Epoch: [1]  [680/893]  eta: 0:05:15  lr: 0.000771  min_lr: 0.000001  loss: 0.4658 (0.4725)  class_acc: 0.7857 (0.7802)  loss_scale: 16384.0000 (17610.9956)  weight_decay: 0.0500 (0.0500)  time: 1.4592  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.90 GB / 503.51 GB
Epoch: [1]  [690/893]  eta: 0:05:00  lr: 0.000776  min_lr: 0.000001  loss: 0.4666 (0.4724)  class_acc: 0.7500 (0.7802)  loss_scale: 16384.0000 (17593.2388)  weight_decay: 0.0500 (0.0500)  time: 1.4598  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.96 GB / 503.51 GB
Epoch: [1]  [700/893]  eta: 0:04:46  lr: 0.000781  min_lr: 0.000001  loss: 0.4690 (0.4726)  class_acc: 0.7857 (0.7800)  loss_scale: 16384.0000 (17575.9886)  weight_decay: 0.0500 (0.0500)  time: 1.4586  data: 0.0003  max mem: 31081
[2025-03-10 20:40:29,815] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:40:29,815] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.32 GB / 503.51 GB
Epoch: [1]  [710/893]  eta: 0:04:31  lr: 0.000786  min_lr: 0.000001  loss: 0.4683 (0.4724)  class_acc: 0.8036 (0.7801)  loss_scale: 16384.0000 (17789.6596)  weight_decay: 0.0500 (0.0500)  time: 1.4570  data: 0.0003  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.49 GB / 503.51 GB
Epoch: [1]  [720/893]  eta: 0:04:16  lr: 0.000791  min_lr: 0.000001  loss: 0.4304 (0.4719)  class_acc: 0.8036 (0.7803)  loss_scale: 32768.0000 (17997.4036)  weight_decay: 0.0500 (0.0500)  time: 1.4588  data: 0.0002  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.10 GB / 503.51 GB
Epoch: [1]  [730/893]  eta: 0:04:01  lr: 0.000796  min_lr: 0.000001  loss: 0.4143 (0.4711)  class_acc: 0.8214 (0.7810)  loss_scale: 32768.0000 (18199.4637)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0002  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.00 GB / 503.51 GB
Epoch: [1]  [740/893]  eta: 0:03:46  lr: 0.000801  min_lr: 0.000001  loss: 0.4387 (0.4717)  class_acc: 0.8036 (0.7807)  loss_scale: 32768.0000 (18396.0702)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.11 GB / 503.51 GB
Epoch: [1]  [750/893]  eta: 0:03:31  lr: 0.000806  min_lr: 0.000001  loss: 0.4802 (0.4715)  class_acc: 0.7679 (0.7807)  loss_scale: 32768.0000 (18587.4407)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.36 GB / 503.51 GB
Epoch: [1]  [760/893]  eta: 0:03:16  lr: 0.000810  min_lr: 0.000001  loss: 0.4414 (0.4710)  class_acc: 0.7679 (0.7808)  loss_scale: 32768.0000 (18773.7819)  weight_decay: 0.0500 (0.0500)  time: 1.4574  data: 0.0003  max mem: 31081
[2025-03-10 20:42:00,490] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 1655
[2025-03-10 20:42:00,490] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-03-10 20:42:00,490] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.06 GB / 503.51 GB
Epoch: [1]  [770/893]  eta: 0:03:02  lr: 0.000815  min_lr: 0.000001  loss: 0.4216 (0.4706)  class_acc: 0.8036 (0.7811)  loss_scale: 32768.0000 (18785.2866)  weight_decay: 0.0500 (0.0500)  time: 1.4627  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.80 GB / 503.51 GB
Epoch: [1]  [780/893]  eta: 0:02:47  lr: 0.000820  min_lr: 0.000001  loss: 0.4336 (0.4703)  class_acc: 0.8036 (0.7814)  loss_scale: 16384.0000 (18754.5403)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.74 GB / 503.51 GB
Epoch: [1]  [790/893]  eta: 0:02:32  lr: 0.000825  min_lr: 0.000001  loss: 0.4543 (0.4701)  class_acc: 0.7857 (0.7817)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  time: 1.4567  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.08 GB / 503.51 GB
Epoch: [1]  [800/893]  eta: 0:02:17  lr: 0.000830  min_lr: 0.000001  loss: 0.4502 (0.4697)  class_acc: 0.8214 (0.7821)  loss_scale: 16384.0000 (18695.3508)  weight_decay: 0.0500 (0.0500)  time: 1.4550  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.03 GB / 503.51 GB
Epoch: [1]  [810/893]  eta: 0:02:02  lr: 0.000835  min_lr: 0.000001  loss: 0.4502 (0.4702)  class_acc: 0.7857 (0.7817)  loss_scale: 16384.0000 (18666.8508)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.64 GB / 503.51 GB
Epoch: [1]  [820/893]  eta: 0:01:47  lr: 0.000840  min_lr: 0.000001  loss: 0.4346 (0.4701)  class_acc: 0.7679 (0.7818)  loss_scale: 16384.0000 (18639.0451)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.54 GB / 503.51 GB
Epoch: [1]  [830/893]  eta: 0:01:33  lr: 0.000845  min_lr: 0.000001  loss: 0.4502 (0.4702)  class_acc: 0.7857 (0.7817)  loss_scale: 16384.0000 (18611.9085)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0004  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.41 GB / 503.51 GB
Epoch: [1]  [840/893]  eta: 0:01:18  lr: 0.000850  min_lr: 0.000001  loss: 0.4517 (0.4701)  class_acc: 0.8036 (0.7818)  loss_scale: 16384.0000 (18585.4174)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0004  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.33 GB / 503.51 GB
Epoch: [1]  [850/893]  eta: 0:01:03  lr: 0.000855  min_lr: 0.000001  loss: 0.4778 (0.4705)  class_acc: 0.8036 (0.7816)  loss_scale: 16384.0000 (18559.5488)  weight_decay: 0.0500 (0.0500)  time: 1.4673  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.08 GB / 503.51 GB
Epoch: [1]  [860/893]  eta: 0:00:48  lr: 0.000859  min_lr: 0.000001  loss: 0.4814 (0.4703)  class_acc: 0.7857 (0.7817)  loss_scale: 16384.0000 (18534.2811)  weight_decay: 0.0500 (0.0500)  time: 1.4541  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.50 GB / 503.51 GB
Epoch: [1]  [870/893]  eta: 0:00:33  lr: 0.000864  min_lr: 0.000001  loss: 0.4797 (0.4702)  class_acc: 0.7857 (0.7817)  loss_scale: 16384.0000 (18509.5936)  weight_decay: 0.0500 (0.0500)  time: 1.4406  data: 0.0002  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.83 GB / 503.51 GB
Epoch: [1]  [880/893]  eta: 0:00:19  lr: 0.000869  min_lr: 0.000001  loss: 0.4470 (0.4697)  class_acc: 0.7857 (0.7818)  loss_scale: 16384.0000 (18485.4665)  weight_decay: 0.0500 (0.0500)  time: 1.4424  data: 0.0002  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.03 GB / 503.51 GB
Epoch: [1]  [890/893]  eta: 0:00:04  lr: 0.000874  min_lr: 0.000001  loss: 0.4360 (0.4698)  class_acc: 0.8036 (0.7820)  loss_scale: 16384.0000 (18461.8810)  weight_decay: 0.0500 (0.0500)  time: 1.4393  data: 0.0001  max mem: 31081
Epoch: [1]  [892/893]  eta: 0:00:01  lr: 0.000875  min_lr: 0.000001  loss: 0.4658 (0.4698)  class_acc: 0.8036 (0.7820)  loss_scale: 16384.0000 (18459.5516)  weight_decay: 0.0500 (0.0500)  time: 1.3845  data: 0.0001  max mem: 31081
Epoch: [1] Total time: 0:21:58 (1.4761 s / it)
Averaged stats: lr: 0.000875  min_lr: 0.000001  loss: 0.4658 (0.4698)  class_acc: 0.8036 (0.7820)  loss_scale: 16384.0000 (18459.5516)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:18:29  loss: 0.4713 (0.4713)  acc: 78.5714 (78.5714)  time: 11.4137  data: 10.8929  max mem: 31081
Val:  [ 10/728]  eta: 0:18:26  loss: 0.3298 (0.4150)  acc: 83.3333 (81.7100)  time: 1.5411  data: 1.0174  max mem: 31081
Val:  [ 20/728]  eta: 0:14:06  loss: 0.3864 (0.4230)  acc: 80.9524 (80.8957)  time: 0.6847  data: 0.1621  max mem: 31081
Val:  [ 30/728]  eta: 0:12:44  loss: 0.4414 (0.4412)  acc: 79.7619 (79.3779)  time: 0.8498  data: 0.3275  max mem: 31081
Val:  [ 40/728]  eta: 0:11:47  loss: 0.4484 (0.4705)  acc: 78.5714 (77.7294)  time: 0.8522  data: 0.3279  max mem: 31081
Val:  [ 50/728]  eta: 0:11:11  loss: 0.4484 (0.4578)  acc: 77.3810 (78.1746)  time: 0.8299  data: 0.3077  max mem: 31081
Val:  [ 60/728]  eta: 0:10:19  loss: 0.4401 (0.4596)  acc: 77.3810 (78.2397)  time: 0.7216  data: 0.2007  max mem: 31081
Val:  [ 70/728]  eta: 0:09:49  loss: 0.4156 (0.4549)  acc: 80.9524 (78.6217)  time: 0.6537  data: 0.1327  max mem: 31081
Val:  [ 80/728]  eta: 0:09:35  loss: 0.4402 (0.4692)  acc: 78.5714 (77.6749)  time: 0.7690  data: 0.2482  max mem: 31081
Val:  [ 90/728]  eta: 0:09:21  loss: 0.4759 (0.4787)  acc: 72.6190 (77.2763)  time: 0.8222  data: 0.3016  max mem: 31081
Val:  [100/728]  eta: 0:09:11  loss: 0.4039 (0.4710)  acc: 82.1429 (77.9114)  time: 0.8334  data: 0.3132  max mem: 31081
Val:  [110/728]  eta: 0:09:01  loss: 0.3877 (0.4670)  acc: 82.1429 (78.1746)  time: 0.8628  data: 0.3404  max mem: 31081
Val:  [120/728]  eta: 0:08:46  loss: 0.3681 (0.4617)  acc: 84.5238 (78.4730)  time: 0.8120  data: 0.2903  max mem: 31081
Val:  [130/728]  eta: 0:08:25  loss: 0.3637 (0.4638)  acc: 83.3333 (78.2897)  time: 0.6754  data: 0.1529  max mem: 31081
Val:  [140/728]  eta: 0:08:14  loss: 0.4803 (0.4691)  acc: 71.4286 (77.6765)  time: 0.6899  data: 0.1671  max mem: 31081
Val:  [150/728]  eta: 0:08:01  loss: 0.4414 (0.4672)  acc: 73.8095 (77.8303)  time: 0.7509  data: 0.2278  max mem: 31081
Val:  [160/728]  eta: 0:07:52  loss: 0.4299 (0.4691)  acc: 80.9524 (77.8246)  time: 0.7674  data: 0.2445  max mem: 31081
Val:  [170/728]  eta: 0:07:43  loss: 0.4126 (0.4683)  acc: 79.7619 (77.8683)  time: 0.8100  data: 0.2852  max mem: 31081
Val:  [180/728]  eta: 0:07:33  loss: 0.4126 (0.4686)  acc: 79.7619 (77.8874)  time: 0.7819  data: 0.2568  max mem: 31081
Val:  [190/728]  eta: 0:07:18  loss: 0.4651 (0.4689)  acc: 75.0000 (77.8173)  time: 0.6774  data: 0.1580  max mem: 31081
Val:  [200/728]  eta: 0:07:12  loss: 0.4124 (0.4668)  acc: 78.5714 (77.8370)  time: 0.7456  data: 0.2267  max mem: 31081
Val:  [210/728]  eta: 0:07:04  loss: 0.4124 (0.4684)  acc: 79.7619 (77.9621)  time: 0.8615  data: 0.3422  max mem: 31081
Val:  [220/728]  eta: 0:06:58  loss: 0.4414 (0.4677)  acc: 79.7619 (77.9681)  time: 0.8685  data: 0.3462  max mem: 31081
Val:  [230/728]  eta: 0:06:48  loss: 0.4414 (0.4692)  acc: 76.1905 (77.8499)  time: 0.8435  data: 0.3187  max mem: 31081
Val:  [240/728]  eta: 0:06:38  loss: 0.3698 (0.4673)  acc: 80.9524 (77.9441)  time: 0.7308  data: 0.2102  max mem: 31081
Val:  [250/728]  eta: 0:06:26  loss: 0.3994 (0.4670)  acc: 76.1905 (77.9975)  time: 0.6620  data: 0.1423  max mem: 31081
Val:  [260/728]  eta: 0:06:19  loss: 0.4219 (0.4665)  acc: 78.5714 (78.0150)  time: 0.7570  data: 0.2322  max mem: 31081
Val:  [270/728]  eta: 0:06:11  loss: 0.4015 (0.4658)  acc: 80.9524 (78.0750)  time: 0.8425  data: 0.3157  max mem: 31081
Val:  [280/728]  eta: 0:06:06  loss: 0.4604 (0.4662)  acc: 77.3810 (78.1647)  time: 0.9092  data: 0.3863  max mem: 31081
Val:  [290/728]  eta: 0:05:58  loss: 0.4525 (0.4661)  acc: 77.3810 (78.1582)  time: 0.9121  data: 0.3922  max mem: 31081
Val:  [300/728]  eta: 0:05:45  loss: 0.4524 (0.4657)  acc: 77.3810 (78.1601)  time: 0.6681  data: 0.1479  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.4627 (0.4678)  acc: 77.3810 (78.0738)  time: 0.6742  data: 0.1531  max mem: 31081
Val:  [320/728]  eta: 0:05:29  loss: 0.4715 (0.4669)  acc: 78.5714 (78.1931)  time: 0.8050  data: 0.2843  max mem: 31081
Val:  [330/728]  eta: 0:05:22  loss: 0.4065 (0.4653)  acc: 80.9524 (78.2945)  time: 0.8401  data: 0.3213  max mem: 31081
Val:  [340/728]  eta: 0:05:15  loss: 0.4177 (0.4628)  acc: 80.9524 (78.4667)  time: 0.9097  data: 0.3889  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3801 (0.4625)  acc: 80.9524 (78.4120)  time: 0.8554  data: 0.3313  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.3801 (0.4604)  acc: 83.3333 (78.5681)  time: 0.6523  data: 0.1324  max mem: 31081
Val:  [370/728]  eta: 0:04:49  loss: 0.4319 (0.4617)  acc: 83.3333 (78.5458)  time: 0.7041  data: 0.1851  max mem: 31081
Val:  [380/728]  eta: 0:04:41  loss: 0.4757 (0.4626)  acc: 77.3810 (78.5214)  time: 0.8680  data: 0.3448  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.3761 (0.4593)  acc: 82.1429 (78.7115)  time: 0.8397  data: 0.3154  max mem: 31081
Val:  [400/728]  eta: 0:04:26  loss: 0.3529 (0.4586)  acc: 82.1429 (78.7406)  time: 0.8721  data: 0.3470  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.4363 (0.4569)  acc: 79.7619 (78.8466)  time: 0.8759  data: 0.3529  max mem: 31081
Val:  [420/728]  eta: 0:04:08  loss: 0.4618 (0.4583)  acc: 77.3810 (78.7496)  time: 0.6838  data: 0.1616  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.5595 (0.4598)  acc: 71.4286 (78.6294)  time: 0.6983  data: 0.1730  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.4292 (0.4595)  acc: 78.5714 (78.6416)  time: 0.8411  data: 0.3160  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4152 (0.4615)  acc: 79.7619 (78.4949)  time: 0.8366  data: 0.3136  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.4701 (0.4634)  acc: 72.6190 (78.4242)  time: 0.8438  data: 0.3210  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.4243 (0.4617)  acc: 82.1429 (78.5537)  time: 0.8039  data: 0.2814  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.4116 (0.4610)  acc: 82.1429 (78.6011)  time: 0.6516  data: 0.1309  max mem: 31081
Val:  [490/728]  eta: 0:03:11  loss: 0.4513 (0.4606)  acc: 76.1905 (78.6029)  time: 0.6917  data: 0.1704  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.4513 (0.4601)  acc: 76.1905 (78.6047)  time: 0.8486  data: 0.3259  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3710 (0.4589)  acc: 83.3333 (78.6623)  time: 0.8900  data: 0.3570  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4137 (0.4603)  acc: 80.9524 (78.5806)  time: 0.8585  data: 0.3259  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.4853 (0.4599)  acc: 73.8095 (78.6140)  time: 0.7379  data: 0.2146  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.4138 (0.4617)  acc: 78.5714 (78.5208)  time: 0.6149  data: 0.0902  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.4822 (0.4622)  acc: 75.0000 (78.4699)  time: 0.6714  data: 0.1456  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.4394 (0.4616)  acc: 76.1905 (78.5120)  time: 0.7606  data: 0.2340  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.3922 (0.4629)  acc: 80.9524 (78.4422)  time: 0.8063  data: 0.2815  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4460 (0.4629)  acc: 80.9524 (78.4485)  time: 0.8428  data: 0.3184  max mem: 31081
Val:  [590/728]  eta: 0:01:50  loss: 0.4460 (0.4638)  acc: 80.9524 (78.3982)  time: 0.7297  data: 0.2044  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4050 (0.4642)  acc: 78.5714 (78.3595)  time: 0.6780  data: 0.1567  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4359 (0.4653)  acc: 78.5714 (78.2987)  time: 0.6594  data: 0.1363  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4326 (0.4652)  acc: 78.5714 (78.3107)  time: 0.7665  data: 0.2404  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4326 (0.4650)  acc: 79.7619 (78.3111)  time: 0.8565  data: 0.3330  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.4476 (0.4638)  acc: 80.9524 (78.3913)  time: 0.8087  data: 0.2856  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4279 (0.4633)  acc: 82.1429 (78.4196)  time: 0.7912  data: 0.2688  max mem: 31081
Val:  [660/728]  eta: 0:00:54  loss: 0.4578 (0.4638)  acc: 77.3810 (78.3787)  time: 0.7973  data: 0.2756  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4876 (0.4647)  acc: 76.1905 (78.3266)  time: 0.6846  data: 0.1616  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4125 (0.4641)  acc: 77.3810 (78.3494)  time: 0.6589  data: 0.1327  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3809 (0.4631)  acc: 83.3333 (78.4129)  time: 0.8419  data: 0.3160  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4000 (0.4628)  acc: 83.3333 (78.4254)  time: 0.8451  data: 0.3198  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.3854 (0.4622)  acc: 80.9524 (78.4676)  time: 0.8259  data: 0.3014  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4111 (0.4631)  acc: 77.3810 (78.4080)  time: 0.8079  data: 0.2936  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4680 (0.4641)  acc: 75.0000 (78.3685)  time: 0.7833  data: 0.2936  max mem: 31081
Val: Total time: 0:09:34 (0.7897 s / it)
* Acc@1 78.368 AP 0.7935552000999451 loss 0.464
Accuracy of the network on the 61096 val videos: 78.4%
[2025-03-10 20:54:43,279] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestauroc is about to be saved!
[2025-03-10 20:54:43,282] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestauroc/mp_rank_00_model_states.pt
[2025-03-10 20:54:43,282] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestauroc/mp_rank_00_model_states.pt...
[2025-03-10 20:54:43,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestauroc/mp_rank_00_model_states.pt.
[2025-03-10 20:54:43,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestauroc is ready now!
[2025-03-10 20:54:43,537] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestap is about to be saved!
[2025-03-10 20:54:43,539] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt
[2025-03-10 20:54:43,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt...
[2025-03-10 20:54:43,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt.
[2025-03-10 20:54:43,782] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestap is ready now!
[2025-03-10 20:54:43,783] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestacc is about to be saved!
[2025-03-10 20:54:43,785] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt
[2025-03-10 20:54:43,785] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt...
[2025-03-10 20:54:44,026] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt.
[2025-03-10 20:54:44,026] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestacc is ready now!
Max accuracy: 0.85%
	training another epoch...
[2025-03-10 20:54:57,824] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 20:54:57,825] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.50 GB / 503.51 GB
Epoch: [2]  [  0/893]  eta: 3:19:44  lr: 0.000875  min_lr: 0.000001  loss: 0.3936 (0.3936)  class_acc: 0.8571 (0.8571)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 13.4207  data: 12.1265  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.91 GB / 503.51 GB
Epoch: [2]  [ 10/893]  eta: 0:37:54  lr: 0.000880  min_lr: 0.000001  loss: 0.4351 (0.4478)  class_acc: 0.7857 (0.8068)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5755  data: 1.1029  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.32 GB / 503.51 GB
Epoch: [2]  [ 20/893]  eta: 0:29:55  lr: 0.000885  min_lr: 0.000001  loss: 0.4360 (0.4458)  class_acc: 0.7857 (0.7993)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4880  data: 0.0005  max mem: 31081
[2025-03-10 20:55:30,583] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 1806
[2025-03-10 20:55:30,583] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-03-10 20:55:30,584] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.74 GB / 503.51 GB
Epoch: [2]  [ 30/893]  eta: 0:27:01  lr: 0.000890  min_lr: 0.000001  loss: 0.4360 (0.4379)  class_acc: 0.8036 (0.8030)  loss_scale: 32768.0000 (28011.3548)  weight_decay: 0.0500 (0.0500)  time: 1.4958  data: 0.0005  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.44 GB / 503.51 GB
Epoch: [2]  [ 40/893]  eta: 0:25:23  lr: 0.000895  min_lr: 0.000001  loss: 0.4424 (0.4443)  class_acc: 0.7857 (0.7988)  loss_scale: 16384.0000 (25175.4146)  weight_decay: 0.0500 (0.0500)  time: 1.5011  data: 0.0006  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.01 GB / 503.51 GB
Epoch: [2]  [ 50/893]  eta: 0:24:15  lr: 0.000900  min_lr: 0.000001  loss: 0.4485 (0.4457)  class_acc: 0.7679 (0.7980)  loss_scale: 16384.0000 (23451.6078)  weight_decay: 0.0500 (0.0500)  time: 1.4901  data: 0.0006  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.01 GB / 503.51 GB
Epoch: [2]  [ 60/893]  eta: 0:23:23  lr: 0.000905  min_lr: 0.000001  loss: 0.4263 (0.4425)  class_acc: 0.8036 (0.8015)  loss_scale: 16384.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  time: 1.4785  data: 0.0004  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.80 GB / 503.51 GB
Epoch: [2]  [ 70/893]  eta: 0:22:41  lr: 0.000910  min_lr: 0.000001  loss: 0.4365 (0.4436)  class_acc: 0.8036 (0.8016)  loss_scale: 16384.0000 (21460.7324)  weight_decay: 0.0500 (0.0500)  time: 1.4711  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.34 GB / 503.51 GB
Epoch: [2]  [ 80/893]  eta: 0:22:05  lr: 0.000914  min_lr: 0.000001  loss: 0.4404 (0.4423)  class_acc: 0.8036 (0.8018)  loss_scale: 16384.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.65 GB / 503.51 GB
Epoch: [2]  [ 90/893]  eta: 0:21:33  lr: 0.000919  min_lr: 0.000001  loss: 0.4341 (0.4439)  class_acc: 0.7857 (0.7996)  loss_scale: 16384.0000 (20344.9670)  weight_decay: 0.0500 (0.0500)  time: 1.4554  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.04 GB / 503.51 GB
Epoch: [2]  [100/893]  eta: 0:21:05  lr: 0.000924  min_lr: 0.000001  loss: 0.4526 (0.4429)  class_acc: 0.7857 (0.7999)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  time: 1.4567  data: 0.0004  max mem: 31081
[2025-03-10 20:57:31,414] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 1888
[2025-03-10 20:57:31,414] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 20:57:31,414] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.31 GB / 503.51 GB
Epoch: [2]  [110/893]  eta: 0:20:39  lr: 0.000929  min_lr: 0.000001  loss: 0.4526 (0.4438)  class_acc: 0.7679 (0.7987)  loss_scale: 16384.0000 (19114.6667)  weight_decay: 0.0500 (0.0500)  time: 1.4554  data: 0.0004  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.74 GB / 503.51 GB
Epoch: [2]  [120/893]  eta: 0:20:15  lr: 0.000934  min_lr: 0.000001  loss: 0.4138 (0.4439)  class_acc: 0.7857 (0.7981)  loss_scale: 8192.0000 (18211.9669)  weight_decay: 0.0500 (0.0500)  time: 1.4550  data: 0.0004  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.50 GB / 503.51 GB
Epoch: [2]  [130/893]  eta: 0:19:53  lr: 0.000939  min_lr: 0.000001  loss: 0.4180 (0.4424)  class_acc: 0.8036 (0.7993)  loss_scale: 8192.0000 (17447.0840)  weight_decay: 0.0500 (0.0500)  time: 1.4573  data: 0.0004  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.18 GB / 503.51 GB
Epoch: [2]  [140/893]  eta: 0:19:32  lr: 0.000944  min_lr: 0.000001  loss: 0.4180 (0.4417)  class_acc: 0.8036 (0.7994)  loss_scale: 8192.0000 (16790.6950)  weight_decay: 0.0500 (0.0500)  time: 1.4616  data: 0.0002  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.47 GB / 503.51 GB
Epoch: [2]  [150/893]  eta: 0:19:12  lr: 0.000949  min_lr: 0.000001  loss: 0.4304 (0.4446)  class_acc: 0.7857 (0.7978)  loss_scale: 8192.0000 (16221.2450)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.51 GB / 503.51 GB
Epoch: [2]  [160/893]  eta: 0:18:52  lr: 0.000954  min_lr: 0.000001  loss: 0.4680 (0.4458)  class_acc: 0.7500 (0.7965)  loss_scale: 8192.0000 (15722.5342)  weight_decay: 0.0500 (0.0500)  time: 1.4587  data: 0.0004  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.94 GB / 503.51 GB
Epoch: [2]  [170/893]  eta: 0:18:33  lr: 0.000959  min_lr: 0.000001  loss: 0.4678 (0.4478)  class_acc: 0.7857 (0.7962)  loss_scale: 8192.0000 (15282.1520)  weight_decay: 0.0500 (0.0500)  time: 1.4572  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.90 GB / 503.51 GB
Epoch: [2]  [180/893]  eta: 0:18:15  lr: 0.000964  min_lr: 0.000001  loss: 0.4358 (0.4472)  class_acc: 0.8214 (0.7968)  loss_scale: 8192.0000 (14890.4309)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.80 GB / 503.51 GB
Epoch: [2]  [190/893]  eta: 0:17:56  lr: 0.000968  min_lr: 0.000001  loss: 0.4194 (0.4458)  class_acc: 0.8214 (0.7972)  loss_scale: 8192.0000 (14539.7277)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.46 GB / 503.51 GB
Epoch: [2]  [200/893]  eta: 0:17:39  lr: 0.000973  min_lr: 0.000001  loss: 0.4250 (0.4462)  class_acc: 0.7857 (0.7973)  loss_scale: 8192.0000 (14223.9204)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.81 GB / 503.51 GB
Epoch: [2]  [210/893]  eta: 0:17:22  lr: 0.000978  min_lr: 0.000001  loss: 0.4521 (0.4465)  class_acc: 0.7857 (0.7976)  loss_scale: 8192.0000 (13938.0474)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
[2025-03-10 21:00:13,696] [INFO] [logging.py:129:log_dist] [Rank 0] step=2000, skipped=7, lr=[1.2808245358116164e-06, 1.2808245358116164e-06, 2.1347075596860273e-06, 2.1347075596860273e-06, 3.557845932810046e-06, 3.557845932810046e-06, 5.929743221350076e-06, 5.929743221350076e-06, 9.882905368916796e-06, 9.882905368916796e-06, 1.6471508948194656e-05, 1.6471508948194656e-05, 2.7452514913657767e-05, 2.7452514913657767e-05, 4.575419152276294e-05, 4.575419152276294e-05, 7.625698587127158e-05, 7.625698587127158e-05, 0.00012709497645211932, 0.00012709497645211932, 0.00021182496075353217, 0.00021182496075353217, 0.000353041601255887, 0.000353041601255887, 0.0005884026687598117, 0.0005884026687598117, 0.000980671114599686, 0.000980671114599686], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-10 21:00:13,697] [INFO] [timer.py:264:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=61.38227650769898, CurrSamplesPerSec=61.70895661651598, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.12 GB / 503.51 GB
Epoch: [2]  [220/893]  eta: 0:17:05  lr: 0.000983  min_lr: 0.000001  loss: 0.4629 (0.4474)  class_acc: 0.7857 (0.7973)  loss_scale: 8192.0000 (13678.0452)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0002  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.94 GB / 503.51 GB
Epoch: [2]  [230/893]  eta: 0:16:48  lr: 0.000988  min_lr: 0.000001  loss: 0.4587 (0.4473)  class_acc: 0.8036 (0.7976)  loss_scale: 8192.0000 (13440.5541)  weight_decay: 0.0500 (0.0500)  time: 1.4652  data: 0.0002  max mem: 31081
[2025-03-10 21:00:40,013] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:00:40,013] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.28 GB / 503.51 GB
Epoch: [2]  [240/893]  eta: 0:16:31  lr: 0.000993  min_lr: 0.000001  loss: 0.4355 (0.4475)  class_acc: 0.7857 (0.7973)  loss_scale: 8192.0000 (13494.7054)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.13 GB / 503.51 GB
Epoch: [2]  [250/893]  eta: 0:16:14  lr: 0.000998  min_lr: 0.000001  loss: 0.4324 (0.4463)  class_acc: 0.8036 (0.7983)  loss_scale: 16384.0000 (13609.8167)  weight_decay: 0.0500 (0.0500)  time: 1.4571  data: 0.0003  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.26 GB / 503.51 GB
Epoch: [2]  [260/893]  eta: 0:15:58  lr: 0.001003  min_lr: 0.000001  loss: 0.4285 (0.4456)  class_acc: 0.8214 (0.7985)  loss_scale: 16384.0000 (13716.1073)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.66 GB / 503.51 GB
Epoch: [2]  [270/893]  eta: 0:15:41  lr: 0.001008  min_lr: 0.000001  loss: 0.4382 (0.4462)  class_acc: 0.8036 (0.7986)  loss_scale: 16384.0000 (13814.5535)  weight_decay: 0.0500 (0.0500)  time: 1.4568  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.54 GB / 503.51 GB
Epoch: [2]  [280/893]  eta: 0:15:25  lr: 0.001013  min_lr: 0.000001  loss: 0.4697 (0.4469)  class_acc: 0.7857 (0.7980)  loss_scale: 16384.0000 (13905.9929)  weight_decay: 0.0500 (0.0500)  time: 1.4569  data: 0.0004  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.18 GB / 503.51 GB
Epoch: [2]  [290/893]  eta: 0:15:09  lr: 0.001017  min_lr: 0.000001  loss: 0.4702 (0.4480)  class_acc: 0.7679 (0.7968)  loss_scale: 16384.0000 (13991.1478)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.42 GB / 503.51 GB
Epoch: [2]  [300/893]  eta: 0:14:53  lr: 0.001022  min_lr: 0.000001  loss: 0.4697 (0.4483)  class_acc: 0.7679 (0.7966)  loss_scale: 16384.0000 (14070.6445)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0002  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.68 GB / 503.51 GB
Epoch: [2]  [310/893]  eta: 0:14:37  lr: 0.001027  min_lr: 0.000001  loss: 0.4302 (0.4484)  class_acc: 0.8036 (0.7965)  loss_scale: 16384.0000 (14145.0289)  weight_decay: 0.0500 (0.0500)  time: 1.4573  data: 0.0002  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.16 GB / 503.51 GB
Epoch: [2]  [320/893]  eta: 0:14:21  lr: 0.001032  min_lr: 0.000001  loss: 0.4302 (0.4477)  class_acc: 0.8036 (0.7972)  loss_scale: 16384.0000 (14214.7788)  weight_decay: 0.0500 (0.0500)  time: 1.4552  data: 0.0002  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.83 GB / 503.51 GB
Epoch: [2]  [330/893]  eta: 0:14:05  lr: 0.001037  min_lr: 0.000001  loss: 0.4521 (0.4482)  class_acc: 0.7857 (0.7973)  loss_scale: 16384.0000 (14280.3142)  weight_decay: 0.0500 (0.0500)  time: 1.4568  data: 0.0002  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.17 GB / 503.51 GB
Epoch: [2]  [340/893]  eta: 0:13:49  lr: 0.001042  min_lr: 0.000001  loss: 0.4363 (0.4473)  class_acc: 0.8036 (0.7978)  loss_scale: 16384.0000 (14342.0059)  weight_decay: 0.0500 (0.0500)  time: 1.4581  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.51 GB / 503.51 GB
Epoch: [2]  [350/893]  eta: 0:13:34  lr: 0.001047  min_lr: 0.000001  loss: 0.4292 (0.4474)  class_acc: 0.8036 (0.7975)  loss_scale: 16384.0000 (14400.1823)  weight_decay: 0.0500 (0.0500)  time: 1.4602  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.65 GB / 503.51 GB
Epoch: [2]  [360/893]  eta: 0:13:18  lr: 0.001052  min_lr: 0.000001  loss: 0.4407 (0.4474)  class_acc: 0.7857 (0.7968)  loss_scale: 16384.0000 (14455.1357)  weight_decay: 0.0500 (0.0500)  time: 1.4574  data: 0.0004  max mem: 31081
[2025-03-10 21:03:46,746] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:03:46,746] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-03-10 21:03:51,071] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 2148
[2025-03-10 21:03:51,071] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-03-10 21:03:51,071] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.27 GB / 503.51 GB
Epoch: [2]  [370/893]  eta: 0:13:02  lr: 0.001057  min_lr: 0.000001  loss: 0.4324 (0.4471)  class_acc: 0.7857 (0.7968)  loss_scale: 16384.0000 (14639.6119)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.63 GB / 503.51 GB
Epoch: [2]  [380/893]  eta: 0:12:47  lr: 0.001062  min_lr: 0.000001  loss: 0.4272 (0.4466)  class_acc: 0.8036 (0.7969)  loss_scale: 16384.0000 (14685.3963)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0002  max mem: 31081
[2025-03-10 21:04:18,901] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 2167
[2025-03-10 21:04:18,901] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 21:04:18,901] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.45 GB / 503.51 GB
Epoch: [2]  [390/893]  eta: 0:12:32  lr: 0.001067  min_lr: 0.000001  loss: 0.4272 (0.4470)  class_acc: 0.8036 (0.7968)  loss_scale: 16384.0000 (14561.2276)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0004  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.93 GB / 503.51 GB
Epoch: [2]  [400/893]  eta: 0:12:16  lr: 0.001071  min_lr: 0.000001  loss: 0.4302 (0.4470)  class_acc: 0.8036 (0.7968)  loss_scale: 8192.0000 (14402.3940)  weight_decay: 0.0500 (0.0500)  time: 1.4611  data: 0.0004  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.41 GB / 503.51 GB
Epoch: [2]  [410/893]  eta: 0:12:01  lr: 0.001076  min_lr: 0.000001  loss: 0.4021 (0.4460)  class_acc: 0.8036 (0.7972)  loss_scale: 8192.0000 (14251.2895)  weight_decay: 0.0500 (0.0500)  time: 1.4585  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.33 GB / 503.51 GB
Epoch: [2]  [420/893]  eta: 0:11:46  lr: 0.001081  min_lr: 0.000001  loss: 0.4050 (0.4461)  class_acc: 0.8036 (0.7971)  loss_scale: 8192.0000 (14107.3634)  weight_decay: 0.0500 (0.0500)  time: 1.4554  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.99 GB / 503.51 GB
Epoch: [2]  [430/893]  eta: 0:11:30  lr: 0.001086  min_lr: 0.000001  loss: 0.4312 (0.4459)  class_acc: 0.7857 (0.7969)  loss_scale: 8192.0000 (13970.1160)  weight_decay: 0.0500 (0.0500)  time: 1.4584  data: 0.0003  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.98 GB / 503.51 GB
Epoch: [2]  [440/893]  eta: 0:11:15  lr: 0.001091  min_lr: 0.000001  loss: 0.4348 (0.4459)  class_acc: 0.7679 (0.7966)  loss_scale: 8192.0000 (13839.0930)  weight_decay: 0.0500 (0.0500)  time: 1.4594  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.93 GB / 503.51 GB
Epoch: [2]  [450/893]  eta: 0:11:00  lr: 0.001096  min_lr: 0.000001  loss: 0.4148 (0.4455)  class_acc: 0.7857 (0.7969)  loss_scale: 8192.0000 (13713.8803)  weight_decay: 0.0500 (0.0500)  time: 1.4599  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.26 GB / 503.51 GB
Epoch: [2]  [460/893]  eta: 0:10:45  lr: 0.001101  min_lr: 0.000001  loss: 0.4148 (0.4454)  class_acc: 0.8036 (0.7970)  loss_scale: 8192.0000 (13594.0998)  weight_decay: 0.0500 (0.0500)  time: 1.4619  data: 0.0002  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.26 GB / 503.51 GB
Epoch: [2]  [470/893]  eta: 0:10:30  lr: 0.001106  min_lr: 0.000001  loss: 0.4565 (0.4460)  class_acc: 0.7857 (0.7964)  loss_scale: 8192.0000 (13479.4055)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.38 GB / 503.51 GB
Epoch: [2]  [480/893]  eta: 0:10:14  lr: 0.001111  min_lr: 0.000001  loss: 0.4209 (0.4455)  class_acc: 0.7857 (0.7969)  loss_scale: 8192.0000 (13369.4802)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.16 GB / 503.51 GB
Epoch: [2]  [490/893]  eta: 0:09:59  lr: 0.001116  min_lr: 0.000001  loss: 0.4463 (0.4456)  class_acc: 0.7679 (0.7965)  loss_scale: 8192.0000 (13264.0326)  weight_decay: 0.0500 (0.0500)  time: 1.4514  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.89 GB / 503.51 GB
Epoch: [2]  [500/893]  eta: 0:09:44  lr: 0.001120  min_lr: 0.000001  loss: 0.4255 (0.4451)  class_acc: 0.8036 (0.7973)  loss_scale: 8192.0000 (13162.7944)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.83 GB / 503.51 GB
Epoch: [2]  [510/893]  eta: 0:09:29  lr: 0.001125  min_lr: 0.000001  loss: 0.4399 (0.4467)  class_acc: 0.8214 (0.7967)  loss_scale: 8192.0000 (13065.5186)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
[2025-03-10 21:07:27,274] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:07:27,274] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.49 GB / 503.51 GB
Epoch: [2]  [520/893]  eta: 0:09:14  lr: 0.001130  min_lr: 0.000001  loss: 0.4866 (0.4468)  class_acc: 0.7857 (0.7968)  loss_scale: 8192.0000 (13113.4894)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.96 GB / 503.51 GB
Epoch: [2]  [530/893]  eta: 0:08:59  lr: 0.001135  min_lr: 0.000001  loss: 0.4583 (0.4471)  class_acc: 0.7857 (0.7962)  loss_scale: 16384.0000 (13175.0810)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0004  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.78 GB / 503.51 GB
Epoch: [2]  [540/893]  eta: 0:08:44  lr: 0.001140  min_lr: 0.000001  loss: 0.4583 (0.4470)  class_acc: 0.8036 (0.7963)  loss_scale: 16384.0000 (13234.3956)  weight_decay: 0.0500 (0.0500)  time: 1.4582  data: 0.0004  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.13 GB / 503.51 GB
Epoch: [2]  [550/893]  eta: 0:08:29  lr: 0.001145  min_lr: 0.000001  loss: 0.4338 (0.4468)  class_acc: 0.8214 (0.7963)  loss_scale: 16384.0000 (13291.5572)  weight_decay: 0.0500 (0.0500)  time: 1.4558  data: 0.0002  max mem: 31081
[2025-03-10 21:08:31,503] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 2340
[2025-03-10 21:08:31,503] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 21:08:31,503] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.70 GB / 503.51 GB
Epoch: [2]  [560/893]  eta: 0:08:14  lr: 0.001150  min_lr: 0.000002  loss: 0.4338 (0.4468)  class_acc: 0.8036 (0.7965)  loss_scale: 16384.0000 (13273.6684)  weight_decay: 0.0500 (0.0500)  time: 1.4519  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.89 GB / 503.51 GB
Epoch: [2]  [570/893]  eta: 0:07:59  lr: 0.001155  min_lr: 0.000002  loss: 0.4138 (0.4462)  class_acc: 0.8036 (0.7968)  loss_scale: 8192.0000 (13184.6725)  weight_decay: 0.0500 (0.0500)  time: 1.4477  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.92 GB / 503.51 GB
Epoch: [2]  [580/893]  eta: 0:07:44  lr: 0.001160  min_lr: 0.000002  loss: 0.4119 (0.4456)  class_acc: 0.8036 (0.7968)  loss_scale: 8192.0000 (13098.7401)  weight_decay: 0.0500 (0.0500)  time: 1.4567  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.02 GB / 503.51 GB
Epoch: [2]  [590/893]  eta: 0:07:29  lr: 0.001165  min_lr: 0.000002  loss: 0.4001 (0.4451)  class_acc: 0.8036 (0.7968)  loss_scale: 8192.0000 (13015.7157)  weight_decay: 0.0500 (0.0500)  time: 1.4612  data: 0.0004  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.65 GB / 503.51 GB
Epoch: [2]  [600/893]  eta: 0:07:14  lr: 0.001170  min_lr: 0.000002  loss: 0.4001 (0.4450)  class_acc: 0.8036 (0.7970)  loss_scale: 8192.0000 (12935.4542)  weight_decay: 0.0500 (0.0500)  time: 1.4577  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.36 GB / 503.51 GB
Epoch: [2]  [610/893]  eta: 0:06:59  lr: 0.001174  min_lr: 0.000002  loss: 0.4607 (0.4448)  class_acc: 0.8036 (0.7973)  loss_scale: 8192.0000 (12857.8200)  weight_decay: 0.0500 (0.0500)  time: 1.4555  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.51 GB / 503.51 GB
Epoch: [2]  [620/893]  eta: 0:06:44  lr: 0.001179  min_lr: 0.000002  loss: 0.4402 (0.4448)  class_acc: 0.8036 (0.7975)  loss_scale: 8192.0000 (12782.6860)  weight_decay: 0.0500 (0.0500)  time: 1.4582  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.19 GB / 503.51 GB
Epoch: [2]  [630/893]  eta: 0:06:29  lr: 0.001184  min_lr: 0.000002  loss: 0.4355 (0.4451)  class_acc: 0.7857 (0.7974)  loss_scale: 8192.0000 (12709.9334)  weight_decay: 0.0500 (0.0500)  time: 1.4589  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.99 GB / 503.51 GB
Epoch: [2]  [640/893]  eta: 0:06:14  lr: 0.001189  min_lr: 0.000002  loss: 0.4297 (0.4452)  class_acc: 0.7857 (0.7973)  loss_scale: 8192.0000 (12639.4509)  weight_decay: 0.0500 (0.0500)  time: 1.4583  data: 0.0002  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.05 GB / 503.51 GB
Epoch: [2]  [650/893]  eta: 0:05:59  lr: 0.001194  min_lr: 0.000002  loss: 0.4429 (0.4460)  class_acc: 0.8036 (0.7970)  loss_scale: 8192.0000 (12571.1336)  weight_decay: 0.0500 (0.0500)  time: 1.4572  data: 0.0004  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.55 GB / 503.51 GB
Epoch: [2]  [660/893]  eta: 0:05:44  lr: 0.001199  min_lr: 0.000002  loss: 0.4487 (0.4461)  class_acc: 0.7857 (0.7968)  loss_scale: 8192.0000 (12504.8835)  weight_decay: 0.0500 (0.0500)  time: 1.4584  data: 0.0004  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.79 GB / 503.51 GB
Epoch: [2]  [670/893]  eta: 0:05:30  lr: 0.001204  min_lr: 0.000002  loss: 0.4504 (0.4463)  class_acc: 0.7857 (0.7965)  loss_scale: 8192.0000 (12440.6080)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0002  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.08 GB / 503.51 GB
Epoch: [2]  [680/893]  eta: 0:05:15  lr: 0.001209  min_lr: 0.000002  loss: 0.4309 (0.4458)  class_acc: 0.8036 (0.7969)  loss_scale: 8192.0000 (12378.2203)  weight_decay: 0.0500 (0.0500)  time: 1.4650  data: 0.0003  max mem: 31081
[2025-03-10 21:11:39,824] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:11:39,824] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.15 GB / 503.51 GB
Epoch: [2]  [690/893]  eta: 0:05:00  lr: 0.001214  min_lr: 0.000002  loss: 0.3984 (0.4457)  class_acc: 0.8036 (0.7969)  loss_scale: 8192.0000 (12388.7699)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0004  max mem: 31081
[2025-03-10 21:12:01,757] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 2484
[2025-03-10 21:12:01,757] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 21:12:01,758] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.56 GB / 503.51 GB
Epoch: [2]  [700/893]  eta: 0:04:45  lr: 0.001219  min_lr: 0.000002  loss: 0.4434 (0.4458)  class_acc: 0.7679 (0.7968)  loss_scale: 16384.0000 (12434.0770)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0004  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.33 GB / 503.51 GB
Epoch: [2]  [710/893]  eta: 0:04:30  lr: 0.001224  min_lr: 0.000002  loss: 0.5005 (0.4462)  class_acc: 0.7857 (0.7967)  loss_scale: 8192.0000 (12374.4135)  weight_decay: 0.0500 (0.0500)  time: 1.4516  data: 0.0002  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.10 GB / 503.51 GB
Epoch: [2]  [720/893]  eta: 0:04:15  lr: 0.001228  min_lr: 0.000002  loss: 0.4536 (0.4462)  class_acc: 0.8036 (0.7968)  loss_scale: 8192.0000 (12316.4050)  weight_decay: 0.0500 (0.0500)  time: 1.4552  data: 0.0002  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.75 GB / 503.51 GB
Epoch: [2]  [730/893]  eta: 0:04:01  lr: 0.001233  min_lr: 0.000002  loss: 0.4536 (0.4462)  class_acc: 0.8036 (0.7966)  loss_scale: 8192.0000 (12259.9836)  weight_decay: 0.0500 (0.0500)  time: 1.4582  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.99 GB / 503.51 GB
Epoch: [2]  [740/893]  eta: 0:03:46  lr: 0.001238  min_lr: 0.000002  loss: 0.4326 (0.4459)  class_acc: 0.8036 (0.7969)  loss_scale: 8192.0000 (12205.0850)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.41 GB / 503.51 GB
Epoch: [2]  [750/893]  eta: 0:03:31  lr: 0.001243  min_lr: 0.000002  loss: 0.3862 (0.4452)  class_acc: 0.8214 (0.7973)  loss_scale: 8192.0000 (12151.6485)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.24 GB / 503.51 GB
Epoch: [2]  [760/893]  eta: 0:03:16  lr: 0.001248  min_lr: 0.000002  loss: 0.3831 (0.4453)  class_acc: 0.8214 (0.7972)  loss_scale: 8192.0000 (12099.6163)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.68 GB / 503.51 GB
Epoch: [2]  [770/893]  eta: 0:03:01  lr: 0.001253  min_lr: 0.000002  loss: 0.4502 (0.4457)  class_acc: 0.7857 (0.7969)  loss_scale: 8192.0000 (12048.9339)  weight_decay: 0.0500 (0.0500)  time: 1.4728  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.35 GB / 503.51 GB
Epoch: [2]  [780/893]  eta: 0:02:47  lr: 0.001258  min_lr: 0.000002  loss: 0.4502 (0.4460)  class_acc: 0.7500 (0.7963)  loss_scale: 8192.0000 (11999.5493)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.46 GB / 503.51 GB
Epoch: [2]  [790/893]  eta: 0:02:32  lr: 0.001263  min_lr: 0.000002  loss: 0.4648 (0.4467)  class_acc: 0.7500 (0.7960)  loss_scale: 8192.0000 (11951.4134)  weight_decay: 0.0500 (0.0500)  time: 1.4591  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.17 GB / 503.51 GB
Epoch: [2]  [800/893]  eta: 0:02:17  lr: 0.001268  min_lr: 0.000002  loss: 0.4329 (0.4466)  class_acc: 0.8036 (0.7962)  loss_scale: 8192.0000 (11904.4794)  weight_decay: 0.0500 (0.0500)  time: 1.4602  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.50 GB / 503.51 GB
Epoch: [2]  [810/893]  eta: 0:02:02  lr: 0.001273  min_lr: 0.000002  loss: 0.4358 (0.4467)  class_acc: 0.7857 (0.7960)  loss_scale: 8192.0000 (11858.7028)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.94 GB / 503.51 GB
Epoch: [2]  [820/893]  eta: 0:01:47  lr: 0.001277  min_lr: 0.000002  loss: 0.4341 (0.4465)  class_acc: 0.7857 (0.7961)  loss_scale: 8192.0000 (11814.0414)  weight_decay: 0.0500 (0.0500)  time: 1.4578  data: 0.0003  max mem: 31081
[2025-03-10 21:15:10,242] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:15:10,242] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.12 GB / 503.51 GB
Epoch: [2]  [830/893]  eta: 0:01:33  lr: 0.001282  min_lr: 0.000002  loss: 0.4355 (0.4466)  class_acc: 0.8036 (0.7959)  loss_scale: 8192.0000 (11790.1709)  weight_decay: 0.0500 (0.0500)  time: 1.4579  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.97 GB / 503.51 GB
Epoch: [2]  [840/893]  eta: 0:01:18  lr: 0.001287  min_lr: 0.000002  loss: 0.4438 (0.4465)  class_acc: 0.7679 (0.7959)  loss_scale: 16384.0000 (11844.7943)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.74 GB / 503.51 GB
Epoch: [2]  [850/893]  eta: 0:01:03  lr: 0.001292  min_lr: 0.000002  loss: 0.4258 (0.4465)  class_acc: 0.7857 (0.7959)  loss_scale: 16384.0000 (11898.1340)  weight_decay: 0.0500 (0.0500)  time: 1.4627  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.86 GB / 503.51 GB
Epoch: [2]  [860/893]  eta: 0:00:48  lr: 0.001297  min_lr: 0.000002  loss: 0.4116 (0.4463)  class_acc: 0.7857 (0.7960)  loss_scale: 16384.0000 (11950.2346)  weight_decay: 0.0500 (0.0500)  time: 1.4464  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.40 GB / 503.51 GB
Epoch: [2]  [870/893]  eta: 0:00:33  lr: 0.001302  min_lr: 0.000002  loss: 0.4045 (0.4464)  class_acc: 0.8036 (0.7960)  loss_scale: 16384.0000 (12001.1389)  weight_decay: 0.0500 (0.0500)  time: 1.4343  data: 0.0002  max mem: 31081
[2025-03-10 21:16:19,704] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 2661
[2025-03-10 21:16:19,704] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 21:16:19,704] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.67 GB / 503.51 GB
Epoch: [2]  [880/893]  eta: 0:00:19  lr: 0.001307  min_lr: 0.000002  loss: 0.4397 (0.4465)  class_acc: 0.7857 (0.7958)  loss_scale: 16384.0000 (12013.6935)  weight_decay: 0.0500 (0.0500)  time: 1.4350  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.63 GB / 503.51 GB
Epoch: [2]  [890/893]  eta: 0:00:04  lr: 0.001312  min_lr: 0.000002  loss: 0.4685 (0.4468)  class_acc: 0.7679 (0.7956)  loss_scale: 8192.0000 (11970.8013)  weight_decay: 0.0500 (0.0500)  time: 1.4368  data: 0.0001  max mem: 31081
Epoch: [2]  [892/893]  eta: 0:00:01  lr: 0.001312  min_lr: 0.000002  loss: 0.4814 (0.4469)  class_acc: 0.7679 (0.7955)  loss_scale: 8192.0000 (11966.5650)  weight_decay: 0.0500 (0.0500)  time: 1.3854  data: 0.0001  max mem: 31081
Epoch: [2] Total time: 0:21:55 (1.4736 s / it)
Averaged stats: lr: 0.001312  min_lr: 0.000002  loss: 0.4814 (0.4469)  class_acc: 0.7679 (0.7955)  loss_scale: 8192.0000 (11966.5650)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:15:54  loss: 0.5734 (0.5734)  acc: 70.2381 (70.2381)  time: 11.2012  data: 10.6692  max mem: 31081
Val:  [ 10/728]  eta: 0:18:27  loss: 0.4007 (0.4371)  acc: 84.5238 (81.1688)  time: 1.5420  data: 1.0202  max mem: 31081
Val:  [ 20/728]  eta: 0:14:13  loss: 0.4006 (0.4246)  acc: 83.3333 (81.5193)  time: 0.7061  data: 0.1827  max mem: 31081
Val:  [ 30/728]  eta: 0:12:36  loss: 0.4006 (0.4362)  acc: 80.9524 (80.2227)  time: 0.8314  data: 0.3070  max mem: 31081
Val:  [ 40/728]  eta: 0:11:51  loss: 0.3951 (0.4472)  acc: 76.1905 (79.1521)  time: 0.8543  data: 0.3311  max mem: 31081
Val:  [ 50/728]  eta: 0:11:17  loss: 0.3804 (0.4301)  acc: 79.7619 (80.1587)  time: 0.8671  data: 0.3437  max mem: 31081
Val:  [ 60/728]  eta: 0:10:24  loss: 0.4224 (0.4396)  acc: 78.5714 (79.7619)  time: 0.7298  data: 0.2039  max mem: 31081
Val:  [ 70/728]  eta: 0:09:51  loss: 0.4906 (0.4418)  acc: 76.1905 (79.3763)  time: 0.6461  data: 0.1213  max mem: 31081
Val:  [ 80/728]  eta: 0:09:41  loss: 0.4341 (0.4549)  acc: 75.0000 (78.5567)  time: 0.7835  data: 0.2584  max mem: 31081
Val:  [ 90/728]  eta: 0:09:26  loss: 0.4341 (0.4623)  acc: 82.1429 (78.3360)  time: 0.8490  data: 0.3244  max mem: 31081
Val:  [100/728]  eta: 0:09:15  loss: 0.4081 (0.4570)  acc: 83.3333 (78.6539)  time: 0.8299  data: 0.3088  max mem: 31081
Val:  [110/728]  eta: 0:09:06  loss: 0.3957 (0.4541)  acc: 82.1429 (78.8396)  time: 0.8660  data: 0.3441  max mem: 31081
Val:  [120/728]  eta: 0:08:48  loss: 0.3829 (0.4466)  acc: 84.5238 (79.3979)  time: 0.7991  data: 0.2801  max mem: 31081
Val:  [130/728]  eta: 0:08:29  loss: 0.3613 (0.4510)  acc: 84.5238 (79.1803)  time: 0.6782  data: 0.1595  max mem: 31081
Val:  [140/728]  eta: 0:08:18  loss: 0.4429 (0.4559)  acc: 77.3810 (78.6643)  time: 0.7162  data: 0.1953  max mem: 31081
Val:  [150/728]  eta: 0:08:03  loss: 0.4560 (0.4545)  acc: 78.5714 (78.8079)  time: 0.7317  data: 0.2101  max mem: 31081
Val:  [160/728]  eta: 0:07:57  loss: 0.4266 (0.4573)  acc: 77.3810 (78.6232)  time: 0.7820  data: 0.2595  max mem: 31081
Val:  [170/728]  eta: 0:07:48  loss: 0.4147 (0.4565)  acc: 78.5714 (78.6062)  time: 0.8620  data: 0.3415  max mem: 31081
Val:  [180/728]  eta: 0:07:34  loss: 0.4001 (0.4553)  acc: 82.1429 (78.8016)  time: 0.7404  data: 0.2223  max mem: 31081
Val:  [190/728]  eta: 0:07:22  loss: 0.4174 (0.4538)  acc: 80.9524 (78.9205)  time: 0.6795  data: 0.1586  max mem: 31081
Val:  [200/728]  eta: 0:07:14  loss: 0.4143 (0.4517)  acc: 78.5714 (78.9386)  time: 0.7684  data: 0.2438  max mem: 31081
Val:  [210/728]  eta: 0:07:08  loss: 0.4132 (0.4528)  acc: 82.1429 (78.9946)  time: 0.8641  data: 0.3392  max mem: 31081
Val:  [220/728]  eta: 0:07:01  loss: 0.3970 (0.4531)  acc: 82.1429 (78.9593)  time: 0.9059  data: 0.3841  max mem: 31081
Val:  [230/728]  eta: 0:06:52  loss: 0.4319 (0.4548)  acc: 78.5714 (78.8343)  time: 0.8349  data: 0.3127  max mem: 31081
Val:  [240/728]  eta: 0:06:39  loss: 0.4049 (0.4519)  acc: 79.7619 (79.0358)  time: 0.6880  data: 0.1621  max mem: 31081
Val:  [250/728]  eta: 0:06:29  loss: 0.4061 (0.4511)  acc: 78.5714 (79.0884)  time: 0.6642  data: 0.1386  max mem: 31081
Val:  [260/728]  eta: 0:06:22  loss: 0.4116 (0.4504)  acc: 82.1429 (79.2191)  time: 0.7938  data: 0.2671  max mem: 31081
Val:  [270/728]  eta: 0:06:14  loss: 0.4017 (0.4501)  acc: 83.3333 (79.2435)  time: 0.8476  data: 0.3235  max mem: 31081
Val:  [280/728]  eta: 0:06:09  loss: 0.3813 (0.4493)  acc: 80.9524 (79.3552)  time: 0.9205  data: 0.4007  max mem: 31081
Val:  [290/728]  eta: 0:06:00  loss: 0.3783 (0.4475)  acc: 82.1429 (79.4674)  time: 0.9126  data: 0.3939  max mem: 31081
Val:  [300/728]  eta: 0:05:48  loss: 0.3783 (0.4465)  acc: 83.3333 (79.5246)  time: 0.6675  data: 0.1462  max mem: 31081
Val:  [310/728]  eta: 0:05:40  loss: 0.4758 (0.4483)  acc: 78.5714 (79.4365)  time: 0.6768  data: 0.1544  max mem: 31081
Val:  [320/728]  eta: 0:05:31  loss: 0.3977 (0.4476)  acc: 78.5714 (79.4281)  time: 0.8114  data: 0.2910  max mem: 31081
Val:  [330/728]  eta: 0:05:24  loss: 0.3993 (0.4470)  acc: 79.7619 (79.4166)  time: 0.8477  data: 0.3273  max mem: 31081
Val:  [340/728]  eta: 0:05:17  loss: 0.3993 (0.4439)  acc: 83.3333 (79.6153)  time: 0.9009  data: 0.3821  max mem: 31081
Val:  [350/728]  eta: 0:05:09  loss: 0.3569 (0.4439)  acc: 84.5238 (79.6228)  time: 0.8497  data: 0.3291  max mem: 31081
Val:  [360/728]  eta: 0:04:57  loss: 0.3710 (0.4422)  acc: 83.3333 (79.7157)  time: 0.6586  data: 0.1378  max mem: 31081
Val:  [370/728]  eta: 0:04:50  loss: 0.3897 (0.4443)  acc: 82.1429 (79.5533)  time: 0.7057  data: 0.1833  max mem: 31081
Val:  [380/728]  eta: 0:04:42  loss: 0.5000 (0.4457)  acc: 73.8095 (79.4807)  time: 0.8609  data: 0.3371  max mem: 31081
Val:  [390/728]  eta: 0:04:34  loss: 0.3537 (0.4429)  acc: 82.1429 (79.6432)  time: 0.8310  data: 0.3094  max mem: 31081
Val:  [400/728]  eta: 0:04:27  loss: 0.3326 (0.4416)  acc: 88.0952 (79.7441)  time: 0.8651  data: 0.3449  max mem: 31081
Val:  [410/728]  eta: 0:04:19  loss: 0.3812 (0.4403)  acc: 83.3333 (79.8169)  time: 0.8746  data: 0.3565  max mem: 31081
Val:  [420/728]  eta: 0:04:09  loss: 0.4058 (0.4415)  acc: 75.0000 (79.7591)  time: 0.6862  data: 0.1672  max mem: 31081
Val:  [430/728]  eta: 0:04:01  loss: 0.4636 (0.4420)  acc: 75.0000 (79.7067)  time: 0.6974  data: 0.1774  max mem: 31081
Val:  [440/728]  eta: 0:03:53  loss: 0.4353 (0.4413)  acc: 79.7619 (79.7538)  time: 0.8361  data: 0.3163  max mem: 31081
Val:  [450/728]  eta: 0:03:45  loss: 0.4201 (0.4424)  acc: 82.1429 (79.6722)  time: 0.8364  data: 0.3137  max mem: 31081
Val:  [460/728]  eta: 0:03:37  loss: 0.4646 (0.4441)  acc: 77.3810 (79.5347)  time: 0.8457  data: 0.3215  max mem: 31081
Val:  [470/728]  eta: 0:03:29  loss: 0.4131 (0.4430)  acc: 78.5714 (79.5723)  time: 0.7943  data: 0.2747  max mem: 31081
Val:  [480/728]  eta: 0:03:19  loss: 0.4003 (0.4423)  acc: 78.5714 (79.6134)  time: 0.6482  data: 0.1280  max mem: 31081
Val:  [490/728]  eta: 0:03:11  loss: 0.4568 (0.4422)  acc: 77.3810 (79.5752)  time: 0.6914  data: 0.1657  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.3998 (0.4416)  acc: 82.1429 (79.6241)  time: 0.8445  data: 0.3203  max mem: 31081
Val:  [510/728]  eta: 0:02:56  loss: 0.3684 (0.4402)  acc: 84.5238 (79.7107)  time: 0.8785  data: 0.3568  max mem: 31081
Val:  [520/728]  eta: 0:02:48  loss: 0.4137 (0.4415)  acc: 78.5714 (79.6339)  time: 0.8566  data: 0.3338  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.4922 (0.4414)  acc: 76.1905 (79.6677)  time: 0.7436  data: 0.2215  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.4567 (0.4435)  acc: 77.3810 (79.5287)  time: 0.6110  data: 0.0898  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.4766 (0.4443)  acc: 75.0000 (79.4573)  time: 0.6631  data: 0.1402  max mem: 31081
Val:  [560/728]  eta: 0:02:14  loss: 0.4487 (0.4442)  acc: 76.1905 (79.4775)  time: 0.7547  data: 0.2316  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4487 (0.4462)  acc: 77.3810 (79.3783)  time: 0.8120  data: 0.2901  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4628 (0.4474)  acc: 77.3810 (79.3193)  time: 0.8377  data: 0.3165  max mem: 31081
Val:  [590/728]  eta: 0:01:50  loss: 0.4628 (0.4477)  acc: 77.3810 (79.3067)  time: 0.7294  data: 0.2102  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4177 (0.4480)  acc: 79.7619 (79.2845)  time: 0.6532  data: 0.1329  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4268 (0.4494)  acc: 79.7619 (79.1637)  time: 0.6561  data: 0.1338  max mem: 31081
Val:  [620/728]  eta: 0:01:26  loss: 0.4268 (0.4505)  acc: 78.5714 (79.1389)  time: 0.8265  data: 0.3048  max mem: 31081
Val:  [630/728]  eta: 0:01:18  loss: 0.3992 (0.4500)  acc: 79.7619 (79.1884)  time: 0.8686  data: 0.3465  max mem: 31081
Val:  [640/728]  eta: 0:01:10  loss: 0.4491 (0.4495)  acc: 82.1429 (79.2177)  time: 0.7829  data: 0.2605  max mem: 31081
Val:  [650/728]  eta: 0:01:02  loss: 0.4406 (0.4496)  acc: 79.7619 (79.1895)  time: 0.7990  data: 0.2744  max mem: 31081
Val:  [660/728]  eta: 0:00:54  loss: 0.4328 (0.4498)  acc: 79.7619 (79.1820)  time: 0.7448  data: 0.2187  max mem: 31081
Val:  [670/728]  eta: 0:00:46  loss: 0.4469 (0.4507)  acc: 79.7619 (79.1409)  time: 0.6805  data: 0.1558  max mem: 31081
Val:  [680/728]  eta: 0:00:38  loss: 0.4236 (0.4500)  acc: 79.7619 (79.1850)  time: 0.7219  data: 0.1979  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3512 (0.4491)  acc: 84.5238 (79.2537)  time: 0.8369  data: 0.3138  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3931 (0.4492)  acc: 84.5238 (79.2524)  time: 0.8631  data: 0.3375  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4142 (0.4490)  acc: 82.1429 (79.2613)  time: 0.8112  data: 0.2856  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4312 (0.4497)  acc: 76.1905 (79.2038)  time: 0.7636  data: 0.2476  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4532 (0.4506)  acc: 76.1905 (79.1410)  time: 0.7401  data: 0.2476  max mem: 31081
Val: Total time: 0:09:35 (0.7909 s / it)
* Acc@1 79.141 AP 0.8047839403152466 loss 0.451
Accuracy of the network on the 61096 val videos: 79.1%
[2025-03-10 21:26:16,980] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestauroc is about to be saved!
[2025-03-10 21:26:16,983] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestauroc/mp_rank_00_model_states.pt
[2025-03-10 21:26:16,983] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestauroc/mp_rank_00_model_states.pt...
[2025-03-10 21:26:17,243] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestauroc/mp_rank_00_model_states.pt.
[2025-03-10 21:26:17,243] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestauroc is ready now!
[2025-03-10 21:26:17,243] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestap is about to be saved!
[2025-03-10 21:26:17,246] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt
[2025-03-10 21:26:17,246] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt...
[2025-03-10 21:26:17,489] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt.
[2025-03-10 21:26:17,489] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestap is ready now!
[2025-03-10 21:26:17,489] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestacc is about to be saved!
[2025-03-10 21:26:17,492] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt
[2025-03-10 21:26:17,492] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt...
[2025-03-10 21:26:17,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt.
[2025-03-10 21:26:17,733] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestacc is ready now!
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.13 GB / 503.51 GB
Epoch: [3]  [  0/893]  eta: 3:04:52  lr: 0.001313  min_lr: 0.000002  loss: 0.3425 (0.3425)  class_acc: 0.8929 (0.8929)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 12.4213  data: 11.0607  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.72 GB / 503.51 GB
Epoch: [3]  [ 10/893]  eta: 0:37:55  lr: 0.001318  min_lr: 0.000002  loss: 0.4312 (0.4284)  class_acc: 0.7857 (0.8133)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5768  data: 1.0962  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.25 GB / 503.51 GB
Epoch: [3]  [ 20/893]  eta: 0:29:59  lr: 0.001323  min_lr: 0.000002  loss: 0.4236 (0.4177)  class_acc: 0.7857 (0.8155)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5427  data: 0.0503  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.67 GB / 503.51 GB
Epoch: [3]  [ 30/893]  eta: 0:27:04  lr: 0.001328  min_lr: 0.000002  loss: 0.4080 (0.4160)  class_acc: 0.8214 (0.8122)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4998  data: 0.0006  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.00 GB / 503.51 GB
Epoch: [3]  [ 40/893]  eta: 0:25:29  lr: 0.001332  min_lr: 0.000002  loss: 0.4194 (0.4285)  class_acc: 0.8036 (0.8027)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5113  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.07 GB / 503.51 GB
Epoch: [3]  [ 50/893]  eta: 0:24:22  lr: 0.001337  min_lr: 0.000002  loss: 0.4619 (0.4377)  class_acc: 0.7857 (0.8001)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5077  data: 0.0004  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.37 GB / 503.51 GB
Epoch: [3]  [ 60/893]  eta: 0:23:28  lr: 0.001342  min_lr: 0.000002  loss: 0.4578 (0.4420)  class_acc: 0.8036 (0.7980)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4810  data: 0.0003  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.92 GB / 503.51 GB
Epoch: [3]  [ 70/893]  eta: 0:22:45  lr: 0.001347  min_lr: 0.000002  loss: 0.4578 (0.4456)  class_acc: 0.7679 (0.7948)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0002  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.00 GB / 503.51 GB
Epoch: [3]  [ 80/893]  eta: 0:22:09  lr: 0.001352  min_lr: 0.000002  loss: 0.4485 (0.4454)  class_acc: 0.7679 (0.7937)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.49 GB / 503.51 GB
Epoch: [3]  [ 90/893]  eta: 0:21:36  lr: 0.001357  min_lr: 0.000002  loss: 0.4121 (0.4403)  class_acc: 0.8214 (0.7981)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4586  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.11 GB / 503.51 GB
Epoch: [3]  [100/893]  eta: 0:21:08  lr: 0.001362  min_lr: 0.000002  loss: 0.4177 (0.4400)  class_acc: 0.8214 (0.8002)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4580  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.55 GB / 503.51 GB
Epoch: [3]  [110/893]  eta: 0:20:42  lr: 0.001367  min_lr: 0.000002  loss: 0.4160 (0.4386)  class_acc: 0.8036 (0.7999)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4566  data: 0.0004  max mem: 31081
[2025-03-10 21:29:20,047] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:29:20,047] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.22 GB / 503.51 GB
Epoch: [3]  [120/893]  eta: 0:20:17  lr: 0.001372  min_lr: 0.000002  loss: 0.4160 (0.4378)  class_acc: 0.7857 (0.8005)  loss_scale: 8192.0000 (8665.9174)  weight_decay: 0.0500 (0.0500)  time: 1.4511  data: 0.0004  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.03 GB / 503.51 GB
Epoch: [3]  [130/893]  eta: 0:19:55  lr: 0.001377  min_lr: 0.000002  loss: 0.4373 (0.4397)  class_acc: 0.7857 (0.7987)  loss_scale: 16384.0000 (9255.0840)  weight_decay: 0.0500 (0.0500)  time: 1.4598  data: 0.0002  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.36 GB / 503.51 GB
Epoch: [3]  [140/893]  eta: 0:19:34  lr: 0.001381  min_lr: 0.000002  loss: 0.4373 (0.4391)  class_acc: 0.7857 (0.7995)  loss_scale: 16384.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  time: 1.4632  data: 0.0003  max mem: 31081
[2025-03-10 21:30:11,125] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 2825
[2025-03-10 21:30:11,125] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 21:30:11,125] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.10 GB / 503.51 GB
Epoch: [3]  [150/893]  eta: 0:19:13  lr: 0.001386  min_lr: 0.000002  loss: 0.4143 (0.4399)  class_acc: 0.8214 (0.8001)  loss_scale: 16384.0000 (10090.8079)  weight_decay: 0.0500 (0.0500)  time: 1.4590  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.28 GB / 503.51 GB
Epoch: [3]  [160/893]  eta: 0:18:53  lr: 0.001391  min_lr: 0.000002  loss: 0.4116 (0.4377)  class_acc: 0.8214 (0.8014)  loss_scale: 8192.0000 (9972.8696)  weight_decay: 0.0500 (0.0500)  time: 1.4585  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.90 GB / 503.51 GB
Epoch: [3]  [170/893]  eta: 0:18:34  lr: 0.001396  min_lr: 0.000002  loss: 0.3921 (0.4351)  class_acc: 0.8214 (0.8018)  loss_scale: 8192.0000 (9868.7251)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.91 GB / 503.51 GB
Epoch: [3]  [180/893]  eta: 0:18:16  lr: 0.001401  min_lr: 0.000002  loss: 0.3984 (0.4357)  class_acc: 0.8036 (0.8021)  loss_scale: 8192.0000 (9776.0884)  weight_decay: 0.0500 (0.0500)  time: 1.4593  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.12 GB / 503.51 GB
Epoch: [3]  [190/893]  eta: 0:17:57  lr: 0.001406  min_lr: 0.000002  loss: 0.4373 (0.4350)  class_acc: 0.8036 (0.8019)  loss_scale: 8192.0000 (9693.1518)  weight_decay: 0.0500 (0.0500)  time: 1.4526  data: 0.0002  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.41 GB / 503.51 GB
Epoch: [3]  [200/893]  eta: 0:17:39  lr: 0.001411  min_lr: 0.000002  loss: 0.4492 (0.4364)  class_acc: 0.7500 (0.8002)  loss_scale: 8192.0000 (9618.4677)  weight_decay: 0.0500 (0.0500)  time: 1.4555  data: 0.0002  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.87 GB / 503.51 GB
Epoch: [3]  [210/893]  eta: 0:17:22  lr: 0.001416  min_lr: 0.000002  loss: 0.4424 (0.4357)  class_acc: 0.7857 (0.8002)  loss_scale: 8192.0000 (9550.8626)  weight_decay: 0.0500 (0.0500)  time: 1.4596  data: 0.0004  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.96 GB / 503.51 GB
Epoch: [3]  [220/893]  eta: 0:17:04  lr: 0.001421  min_lr: 0.000002  loss: 0.4089 (0.4354)  class_acc: 0.8214 (0.8010)  loss_scale: 8192.0000 (9489.3756)  weight_decay: 0.0500 (0.0500)  time: 1.4566  data: 0.0004  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.79 GB / 503.51 GB
Epoch: [3]  [230/893]  eta: 0:16:47  lr: 0.001426  min_lr: 0.000002  loss: 0.4104 (0.4346)  class_acc: 0.8214 (0.8013)  loss_scale: 8192.0000 (9433.2121)  weight_decay: 0.0500 (0.0500)  time: 1.4547  data: 0.0004  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.25 GB / 503.51 GB
Epoch: [3]  [240/893]  eta: 0:16:31  lr: 0.001431  min_lr: 0.000002  loss: 0.4214 (0.4352)  class_acc: 0.8036 (0.8010)  loss_scale: 8192.0000 (9381.7095)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.04 GB / 503.51 GB
Epoch: [3]  [250/893]  eta: 0:16:14  lr: 0.001435  min_lr: 0.000002  loss: 0.4426 (0.4344)  class_acc: 0.7857 (0.8014)  loss_scale: 8192.0000 (9334.3108)  weight_decay: 0.0500 (0.0500)  time: 1.4701  data: 0.0002  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.08 GB / 503.51 GB
Epoch: [3]  [260/893]  eta: 0:15:58  lr: 0.001440  min_lr: 0.000002  loss: 0.4390 (0.4353)  class_acc: 0.7857 (0.8005)  loss_scale: 8192.0000 (9290.5441)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0002  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.47 GB / 503.51 GB
Epoch: [3]  [270/893]  eta: 0:15:41  lr: 0.001445  min_lr: 0.000002  loss: 0.3906 (0.4330)  class_acc: 0.8036 (0.8013)  loss_scale: 8192.0000 (9250.0074)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.0003  max mem: 31081
[2025-03-10 21:33:19,629] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:33:19,629] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.93 GB / 503.51 GB
Epoch: [3]  [280/893]  eta: 0:15:25  lr: 0.001450  min_lr: 0.000002  loss: 0.3992 (0.4336)  class_acc: 0.8036 (0.8008)  loss_scale: 8192.0000 (9299.8149)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.43 GB / 503.51 GB
Epoch: [3]  [290/893]  eta: 0:15:09  lr: 0.001455  min_lr: 0.000002  loss: 0.4399 (0.4328)  class_acc: 0.7857 (0.8011)  loss_scale: 16384.0000 (9543.2577)  weight_decay: 0.0500 (0.0500)  time: 1.4677  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.15 GB / 503.51 GB
Epoch: [3]  [300/893]  eta: 0:14:53  lr: 0.001460  min_lr: 0.000002  loss: 0.4333 (0.4329)  class_acc: 0.8036 (0.8008)  loss_scale: 16384.0000 (9770.5249)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0003  max mem: 31081
[2025-03-10 21:33:57,689] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 2980
[2025-03-10 21:33:57,690] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 21:33:57,690] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.66 GB / 503.51 GB
Epoch: [3]  [310/893]  eta: 0:14:37  lr: 0.001465  min_lr: 0.000002  loss: 0.4419 (0.4341)  class_acc: 0.8036 (0.8005)  loss_scale: 16384.0000 (9798.7910)  weight_decay: 0.0500 (0.0500)  time: 1.4583  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.13 GB / 503.51 GB
Epoch: [3]  [320/893]  eta: 0:14:22  lr: 0.001470  min_lr: 0.000002  loss: 0.4258 (0.4350)  class_acc: 0.8036 (0.7996)  loss_scale: 8192.0000 (9748.7352)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
[2025-03-10 21:34:25,640] [INFO] [logging.py:129:log_dist] [Rank 0] step=3000, skipped=14, lr=[1.9215571700345363e-06, 1.9215571700345363e-06, 3.202595283390894e-06, 3.202595283390894e-06, 5.3376588056514904e-06, 5.3376588056514904e-06, 8.89609800941915e-06, 8.89609800941915e-06, 1.4826830015698586e-05, 1.4826830015698586e-05, 2.471138335949764e-05, 2.471138335949764e-05, 4.118563893249607e-05, 4.118563893249607e-05, 6.864273155416012e-05, 6.864273155416012e-05, 0.00011440455259026686, 0.00011440455259026686, 0.00019067425431711148, 0.00019067425431711148, 0.00031779042386185244, 0.00031779042386185244, 0.0005296507064364208, 0.0005296507064364208, 0.0008827511773940347, 0.0008827511773940347, 0.0014712519623233911, 0.0014712519623233911], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-10 21:34:25,640] [INFO] [timer.py:264:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=61.23613644628121, CurrSamplesPerSec=61.55698347949347, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.25 GB / 503.51 GB
Epoch: [3]  [330/893]  eta: 0:14:06  lr: 0.001475  min_lr: 0.000002  loss: 0.4258 (0.4350)  class_acc: 0.7857 (0.7997)  loss_scale: 8192.0000 (9701.7039)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0002  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.96 GB / 503.51 GB
Epoch: [3]  [340/893]  eta: 0:13:50  lr: 0.001480  min_lr: 0.000002  loss: 0.4370 (0.4349)  class_acc: 0.8036 (0.7996)  loss_scale: 8192.0000 (9657.4311)  weight_decay: 0.0500 (0.0500)  time: 1.4576  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.04 GB / 503.51 GB
Epoch: [3]  [350/893]  eta: 0:13:34  lr: 0.001484  min_lr: 0.000002  loss: 0.4377 (0.4349)  class_acc: 0.7679 (0.7993)  loss_scale: 8192.0000 (9615.6809)  weight_decay: 0.0500 (0.0500)  time: 1.4584  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.36 GB / 503.51 GB
Epoch: [3]  [360/893]  eta: 0:13:19  lr: 0.001489  min_lr: 0.000002  loss: 0.4402 (0.4356)  class_acc: 0.7679 (0.7991)  loss_scale: 8192.0000 (9576.2438)  weight_decay: 0.0500 (0.0500)  time: 1.4616  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.06 GB / 503.51 GB
Epoch: [3]  [370/893]  eta: 0:13:03  lr: 0.001494  min_lr: 0.000002  loss: 0.4214 (0.4349)  class_acc: 0.8036 (0.7998)  loss_scale: 8192.0000 (9538.9326)  weight_decay: 0.0500 (0.0500)  time: 1.4546  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.84 GB / 503.51 GB
Epoch: [3]  [380/893]  eta: 0:12:48  lr: 0.001499  min_lr: 0.000002  loss: 0.4214 (0.4355)  class_acc: 0.8036 (0.7995)  loss_scale: 8192.0000 (9503.5801)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.85 GB / 503.51 GB
Epoch: [3]  [390/893]  eta: 0:12:32  lr: 0.001504  min_lr: 0.000002  loss: 0.4360 (0.4352)  class_acc: 0.7857 (0.7994)  loss_scale: 8192.0000 (9470.0358)  weight_decay: 0.0500 (0.0500)  time: 1.4569  data: 0.0004  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.54 GB / 503.51 GB
Epoch: [3]  [400/893]  eta: 0:12:17  lr: 0.001509  min_lr: 0.000002  loss: 0.4446 (0.4356)  class_acc: 0.8036 (0.7992)  loss_scale: 8192.0000 (9438.1646)  weight_decay: 0.0500 (0.0500)  time: 1.4491  data: 0.0004  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.06 GB / 503.51 GB
Epoch: [3]  [410/893]  eta: 0:12:01  lr: 0.001514  min_lr: 0.000002  loss: 0.4551 (0.4365)  class_acc: 0.7679 (0.7984)  loss_scale: 8192.0000 (9407.8443)  weight_decay: 0.0500 (0.0500)  time: 1.4531  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.99 GB / 503.51 GB
Epoch: [3]  [420/893]  eta: 0:11:46  lr: 0.001519  min_lr: 0.000002  loss: 0.4524 (0.4359)  class_acc: 0.7679 (0.7988)  loss_scale: 8192.0000 (9378.9644)  weight_decay: 0.0500 (0.0500)  time: 1.4569  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.55 GB / 503.51 GB
Epoch: [3]  [430/893]  eta: 0:11:31  lr: 0.001524  min_lr: 0.000002  loss: 0.4492 (0.4361)  class_acc: 0.7679 (0.7985)  loss_scale: 8192.0000 (9351.4246)  weight_decay: 0.0500 (0.0500)  time: 1.4590  data: 0.0003  max mem: 31081
[2025-03-10 21:37:05,839] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:37:05,839] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.69 GB / 503.51 GB
Epoch: [3]  [440/893]  eta: 0:11:15  lr: 0.001529  min_lr: 0.000002  loss: 0.4441 (0.4359)  class_acc: 0.7857 (0.7989)  loss_scale: 8192.0000 (9473.7415)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0002  max mem: 31081
[2025-03-10 21:37:19,040] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 3118
[2025-03-10 21:37:19,040] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 21:37:19,040] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.65 GB / 503.51 GB
Epoch: [3]  [450/893]  eta: 0:11:00  lr: 0.001534  min_lr: 0.000002  loss: 0.4441 (0.4365)  class_acc: 0.7857 (0.7983)  loss_scale: 8192.0000 (9463.4856)  weight_decay: 0.0500 (0.0500)  time: 1.4566  data: 0.0002  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.24 GB / 503.51 GB
Epoch: [3]  [460/893]  eta: 0:10:45  lr: 0.001538  min_lr: 0.000002  loss: 0.4487 (0.4370)  class_acc: 0.7857 (0.7982)  loss_scale: 8192.0000 (9435.9046)  weight_decay: 0.0500 (0.0500)  time: 1.4589  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.01 GB / 503.51 GB
Epoch: [3]  [470/893]  eta: 0:10:30  lr: 0.001543  min_lr: 0.000002  loss: 0.4226 (0.4369)  class_acc: 0.8036 (0.7986)  loss_scale: 8192.0000 (9409.4947)  weight_decay: 0.0500 (0.0500)  time: 1.4572  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.22 GB / 503.51 GB
Epoch: [3]  [480/893]  eta: 0:10:15  lr: 0.001548  min_lr: 0.000002  loss: 0.4275 (0.4369)  class_acc: 0.8036 (0.7986)  loss_scale: 8192.0000 (9384.1830)  weight_decay: 0.0500 (0.0500)  time: 1.4595  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.38 GB / 503.51 GB
Epoch: [3]  [490/893]  eta: 0:09:59  lr: 0.001553  min_lr: 0.000002  loss: 0.4338 (0.4367)  class_acc: 0.8036 (0.7990)  loss_scale: 8192.0000 (9359.9022)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.51 GB / 503.51 GB
Epoch: [3]  [500/893]  eta: 0:09:44  lr: 0.001558  min_lr: 0.000002  loss: 0.4099 (0.4363)  class_acc: 0.8036 (0.7990)  loss_scale: 8192.0000 (9336.5908)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0004  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.22 GB / 503.51 GB
Epoch: [3]  [510/893]  eta: 0:09:29  lr: 0.001563  min_lr: 0.000002  loss: 0.4282 (0.4365)  class_acc: 0.7857 (0.7988)  loss_scale: 8192.0000 (9314.1918)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0004  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.31 GB / 503.51 GB
Epoch: [3]  [520/893]  eta: 0:09:14  lr: 0.001568  min_lr: 0.000002  loss: 0.4282 (0.4360)  class_acc: 0.8036 (0.7995)  loss_scale: 8192.0000 (9292.6526)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.20 GB / 503.51 GB
Epoch: [3]  [530/893]  eta: 0:08:59  lr: 0.001573  min_lr: 0.000002  loss: 0.3992 (0.4358)  class_acc: 0.8393 (0.7999)  loss_scale: 8192.0000 (9271.9247)  weight_decay: 0.0500 (0.0500)  time: 1.4658  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.32 GB / 503.51 GB
Epoch: [3]  [540/893]  eta: 0:08:44  lr: 0.001578  min_lr: 0.000002  loss: 0.4155 (0.4358)  class_acc: 0.8036 (0.8001)  loss_scale: 8192.0000 (9251.9630)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0004  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.86 GB / 503.51 GB
Epoch: [3]  [550/893]  eta: 0:08:29  lr: 0.001583  min_lr: 0.000002  loss: 0.4114 (0.4360)  class_acc: 0.7857 (0.7998)  loss_scale: 8192.0000 (9232.7260)  weight_decay: 0.0500 (0.0500)  time: 1.4612  data: 0.0004  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.53 GB / 503.51 GB
Epoch: [3]  [560/893]  eta: 0:08:14  lr: 0.001588  min_lr: 0.000002  loss: 0.4336 (0.4364)  class_acc: 0.7679 (0.7996)  loss_scale: 8192.0000 (9214.1747)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.49 GB / 503.51 GB
Epoch: [3]  [570/893]  eta: 0:07:59  lr: 0.001592  min_lr: 0.000002  loss: 0.4336 (0.4362)  class_acc: 0.7857 (0.7996)  loss_scale: 8192.0000 (9196.2732)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
[2025-03-10 21:40:27,539] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:40:27,539] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.45 GB / 503.51 GB
Epoch: [3]  [580/893]  eta: 0:07:44  lr: 0.001597  min_lr: 0.000002  loss: 0.4109 (0.4356)  class_acc: 0.8036 (0.7999)  loss_scale: 8192.0000 (9319.9862)  weight_decay: 0.0500 (0.0500)  time: 1.4531  data: 0.0002  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.07 GB / 503.51 GB
Epoch: [3]  [590/893]  eta: 0:07:29  lr: 0.001602  min_lr: 0.000002  loss: 0.4109 (0.4353)  class_acc: 0.8036 (0.8000)  loss_scale: 16384.0000 (9439.5127)  weight_decay: 0.0500 (0.0500)  time: 1.4536  data: 0.0002  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.29 GB / 503.51 GB
Epoch: [3]  [600/893]  eta: 0:07:14  lr: 0.001607  min_lr: 0.000002  loss: 0.4355 (0.4354)  class_acc: 0.8036 (0.8000)  loss_scale: 16384.0000 (9555.0616)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0003  max mem: 31081
[2025-03-10 21:41:14,240] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 3279
[2025-03-10 21:41:14,240] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 21:41:14,240] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.52 GB / 503.51 GB
Epoch: [3]  [610/893]  eta: 0:06:59  lr: 0.001612  min_lr: 0.000002  loss: 0.4355 (0.4356)  class_acc: 0.7857 (0.8000)  loss_scale: 16384.0000 (9559.5679)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.79 GB / 503.51 GB
Epoch: [3]  [620/893]  eta: 0:06:44  lr: 0.001617  min_lr: 0.000002  loss: 0.4355 (0.4363)  class_acc: 0.7857 (0.7995)  loss_scale: 8192.0000 (9537.5459)  weight_decay: 0.0500 (0.0500)  time: 1.4658  data: 0.0002  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.87 GB / 503.51 GB
Epoch: [3]  [630/893]  eta: 0:06:29  lr: 0.001622  min_lr: 0.000002  loss: 0.4443 (0.4362)  class_acc: 0.7857 (0.7994)  loss_scale: 8192.0000 (9516.2219)  weight_decay: 0.0500 (0.0500)  time: 1.4632  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.27 GB / 503.51 GB
Epoch: [3]  [640/893]  eta: 0:06:14  lr: 0.001627  min_lr: 0.000002  loss: 0.4233 (0.4354)  class_acc: 0.8036 (0.7999)  loss_scale: 8192.0000 (9495.5632)  weight_decay: 0.0500 (0.0500)  time: 1.4567  data: 0.0004  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.09 GB / 503.51 GB
Epoch: [3]  [650/893]  eta: 0:06:00  lr: 0.001632  min_lr: 0.000002  loss: 0.4170 (0.4354)  class_acc: 0.8214 (0.8001)  loss_scale: 8192.0000 (9475.5392)  weight_decay: 0.0500 (0.0500)  time: 1.4587  data: 0.0004  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.90 GB / 503.51 GB
Epoch: [3]  [660/893]  eta: 0:05:45  lr: 0.001637  min_lr: 0.000002  loss: 0.4155 (0.4352)  class_acc: 0.8036 (0.8003)  loss_scale: 8192.0000 (9456.1210)  weight_decay: 0.0500 (0.0500)  time: 1.4583  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.25 GB / 503.51 GB
Epoch: [3]  [670/893]  eta: 0:05:30  lr: 0.001641  min_lr: 0.000002  loss: 0.4116 (0.4352)  class_acc: 0.8036 (0.8004)  loss_scale: 8192.0000 (9437.2817)  weight_decay: 0.0500 (0.0500)  time: 1.4566  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.43 GB / 503.51 GB
Epoch: [3]  [680/893]  eta: 0:05:15  lr: 0.001646  min_lr: 0.000002  loss: 0.4304 (0.4355)  class_acc: 0.8036 (0.8001)  loss_scale: 8192.0000 (9418.9956)  weight_decay: 0.0500 (0.0500)  time: 1.4564  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.84 GB / 503.51 GB
Epoch: [3]  [690/893]  eta: 0:05:00  lr: 0.001651  min_lr: 0.000002  loss: 0.4395 (0.4355)  class_acc: 0.7679 (0.7999)  loss_scale: 8192.0000 (9401.2388)  weight_decay: 0.0500 (0.0500)  time: 1.4585  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.96 GB / 503.51 GB
Epoch: [3]  [700/893]  eta: 0:04:45  lr: 0.001656  min_lr: 0.000002  loss: 0.4326 (0.4358)  class_acc: 0.7857 (0.7997)  loss_scale: 8192.0000 (9383.9886)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.63 GB / 503.51 GB
Epoch: [3]  [710/893]  eta: 0:04:30  lr: 0.001661  min_lr: 0.000002  loss: 0.4326 (0.4354)  class_acc: 0.7857 (0.7998)  loss_scale: 8192.0000 (9367.2236)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0002  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.00 GB / 503.51 GB
Epoch: [3]  [720/893]  eta: 0:04:15  lr: 0.001666  min_lr: 0.000002  loss: 0.4116 (0.4351)  class_acc: 0.8036 (0.8001)  loss_scale: 8192.0000 (9350.9237)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.49 GB / 503.51 GB
Epoch: [3]  [730/893]  eta: 0:04:01  lr: 0.001671  min_lr: 0.000002  loss: 0.4075 (0.4348)  class_acc: 0.8214 (0.8002)  loss_scale: 8192.0000 (9335.0698)  weight_decay: 0.0500 (0.0500)  time: 1.4585  data: 0.0003  max mem: 31081
[2025-03-10 21:44:22,499] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:44:22,499] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.08 GB / 503.51 GB
Epoch: [3]  [740/893]  eta: 0:03:46  lr: 0.001676  min_lr: 0.000002  loss: 0.4119 (0.4347)  class_acc: 0.8036 (0.8001)  loss_scale: 8192.0000 (9419.1417)  weight_decay: 0.0500 (0.0500)  time: 1.4597  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.04 GB / 503.51 GB
Epoch: [3]  [750/893]  eta: 0:03:31  lr: 0.001681  min_lr: 0.000002  loss: 0.4111 (0.4344)  class_acc: 0.8036 (0.8005)  loss_scale: 16384.0000 (9511.8828)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.15 GB / 503.51 GB
Epoch: [3]  [760/893]  eta: 0:03:16  lr: 0.001686  min_lr: 0.000002  loss: 0.4172 (0.4344)  class_acc: 0.8036 (0.8004)  loss_scale: 16384.0000 (9602.1866)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0004  max mem: 31081
[2025-03-10 21:45:05,043] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 3437
[2025-03-10 21:45:05,043] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 21:45:05,043] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.24 GB / 503.51 GB
Epoch: [3]  [770/893]  eta: 0:03:01  lr: 0.001691  min_lr: 0.000002  loss: 0.3958 (0.4339)  class_acc: 0.8036 (0.8006)  loss_scale: 8192.0000 (9583.8962)  weight_decay: 0.0500 (0.0500)  time: 1.4699  data: 0.0004  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.70 GB / 503.51 GB
Epoch: [3]  [780/893]  eta: 0:02:47  lr: 0.001695  min_lr: 0.000002  loss: 0.3958 (0.4338)  class_acc: 0.8036 (0.8007)  loss_scale: 8192.0000 (9566.0743)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.73 GB / 503.51 GB
Epoch: [3]  [790/893]  eta: 0:02:32  lr: 0.001700  min_lr: 0.000002  loss: 0.4387 (0.4339)  class_acc: 0.8036 (0.8009)  loss_scale: 8192.0000 (9548.7029)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0004  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.98 GB / 503.51 GB
Epoch: [3]  [800/893]  eta: 0:02:17  lr: 0.001705  min_lr: 0.000002  loss: 0.4258 (0.4336)  class_acc: 0.8214 (0.8011)  loss_scale: 8192.0000 (9531.7653)  weight_decay: 0.0500 (0.0500)  time: 1.4673  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.53 GB / 503.51 GB
Epoch: [3]  [810/893]  eta: 0:02:02  lr: 0.001710  min_lr: 0.000002  loss: 0.4258 (0.4340)  class_acc: 0.8214 (0.8008)  loss_scale: 8192.0000 (9515.2454)  weight_decay: 0.0500 (0.0500)  time: 1.4627  data: 0.0002  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.83 GB / 503.51 GB
Epoch: [3]  [820/893]  eta: 0:01:47  lr: 0.001715  min_lr: 0.000002  loss: 0.4451 (0.4345)  class_acc: 0.7679 (0.8005)  loss_scale: 8192.0000 (9499.1279)  weight_decay: 0.0500 (0.0500)  time: 1.4569  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.96 GB / 503.51 GB
Epoch: [3]  [830/893]  eta: 0:01:33  lr: 0.001720  min_lr: 0.000002  loss: 0.4673 (0.4350)  class_acc: 0.8036 (0.8003)  loss_scale: 8192.0000 (9483.3983)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.92 GB / 503.51 GB
Epoch: [3]  [840/893]  eta: 0:01:18  lr: 0.001725  min_lr: 0.000002  loss: 0.4622 (0.4351)  class_acc: 0.8036 (0.8002)  loss_scale: 8192.0000 (9468.0428)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.57 GB / 503.51 GB
Epoch: [3]  [850/893]  eta: 0:01:03  lr: 0.001730  min_lr: 0.000002  loss: 0.4202 (0.4345)  class_acc: 0.8036 (0.8006)  loss_scale: 8192.0000 (9453.0482)  weight_decay: 0.0500 (0.0500)  time: 1.4550  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.82 GB / 503.51 GB
Epoch: [3]  [860/893]  eta: 0:00:48  lr: 0.001735  min_lr: 0.000002  loss: 0.3970 (0.4343)  class_acc: 0.8214 (0.8006)  loss_scale: 8192.0000 (9438.4019)  weight_decay: 0.0500 (0.0500)  time: 1.4426  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.29 GB / 503.51 GB
Epoch: [3]  [870/893]  eta: 0:00:33  lr: 0.001740  min_lr: 0.000002  loss: 0.4158 (0.4343)  class_acc: 0.8036 (0.8006)  loss_scale: 8192.0000 (9424.0918)  weight_decay: 0.0500 (0.0500)  time: 1.4349  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.75 GB / 503.51 GB
Epoch: [3]  [880/893]  eta: 0:00:19  lr: 0.001745  min_lr: 0.000002  loss: 0.4106 (0.4337)  class_acc: 0.8214 (0.8009)  loss_scale: 8192.0000 (9410.1067)  weight_decay: 0.0500 (0.0500)  time: 1.4376  data: 0.0001  max mem: 31081
[2025-03-10 21:48:12,734] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 21:48:12,734] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.14 GB / 503.51 GB
Epoch: [3]  [890/893]  eta: 0:00:04  lr: 0.001749  min_lr: 0.000002  loss: 0.3804 (0.4333)  class_acc: 0.8393 (0.8010)  loss_scale: 8192.0000 (9405.6296)  weight_decay: 0.0500 (0.0500)  time: 1.4380  data: 0.0001  max mem: 31081
Epoch: [3]  [892/893]  eta: 0:00:01  lr: 0.001750  min_lr: 0.000002  loss: 0.3860 (0.4333)  class_acc: 0.8214 (0.8010)  loss_scale: 8192.0000 (9413.4529)  weight_decay: 0.0500 (0.0500)  time: 1.3868  data: 0.0001  max mem: 31081
Epoch: [3] Total time: 0:21:56 (1.4743 s / it)
Averaged stats: lr: 0.001750  min_lr: 0.000002  loss: 0.3860 (0.4333)  class_acc: 0.8214 (0.8010)  loss_scale: 8192.0000 (9413.4529)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:15:27  loss: 0.7437 (0.7437)  acc: 67.8571 (67.8571)  time: 11.1644  data: 10.6124  max mem: 31081
Val:  [ 10/728]  eta: 0:18:20  loss: 0.3949 (0.4600)  acc: 79.7619 (81.0606)  time: 1.5323  data: 1.0104  max mem: 31081
Val:  [ 20/728]  eta: 0:14:09  loss: 0.3762 (0.4198)  acc: 84.5238 (82.5397)  time: 0.7015  data: 0.1798  max mem: 31081
Val:  [ 30/728]  eta: 0:12:39  loss: 0.3699 (0.4294)  acc: 84.5238 (80.9140)  time: 0.8431  data: 0.3190  max mem: 31081
Val:  [ 40/728]  eta: 0:11:46  loss: 0.4467 (0.4428)  acc: 77.3810 (79.3844)  time: 0.8463  data: 0.3222  max mem: 31081
Val:  [ 50/728]  eta: 0:11:11  loss: 0.3513 (0.4258)  acc: 80.9524 (80.2988)  time: 0.8396  data: 0.3137  max mem: 31081
Val:  [ 60/728]  eta: 0:10:15  loss: 0.4025 (0.4408)  acc: 78.5714 (79.6643)  time: 0.7058  data: 0.1790  max mem: 31081
Val:  [ 70/728]  eta: 0:09:49  loss: 0.4908 (0.4399)  acc: 76.1905 (79.5775)  time: 0.6542  data: 0.1303  max mem: 31081
Val:  [ 80/728]  eta: 0:09:34  loss: 0.4079 (0.4509)  acc: 77.3810 (78.9830)  time: 0.7791  data: 0.2597  max mem: 31081
Val:  [ 90/728]  eta: 0:09:21  loss: 0.4376 (0.4697)  acc: 79.7619 (78.3360)  time: 0.8227  data: 0.3033  max mem: 31081
Val:  [100/728]  eta: 0:09:09  loss: 0.4376 (0.4632)  acc: 80.9524 (78.5479)  time: 0.8269  data: 0.3036  max mem: 31081
Val:  [110/728]  eta: 0:09:00  loss: 0.4060 (0.4666)  acc: 80.9524 (78.5822)  time: 0.8496  data: 0.3258  max mem: 31081
Val:  [120/728]  eta: 0:08:44  loss: 0.3261 (0.4573)  acc: 85.7143 (79.1027)  time: 0.8026  data: 0.2811  max mem: 31081
Val:  [130/728]  eta: 0:08:24  loss: 0.3780 (0.4651)  acc: 83.3333 (78.7804)  time: 0.6753  data: 0.1543  max mem: 31081
Val:  [140/728]  eta: 0:08:14  loss: 0.4353 (0.4681)  acc: 78.5714 (78.4363)  time: 0.7049  data: 0.1812  max mem: 31081
Val:  [150/728]  eta: 0:08:01  loss: 0.4648 (0.4683)  acc: 76.1905 (78.4374)  time: 0.7579  data: 0.2335  max mem: 31081
Val:  [160/728]  eta: 0:07:51  loss: 0.4259 (0.4738)  acc: 79.7619 (78.3126)  time: 0.7545  data: 0.2327  max mem: 31081
Val:  [170/728]  eta: 0:07:43  loss: 0.4232 (0.4722)  acc: 79.7619 (78.4322)  time: 0.8188  data: 0.2974  max mem: 31081
Val:  [180/728]  eta: 0:07:31  loss: 0.4207 (0.4749)  acc: 83.3333 (78.4004)  time: 0.7844  data: 0.2617  max mem: 31081
Val:  [190/728]  eta: 0:07:19  loss: 0.4207 (0.4735)  acc: 83.3333 (78.4218)  time: 0.7033  data: 0.1790  max mem: 31081
Val:  [200/728]  eta: 0:07:11  loss: 0.3946 (0.4709)  acc: 79.7619 (78.4767)  time: 0.7399  data: 0.2150  max mem: 31081
Val:  [210/728]  eta: 0:07:04  loss: 0.4066 (0.4733)  acc: 79.7619 (78.4586)  time: 0.8326  data: 0.3082  max mem: 31081
Val:  [220/728]  eta: 0:06:59  loss: 0.3806 (0.4706)  acc: 80.9524 (78.5876)  time: 0.9104  data: 0.3861  max mem: 31081
Val:  [230/728]  eta: 0:06:49  loss: 0.3862 (0.4747)  acc: 78.5714 (78.4271)  time: 0.8466  data: 0.3235  max mem: 31081
Val:  [240/728]  eta: 0:06:37  loss: 0.4302 (0.4712)  acc: 82.1429 (78.6159)  time: 0.7042  data: 0.1816  max mem: 31081
Val:  [250/728]  eta: 0:06:26  loss: 0.3745 (0.4718)  acc: 82.1429 (78.6521)  time: 0.6629  data: 0.1387  max mem: 31081
Val:  [260/728]  eta: 0:06:19  loss: 0.3817 (0.4698)  acc: 82.1429 (78.7493)  time: 0.7729  data: 0.2481  max mem: 31081
Val:  [270/728]  eta: 0:06:11  loss: 0.3817 (0.4691)  acc: 83.3333 (78.7515)  time: 0.8488  data: 0.3249  max mem: 31081
Val:  [280/728]  eta: 0:06:06  loss: 0.4404 (0.4690)  acc: 78.5714 (78.8002)  time: 0.9139  data: 0.3925  max mem: 31081
Val:  [290/728]  eta: 0:05:58  loss: 0.3866 (0.4671)  acc: 80.9524 (78.8946)  time: 0.9142  data: 0.3943  max mem: 31081
Val:  [300/728]  eta: 0:05:46  loss: 0.3766 (0.4675)  acc: 82.1429 (78.8918)  time: 0.6675  data: 0.1467  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.4590 (0.4688)  acc: 77.3810 (78.8738)  time: 0.6719  data: 0.1502  max mem: 31081
Val:  [320/728]  eta: 0:05:30  loss: 0.3493 (0.4671)  acc: 84.5238 (78.9349)  time: 0.8092  data: 0.2832  max mem: 31081
Val:  [330/728]  eta: 0:05:22  loss: 0.3707 (0.4654)  acc: 82.1429 (78.9886)  time: 0.8400  data: 0.3146  max mem: 31081
Val:  [340/728]  eta: 0:05:15  loss: 0.3821 (0.4618)  acc: 82.1429 (79.1859)  time: 0.8936  data: 0.3712  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3913 (0.4623)  acc: 82.1429 (79.1684)  time: 0.8518  data: 0.3284  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.3947 (0.4608)  acc: 79.7619 (79.2013)  time: 0.6641  data: 0.1387  max mem: 31081
Val:  [370/728]  eta: 0:04:49  loss: 0.3992 (0.4633)  acc: 79.7619 (79.0463)  time: 0.6990  data: 0.1710  max mem: 31081
Val:  [380/728]  eta: 0:04:41  loss: 0.5004 (0.4661)  acc: 76.1905 (78.9433)  time: 0.8439  data: 0.3186  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.3691 (0.4625)  acc: 77.3810 (79.0921)  time: 0.8256  data: 0.3038  max mem: 31081
Val:  [400/728]  eta: 0:04:25  loss: 0.2996 (0.4608)  acc: 86.9048 (79.1978)  time: 0.8659  data: 0.3430  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.3584 (0.4597)  acc: 80.9524 (79.2637)  time: 0.8754  data: 0.3511  max mem: 31081
Val:  [420/728]  eta: 0:04:07  loss: 0.4282 (0.4603)  acc: 79.7619 (79.2642)  time: 0.6882  data: 0.1643  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4841 (0.4610)  acc: 77.3810 (79.2095)  time: 0.6920  data: 0.1689  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.3848 (0.4595)  acc: 78.5714 (79.3273)  time: 0.8275  data: 0.3061  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.3308 (0.4601)  acc: 83.3333 (79.2577)  time: 0.8371  data: 0.3141  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.4026 (0.4636)  acc: 76.1905 (79.1086)  time: 0.8451  data: 0.3218  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.4017 (0.4626)  acc: 75.0000 (79.1401)  time: 0.7961  data: 0.2751  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.3784 (0.4617)  acc: 80.9524 (79.1927)  time: 0.6515  data: 0.1302  max mem: 31081
Val:  [490/728]  eta: 0:03:10  loss: 0.4122 (0.4615)  acc: 79.7619 (79.1630)  time: 0.6951  data: 0.1747  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.4122 (0.4603)  acc: 79.7619 (79.2201)  time: 0.8502  data: 0.3269  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3293 (0.4587)  acc: 85.7143 (79.3076)  time: 0.8752  data: 0.3516  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4170 (0.4599)  acc: 80.9524 (79.2455)  time: 0.8575  data: 0.3374  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.5015 (0.4602)  acc: 76.1905 (79.2395)  time: 0.7489  data: 0.2261  max mem: 31081
Val:  [540/728]  eta: 0:02:29  loss: 0.4919 (0.4621)  acc: 76.1905 (79.1436)  time: 0.6121  data: 0.0881  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.4919 (0.4632)  acc: 73.8095 (79.0856)  time: 0.6627  data: 0.1409  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.4973 (0.4631)  acc: 78.5714 (79.1508)  time: 0.7687  data: 0.2490  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4973 (0.4653)  acc: 79.7619 (79.0739)  time: 0.8019  data: 0.2816  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4065 (0.4654)  acc: 79.7619 (79.0837)  time: 0.8251  data: 0.3015  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.4065 (0.4657)  acc: 79.7619 (79.0690)  time: 0.7269  data: 0.2020  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.3768 (0.4661)  acc: 77.3810 (79.0369)  time: 0.6955  data: 0.1727  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4376 (0.4677)  acc: 75.0000 (78.9241)  time: 0.6625  data: 0.1429  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4659 (0.4694)  acc: 77.3810 (78.8877)  time: 0.7412  data: 0.2206  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4267 (0.4682)  acc: 79.7619 (78.9374)  time: 0.8534  data: 0.3299  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3960 (0.4672)  acc: 83.3333 (78.9782)  time: 0.7992  data: 0.2736  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4228 (0.4676)  acc: 79.7619 (78.9555)  time: 0.7962  data: 0.2702  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4278 (0.4674)  acc: 78.5714 (78.9460)  time: 0.8023  data: 0.2769  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4278 (0.4677)  acc: 80.9524 (78.9405)  time: 0.6867  data: 0.1615  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4401 (0.4671)  acc: 79.7619 (78.9543)  time: 0.6777  data: 0.1550  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3707 (0.4659)  acc: 83.3333 (79.0194)  time: 0.8373  data: 0.3156  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3829 (0.4660)  acc: 84.5238 (79.0045)  time: 0.8389  data: 0.3150  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4434 (0.4662)  acc: 80.9524 (79.0001)  time: 0.8222  data: 0.3019  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4293 (0.4659)  acc: 76.1905 (79.0205)  time: 0.8127  data: 0.3014  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4293 (0.4665)  acc: 79.7619 (78.9986)  time: 0.7908  data: 0.3014  max mem: 31081
Val: Total time: 0:09:34 (0.7888 s / it)
* Acc@1 78.999 AP 0.8035150766372681 loss 0.466
Accuracy of the network on the 61096 val videos: 79.0%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.34 GB / 503.51 GB
Epoch: [4]  [  0/893]  eta: 3:30:22  lr: 0.001750  min_lr: 0.000002  loss: 0.4619 (0.4619)  class_acc: 0.7679 (0.7679)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 14.1351  data: 12.8332  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.53 GB / 503.51 GB
Epoch: [4]  [ 10/893]  eta: 0:38:57  lr: 0.001755  min_lr: 0.000002  loss: 0.4473 (0.4380)  class_acc: 0.8214 (0.8052)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6472  data: 1.1670  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.48 GB / 503.51 GB
Epoch: [4]  [ 20/893]  eta: 0:30:44  lr: 0.001760  min_lr: 0.000002  loss: 0.4143 (0.4222)  class_acc: 0.8214 (0.8112)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5112  data: 0.0006  max mem: 31081
[2025-03-10 21:58:40,553] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 3592
[2025-03-10 21:58:40,553] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 21:58:40,553] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.64 GB / 503.51 GB
Epoch: [4]  [ 30/893]  eta: 0:27:34  lr: 0.001765  min_lr: 0.000002  loss: 0.3953 (0.4167)  class_acc: 0.8214 (0.8145)  loss_scale: 16384.0000 (14534.1935)  weight_decay: 0.0500 (0.0500)  time: 1.5165  data: 0.0006  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.64 GB / 503.51 GB
Epoch: [4]  [ 40/893]  eta: 0:25:51  lr: 0.001770  min_lr: 0.000002  loss: 0.4258 (0.4271)  class_acc: 0.7857 (0.8088)  loss_scale: 8192.0000 (12987.3171)  weight_decay: 0.0500 (0.0500)  time: 1.5119  data: 0.0007  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.04 GB / 503.51 GB
Epoch: [4]  [ 50/893]  eta: 0:24:38  lr: 0.001775  min_lr: 0.000002  loss: 0.4258 (0.4259)  class_acc: 0.7857 (0.8078)  loss_scale: 8192.0000 (12047.0588)  weight_decay: 0.0500 (0.0500)  time: 1.4990  data: 0.0007  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.17 GB / 503.51 GB
Epoch: [4]  [ 60/893]  eta: 0:23:40  lr: 0.001780  min_lr: 0.000002  loss: 0.4036 (0.4241)  class_acc: 0.8214 (0.8097)  loss_scale: 8192.0000 (11415.0820)  weight_decay: 0.0500 (0.0500)  time: 1.4708  data: 0.0004  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.00 GB / 503.51 GB
Epoch: [4]  [ 70/893]  eta: 0:22:55  lr: 0.001785  min_lr: 0.000002  loss: 0.4255 (0.4286)  class_acc: 0.8036 (0.8068)  loss_scale: 8192.0000 (10961.1268)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.59 GB / 503.51 GB
Epoch: [4]  [ 80/893]  eta: 0:22:18  lr: 0.001790  min_lr: 0.000002  loss: 0.4255 (0.4277)  class_acc: 0.7857 (0.8071)  loss_scale: 8192.0000 (10619.2593)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0002  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.29 GB / 503.51 GB
Epoch: [4]  [ 90/893]  eta: 0:21:45  lr: 0.001795  min_lr: 0.000002  loss: 0.3860 (0.4269)  class_acc: 0.8036 (0.8069)  loss_scale: 8192.0000 (10352.5275)  weight_decay: 0.0500 (0.0500)  time: 1.4677  data: 0.0002  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.73 GB / 503.51 GB
Epoch: [4]  [100/893]  eta: 0:21:16  lr: 0.001799  min_lr: 0.000002  loss: 0.4214 (0.4285)  class_acc: 0.7857 (0.8041)  loss_scale: 8192.0000 (10138.6139)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.32 GB / 503.51 GB
Epoch: [4]  [110/893]  eta: 0:20:50  lr: 0.001804  min_lr: 0.000002  loss: 0.4070 (0.4245)  class_acc: 0.8036 (0.8068)  loss_scale: 8192.0000 (9963.2432)  weight_decay: 0.0500 (0.0500)  time: 1.4650  data: 0.0002  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.89 GB / 503.51 GB
Epoch: [4]  [120/893]  eta: 0:20:26  lr: 0.001809  min_lr: 0.000002  loss: 0.3792 (0.4232)  class_acc: 0.8214 (0.8087)  loss_scale: 8192.0000 (9816.8595)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.22 GB / 503.51 GB
Epoch: [4]  [130/893]  eta: 0:20:02  lr: 0.001814  min_lr: 0.000002  loss: 0.4417 (0.4257)  class_acc: 0.8036 (0.8083)  loss_scale: 8192.0000 (9692.8244)  weight_decay: 0.0500 (0.0500)  time: 1.4591  data: 0.0004  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.37 GB / 503.51 GB
Epoch: [4]  [140/893]  eta: 0:19:40  lr: 0.001819  min_lr: 0.000002  loss: 0.4402 (0.4249)  class_acc: 0.8036 (0.8088)  loss_scale: 8192.0000 (9586.3830)  weight_decay: 0.0500 (0.0500)  time: 1.4570  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.30 GB / 503.51 GB
Epoch: [4]  [150/893]  eta: 0:19:19  lr: 0.001824  min_lr: 0.000002  loss: 0.4290 (0.4267)  class_acc: 0.8036 (0.8079)  loss_scale: 8192.0000 (9494.0397)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
[2025-03-10 22:01:50,369] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:01:50,369] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.79 GB / 503.51 GB
Epoch: [4]  [160/893]  eta: 0:19:00  lr: 0.001829  min_lr: 0.000002  loss: 0.4290 (0.4287)  class_acc: 0.7857 (0.8066)  loss_scale: 8192.0000 (9820.2236)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.33 GB / 503.51 GB
Epoch: [4]  [170/893]  eta: 0:18:40  lr: 0.001834  min_lr: 0.000002  loss: 0.4089 (0.4269)  class_acc: 0.8214 (0.8072)  loss_scale: 16384.0000 (10204.0702)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.70 GB / 503.51 GB
Epoch: [4]  [180/893]  eta: 0:18:22  lr: 0.001839  min_lr: 0.000002  loss: 0.3943 (0.4271)  class_acc: 0.8214 (0.8069)  loss_scale: 16384.0000 (10545.5028)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.66 GB / 503.51 GB
Epoch: [4]  [190/893]  eta: 0:18:03  lr: 0.001844  min_lr: 0.000002  loss: 0.4238 (0.4282)  class_acc: 0.7857 (0.8061)  loss_scale: 16384.0000 (10851.1832)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0003  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.61 GB / 503.51 GB
Epoch: [4]  [200/893]  eta: 0:17:45  lr: 0.001849  min_lr: 0.000002  loss: 0.4238 (0.4276)  class_acc: 0.8036 (0.8061)  loss_scale: 16384.0000 (11126.4478)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0003  max mem: 31081
[2025-03-10 22:03:13,845] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 3778
[2025-03-10 22:03:13,845] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 22:03:13,845] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.57 GB / 503.51 GB
Epoch: [4]  [210/893]  eta: 0:17:27  lr: 0.001853  min_lr: 0.000002  loss: 0.4031 (0.4264)  class_acc: 0.8214 (0.8066)  loss_scale: 16384.0000 (11336.7962)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.09 GB / 503.51 GB
Epoch: [4]  [220/893]  eta: 0:17:10  lr: 0.001858  min_lr: 0.000002  loss: 0.4062 (0.4259)  class_acc: 0.8214 (0.8069)  loss_scale: 8192.0000 (11194.4977)  weight_decay: 0.0500 (0.0500)  time: 1.4658  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.70 GB / 503.51 GB
Epoch: [4]  [230/893]  eta: 0:16:53  lr: 0.001863  min_lr: 0.000002  loss: 0.4360 (0.4279)  class_acc: 0.7857 (0.8057)  loss_scale: 8192.0000 (11064.5195)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.94 GB / 503.51 GB
Epoch: [4]  [240/893]  eta: 0:16:36  lr: 0.001868  min_lr: 0.000002  loss: 0.4397 (0.4286)  class_acc: 0.7857 (0.8048)  loss_scale: 8192.0000 (10945.3278)  weight_decay: 0.0500 (0.0500)  time: 1.4711  data: 0.0004  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.17 GB / 503.51 GB
Epoch: [4]  [250/893]  eta: 0:16:19  lr: 0.001873  min_lr: 0.000002  loss: 0.4380 (0.4287)  class_acc: 0.7857 (0.8049)  loss_scale: 8192.0000 (10835.6335)  weight_decay: 0.0500 (0.0500)  time: 1.4703  data: 0.0003  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.65 GB / 503.51 GB
Epoch: [4]  [260/893]  eta: 0:16:02  lr: 0.001878  min_lr: 0.000002  loss: 0.4182 (0.4289)  class_acc: 0.7857 (0.8044)  loss_scale: 8192.0000 (10734.3448)  weight_decay: 0.0500 (0.0500)  time: 1.4589  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.17 GB / 503.51 GB
Epoch: [4]  [270/893]  eta: 0:15:46  lr: 0.001883  min_lr: 0.000002  loss: 0.4226 (0.4295)  class_acc: 0.8036 (0.8042)  loss_scale: 8192.0000 (10640.5314)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.03 GB / 503.51 GB
Epoch: [4]  [280/893]  eta: 0:15:29  lr: 0.001888  min_lr: 0.000002  loss: 0.4465 (0.4305)  class_acc: 0.8036 (0.8042)  loss_scale: 8192.0000 (10553.3950)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.23 GB / 503.51 GB
Epoch: [4]  [290/893]  eta: 0:15:13  lr: 0.001893  min_lr: 0.000002  loss: 0.4272 (0.4314)  class_acc: 0.7857 (0.8037)  loss_scale: 8192.0000 (10472.2474)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.08 GB / 503.51 GB
Epoch: [4]  [300/893]  eta: 0:14:57  lr: 0.001898  min_lr: 0.000002  loss: 0.4172 (0.4304)  class_acc: 0.7857 (0.8040)  loss_scale: 8192.0000 (10396.4917)  weight_decay: 0.0500 (0.0500)  time: 1.4603  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.62 GB / 503.51 GB
Epoch: [4]  [310/893]  eta: 0:14:41  lr: 0.001902  min_lr: 0.000002  loss: 0.3923 (0.4296)  class_acc: 0.8036 (0.8043)  loss_scale: 8192.0000 (10325.6077)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.31 GB / 503.51 GB
Epoch: [4]  [320/893]  eta: 0:14:25  lr: 0.001907  min_lr: 0.000002  loss: 0.3945 (0.4284)  class_acc: 0.8393 (0.8052)  loss_scale: 8192.0000 (10259.1402)  weight_decay: 0.0500 (0.0500)  time: 1.4663  data: 0.0002  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.46 GB / 503.51 GB
Epoch: [4]  [330/893]  eta: 0:14:09  lr: 0.001912  min_lr: 0.000002  loss: 0.3960 (0.4277)  class_acc: 0.8393 (0.8054)  loss_scale: 8192.0000 (10196.6888)  weight_decay: 0.0500 (0.0500)  time: 1.4681  data: 0.0002  max mem: 31081
[2025-03-10 22:06:22,900] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:06:22,900] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.58 GB / 503.51 GB
Epoch: [4]  [340/893]  eta: 0:13:53  lr: 0.001917  min_lr: 0.000003  loss: 0.4309 (0.4281)  class_acc: 0.8214 (0.8055)  loss_scale: 8192.0000 (10185.9472)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0002  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.68 GB / 503.51 GB
Epoch: [4]  [350/893]  eta: 0:13:38  lr: 0.001922  min_lr: 0.000003  loss: 0.4309 (0.4277)  class_acc: 0.8036 (0.8057)  loss_scale: 16384.0000 (10362.5299)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0004  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.81 GB / 503.51 GB
Epoch: [4]  [360/893]  eta: 0:13:22  lr: 0.001927  min_lr: 0.000003  loss: 0.4143 (0.4270)  class_acc: 0.8036 (0.8057)  loss_scale: 16384.0000 (10529.3296)  weight_decay: 0.0500 (0.0500)  time: 1.4649  data: 0.0004  max mem: 31081
[2025-03-10 22:07:00,967] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 3933
[2025-03-10 22:07:00,968] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 22:07:00,968] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.83 GB / 503.51 GB
Epoch: [4]  [370/893]  eta: 0:13:06  lr: 0.001932  min_lr: 0.000003  loss: 0.3931 (0.4274)  class_acc: 0.8214 (0.8058)  loss_scale: 16384.0000 (10554.6523)  weight_decay: 0.0500 (0.0500)  time: 1.4569  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.91 GB / 503.51 GB
Epoch: [4]  [380/893]  eta: 0:12:51  lr: 0.001937  min_lr: 0.000003  loss: 0.3806 (0.4267)  class_acc: 0.8036 (0.8057)  loss_scale: 8192.0000 (10492.6404)  weight_decay: 0.0500 (0.0500)  time: 1.4611  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.36 GB / 503.51 GB
Epoch: [4]  [390/893]  eta: 0:12:35  lr: 0.001942  min_lr: 0.000003  loss: 0.4136 (0.4264)  class_acc: 0.7857 (0.8054)  loss_scale: 8192.0000 (10433.8005)  weight_decay: 0.0500 (0.0500)  time: 1.4685  data: 0.0003  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.93 GB / 503.51 GB
Epoch: [4]  [400/893]  eta: 0:12:20  lr: 0.001947  min_lr: 0.000003  loss: 0.3762 (0.4254)  class_acc: 0.8036 (0.8060)  loss_scale: 8192.0000 (10377.8953)  weight_decay: 0.0500 (0.0500)  time: 1.4715  data: 0.0002  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.40 GB / 503.51 GB
Epoch: [4]  [410/893]  eta: 0:12:04  lr: 0.001952  min_lr: 0.000003  loss: 0.3789 (0.4255)  class_acc: 0.8036 (0.8056)  loss_scale: 8192.0000 (10324.7105)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.74 GB / 503.51 GB
Epoch: [4]  [420/893]  eta: 0:11:49  lr: 0.001956  min_lr: 0.000003  loss: 0.3926 (0.4250)  class_acc: 0.7857 (0.8055)  loss_scale: 8192.0000 (10274.0523)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.94 GB / 503.51 GB
Epoch: [4]  [430/893]  eta: 0:11:33  lr: 0.001961  min_lr: 0.000003  loss: 0.3750 (0.4244)  class_acc: 0.8214 (0.8059)  loss_scale: 8192.0000 (10225.7448)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0003  max mem: 31081
[2025-03-10 22:08:37,689] [INFO] [logging.py:129:log_dist] [Rank 0] step=4000, skipped=20, lr=[2.562289804257456e-06, 2.562289804257456e-06, 4.2704830070957595e-06, 4.2704830070957595e-06, 7.117471678492933e-06, 7.117471678492933e-06, 1.1862452797488222e-05, 1.1862452797488222e-05, 1.9770754662480373e-05, 1.9770754662480373e-05, 3.295125777080062e-05, 3.295125777080062e-05, 5.491876295133437e-05, 5.491876295133437e-05, 9.15312715855573e-05, 9.15312715855573e-05, 0.00015255211930926215, 0.00015255211930926215, 0.00025425353218210364, 0.00025425353218210364, 0.00042375588697017265, 0.00042375588697017265, 0.0007062598116169545, 0.0007062598116169545, 0.0011770996860282574, 0.0011770996860282574, 0.001961832810047096, 0.001961832810047096], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-10 22:08:37,690] [INFO] [timer.py:264:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=61.161493926226875, CurrSamplesPerSec=61.592172843442654, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
[2025-03-10 22:08:39,210] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 4000
[2025-03-10 22:08:39,210] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-10 22:08:39,210] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.88 GB / 503.51 GB
Epoch: [4]  [440/893]  eta: 0:11:18  lr: 0.001966  min_lr: 0.000003  loss: 0.4058 (0.4244)  class_acc: 0.8214 (0.8061)  loss_scale: 8192.0000 (10096.0363)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.85 GB / 503.51 GB
Epoch: [4]  [450/893]  eta: 0:11:03  lr: 0.001971  min_lr: 0.000003  loss: 0.3911 (0.4243)  class_acc: 0.8214 (0.8060)  loss_scale: 4096.0000 (9962.9978)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.09 GB / 503.51 GB
Epoch: [4]  [460/893]  eta: 0:10:48  lr: 0.001976  min_lr: 0.000003  loss: 0.3911 (0.4242)  class_acc: 0.8214 (0.8059)  loss_scale: 4096.0000 (9835.7310)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.41 GB / 503.51 GB
Epoch: [4]  [470/893]  eta: 0:10:32  lr: 0.001981  min_lr: 0.000003  loss: 0.3892 (0.4244)  class_acc: 0.8036 (0.8053)  loss_scale: 4096.0000 (9713.8684)  weight_decay: 0.0500 (0.0500)  time: 1.4559  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.14 GB / 503.51 GB
Epoch: [4]  [480/893]  eta: 0:10:17  lr: 0.001986  min_lr: 0.000003  loss: 0.4160 (0.4246)  class_acc: 0.7857 (0.8055)  loss_scale: 4096.0000 (9597.0728)  weight_decay: 0.0500 (0.0500)  time: 1.4587  data: 0.0002  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.92 GB / 503.51 GB
Epoch: [4]  [490/893]  eta: 0:10:02  lr: 0.001991  min_lr: 0.000003  loss: 0.4192 (0.4245)  class_acc: 0.8214 (0.8056)  loss_scale: 4096.0000 (9485.0346)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0002  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.17 GB / 503.51 GB
Epoch: [4]  [500/893]  eta: 0:09:47  lr: 0.001996  min_lr: 0.000003  loss: 0.4192 (0.4247)  class_acc: 0.8214 (0.8055)  loss_scale: 4096.0000 (9377.4691)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.21 GB / 503.51 GB
Epoch: [4]  [510/893]  eta: 0:09:31  lr: 0.002001  min_lr: 0.000003  loss: 0.4148 (0.4243)  class_acc: 0.8036 (0.8057)  loss_scale: 4096.0000 (9274.1135)  weight_decay: 0.0500 (0.0500)  time: 1.4587  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.57 GB / 503.51 GB
Epoch: [4]  [520/893]  eta: 0:09:16  lr: 0.002005  min_lr: 0.000003  loss: 0.4148 (0.4242)  class_acc: 0.7857 (0.8054)  loss_scale: 4096.0000 (9174.7255)  weight_decay: 0.0500 (0.0500)  time: 1.4590  data: 0.0002  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.35 GB / 503.51 GB
Epoch: [4]  [530/893]  eta: 0:09:01  lr: 0.002010  min_lr: 0.000003  loss: 0.3950 (0.4233)  class_acc: 0.8036 (0.8055)  loss_scale: 4096.0000 (9079.0810)  weight_decay: 0.0500 (0.0500)  time: 1.4693  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.94 GB / 503.51 GB
Epoch: [4]  [540/893]  eta: 0:08:46  lr: 0.002015  min_lr: 0.000003  loss: 0.3950 (0.4230)  class_acc: 0.8393 (0.8059)  loss_scale: 4096.0000 (8986.9723)  weight_decay: 0.0500 (0.0500)  time: 1.4714  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.86 GB / 503.51 GB
Epoch: [4]  [550/893]  eta: 0:08:31  lr: 0.002020  min_lr: 0.000003  loss: 0.4229 (0.4229)  class_acc: 0.8214 (0.8058)  loss_scale: 4096.0000 (8898.2069)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.41 GB / 503.51 GB
Epoch: [4]  [560/893]  eta: 0:08:16  lr: 0.002025  min_lr: 0.000003  loss: 0.3718 (0.4224)  class_acc: 0.8214 (0.8061)  loss_scale: 4096.0000 (8812.6061)  weight_decay: 0.0500 (0.0500)  time: 1.4729  data: 0.0004  max mem: 31081
[2025-03-10 22:11:48,260] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:11:48,260] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.06 GB / 503.51 GB
Epoch: [4]  [570/893]  eta: 0:08:01  lr: 0.002030  min_lr: 0.000003  loss: 0.3684 (0.4222)  class_acc: 0.8214 (0.8063)  loss_scale: 4096.0000 (8801.7373)  weight_decay: 0.0500 (0.0500)  time: 1.4780  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.27 GB / 503.51 GB
Epoch: [4]  [580/893]  eta: 0:07:46  lr: 0.002035  min_lr: 0.000003  loss: 0.3804 (0.4222)  class_acc: 0.8214 (0.8064)  loss_scale: 8192.0000 (8791.2427)  weight_decay: 0.0500 (0.0500)  time: 1.4727  data: 0.0002  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.98 GB / 503.51 GB
Epoch: [4]  [590/893]  eta: 0:07:31  lr: 0.002040  min_lr: 0.000003  loss: 0.4204 (0.4223)  class_acc: 0.8036 (0.8065)  loss_scale: 8192.0000 (8781.1032)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0002  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.97 GB / 503.51 GB
Epoch: [4]  [600/893]  eta: 0:07:16  lr: 0.002045  min_lr: 0.000003  loss: 0.4204 (0.4221)  class_acc: 0.8036 (0.8069)  loss_scale: 8192.0000 (8771.3012)  weight_decay: 0.0500 (0.0500)  time: 1.4583  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.15 GB / 503.51 GB
Epoch: [4]  [610/893]  eta: 0:07:01  lr: 0.002050  min_lr: 0.000003  loss: 0.4153 (0.4223)  class_acc: 0.8036 (0.8069)  loss_scale: 8192.0000 (8761.8200)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.56 GB / 503.51 GB
Epoch: [4]  [620/893]  eta: 0:06:46  lr: 0.002055  min_lr: 0.000003  loss: 0.4104 (0.4225)  class_acc: 0.8036 (0.8068)  loss_scale: 8192.0000 (8752.6441)  weight_decay: 0.0500 (0.0500)  time: 1.4752  data: 0.0003  max mem: 31081
[2025-03-10 22:13:25,059] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 4195
[2025-03-10 22:13:25,059] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-10 22:13:25,059] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.65 GB / 503.51 GB
Epoch: [4]  [630/893]  eta: 0:06:31  lr: 0.002059  min_lr: 0.000003  loss: 0.4170 (0.4230)  class_acc: 0.8036 (0.8063)  loss_scale: 8192.0000 (8717.7940)  weight_decay: 0.0500 (0.0500)  time: 1.4739  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.54 GB / 503.51 GB
Epoch: [4]  [640/893]  eta: 0:06:16  lr: 0.002064  min_lr: 0.000003  loss: 0.4233 (0.4226)  class_acc: 0.8036 (0.8066)  loss_scale: 4096.0000 (8645.6911)  weight_decay: 0.0500 (0.0500)  time: 1.4608  data: 0.0003  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.08 GB / 503.51 GB
Epoch: [4]  [650/893]  eta: 0:06:01  lr: 0.002069  min_lr: 0.000003  loss: 0.4287 (0.4232)  class_acc: 0.8036 (0.8063)  loss_scale: 4096.0000 (8575.8034)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.81 GB / 503.51 GB
Epoch: [4]  [660/893]  eta: 0:05:46  lr: 0.002074  min_lr: 0.000003  loss: 0.4639 (0.4239)  class_acc: 0.7857 (0.8059)  loss_scale: 4096.0000 (8508.0303)  weight_decay: 0.0500 (0.0500)  time: 1.4649  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.17 GB / 503.51 GB
Epoch: [4]  [670/893]  eta: 0:05:31  lr: 0.002079  min_lr: 0.000003  loss: 0.4346 (0.4236)  class_acc: 0.8036 (0.8062)  loss_scale: 4096.0000 (8442.2772)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0002  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.44 GB / 503.51 GB
Epoch: [4]  [680/893]  eta: 0:05:16  lr: 0.002084  min_lr: 0.000003  loss: 0.3711 (0.4232)  class_acc: 0.8393 (0.8064)  loss_scale: 4096.0000 (8378.4552)  weight_decay: 0.0500 (0.0500)  time: 1.4698  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.70 GB / 503.51 GB
Epoch: [4]  [690/893]  eta: 0:05:01  lr: 0.002089  min_lr: 0.000003  loss: 0.4158 (0.4235)  class_acc: 0.8036 (0.8063)  loss_scale: 4096.0000 (8316.4805)  weight_decay: 0.0500 (0.0500)  time: 1.4627  data: 0.0004  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.07 GB / 503.51 GB
Epoch: [4]  [700/893]  eta: 0:04:46  lr: 0.002094  min_lr: 0.000003  loss: 0.4216 (0.4236)  class_acc: 0.7857 (0.8061)  loss_scale: 4096.0000 (8256.2739)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.06 GB / 503.51 GB
Epoch: [4]  [710/893]  eta: 0:04:31  lr: 0.002099  min_lr: 0.000003  loss: 0.4551 (0.4244)  class_acc: 0.7679 (0.8058)  loss_scale: 4096.0000 (8197.7609)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0002  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.98 GB / 503.51 GB
Epoch: [4]  [720/893]  eta: 0:04:16  lr: 0.002104  min_lr: 0.000003  loss: 0.4458 (0.4243)  class_acc: 0.8036 (0.8060)  loss_scale: 4096.0000 (8140.8710)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0002  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.41 GB / 503.51 GB
Epoch: [4]  [730/893]  eta: 0:04:02  lr: 0.002109  min_lr: 0.000003  loss: 0.4028 (0.4238)  class_acc: 0.8214 (0.8064)  loss_scale: 4096.0000 (8085.5376)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.74 GB / 503.51 GB
Epoch: [4]  [740/893]  eta: 0:03:47  lr: 0.002113  min_lr: 0.000003  loss: 0.4033 (0.4239)  class_acc: 0.8036 (0.8064)  loss_scale: 4096.0000 (8031.6977)  weight_decay: 0.0500 (0.0500)  time: 1.4619  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.08 GB / 503.51 GB
Epoch: [4]  [750/893]  eta: 0:03:32  lr: 0.002118  min_lr: 0.000003  loss: 0.4006 (0.4237)  class_acc: 0.8036 (0.8064)  loss_scale: 4096.0000 (7979.2916)  weight_decay: 0.0500 (0.0500)  time: 1.4611  data: 0.0003  max mem: 31081
[2025-03-10 22:16:33,876] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:16:33,876] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.96 GB / 503.51 GB
Epoch: [4]  [760/893]  eta: 0:03:17  lr: 0.002123  min_lr: 0.000003  loss: 0.3931 (0.4237)  class_acc: 0.8036 (0.8063)  loss_scale: 4096.0000 (7955.1748)  weight_decay: 0.0500 (0.0500)  time: 1.4595  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.41 GB / 503.51 GB
Epoch: [4]  [770/893]  eta: 0:03:02  lr: 0.002128  min_lr: 0.000003  loss: 0.4031 (0.4235)  class_acc: 0.8036 (0.8064)  loss_scale: 8192.0000 (7958.2464)  weight_decay: 0.0500 (0.0500)  time: 1.4593  data: 0.0004  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.91 GB / 503.51 GB
Epoch: [4]  [780/893]  eta: 0:02:47  lr: 0.002133  min_lr: 0.000003  loss: 0.4114 (0.4235)  class_acc: 0.8036 (0.8064)  loss_scale: 8192.0000 (7961.2394)  weight_decay: 0.0500 (0.0500)  time: 1.4565  data: 0.0004  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.20 GB / 503.51 GB
Epoch: [4]  [790/893]  eta: 0:02:32  lr: 0.002138  min_lr: 0.000003  loss: 0.4009 (0.4231)  class_acc: 0.8214 (0.8066)  loss_scale: 8192.0000 (7964.1568)  weight_decay: 0.0500 (0.0500)  time: 1.4581  data: 0.0004  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.94 GB / 503.51 GB
Epoch: [4]  [800/893]  eta: 0:02:17  lr: 0.002143  min_lr: 0.000003  loss: 0.3940 (0.4227)  class_acc: 0.8214 (0.8069)  loss_scale: 8192.0000 (7967.0012)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0004  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.69 GB / 503.51 GB
Epoch: [4]  [810/893]  eta: 0:02:03  lr: 0.002148  min_lr: 0.000003  loss: 0.4072 (0.4229)  class_acc: 0.8036 (0.8068)  loss_scale: 8192.0000 (7969.7756)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0005  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.94 GB / 503.51 GB
Epoch: [4]  [820/893]  eta: 0:01:48  lr: 0.002153  min_lr: 0.000003  loss: 0.4045 (0.4229)  class_acc: 0.8036 (0.8070)  loss_scale: 8192.0000 (7972.4823)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0005  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.80 GB / 503.51 GB
Epoch: [4]  [830/893]  eta: 0:01:33  lr: 0.002158  min_lr: 0.000003  loss: 0.4045 (0.4227)  class_acc: 0.8393 (0.8072)  loss_scale: 8192.0000 (7975.1239)  weight_decay: 0.0500 (0.0500)  time: 1.4627  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.76 GB / 503.51 GB
Epoch: [4]  [840/893]  eta: 0:01:18  lr: 0.002162  min_lr: 0.000003  loss: 0.4167 (0.4226)  class_acc: 0.8393 (0.8075)  loss_scale: 8192.0000 (7977.7027)  weight_decay: 0.0500 (0.0500)  time: 1.4594  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.37 GB / 503.51 GB
Epoch: [4]  [850/893]  eta: 0:01:03  lr: 0.002167  min_lr: 0.000003  loss: 0.3933 (0.4221)  class_acc: 0.8214 (0.8076)  loss_scale: 8192.0000 (7980.2209)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0004  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.30 GB / 503.51 GB
Epoch: [4]  [860/893]  eta: 0:00:48  lr: 0.002172  min_lr: 0.000003  loss: 0.4067 (0.4223)  class_acc: 0.8214 (0.8078)  loss_scale: 8192.0000 (7982.6806)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.11 GB / 503.51 GB
Epoch: [4]  [870/893]  eta: 0:00:34  lr: 0.002177  min_lr: 0.000003  loss: 0.4312 (0.4223)  class_acc: 0.8214 (0.8078)  loss_scale: 8192.0000 (7985.0838)  weight_decay: 0.0500 (0.0500)  time: 1.4415  data: 0.0002  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.69 GB / 503.51 GB
Epoch: [4]  [880/893]  eta: 0:00:19  lr: 0.002182  min_lr: 0.000003  loss: 0.4136 (0.4221)  class_acc: 0.8036 (0.8081)  loss_scale: 8192.0000 (7987.4325)  weight_decay: 0.0500 (0.0500)  time: 1.4516  data: 0.0001  max mem: 31081
[2025-03-10 22:19:40,707] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:19:40,708] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.53 GB / 503.51 GB
Epoch: [4]  [890/893]  eta: 0:00:04  lr: 0.002187  min_lr: 0.000003  loss: 0.3918 (0.4219)  class_acc: 0.8214 (0.8080)  loss_scale: 8192.0000 (8054.0875)  weight_decay: 0.0500 (0.0500)  time: 1.4517  data: 0.0002  max mem: 31081
Epoch: [4]  [892/893]  eta: 0:00:01  lr: 0.002188  min_lr: 0.000003  loss: 0.3660 (0.4217)  class_acc: 0.8214 (0.8082)  loss_scale: 8192.0000 (8063.4260)  weight_decay: 0.0500 (0.0500)  time: 1.3939  data: 0.0002  max mem: 31081
Epoch: [4] Total time: 0:22:01 (1.4795 s / it)
Averaged stats: lr: 0.002188  min_lr: 0.000003  loss: 0.3660 (0.4217)  class_acc: 0.8214 (0.8082)  loss_scale: 8192.0000 (8063.4260)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:17:42  loss: 0.6464 (0.6464)  acc: 75.0000 (75.0000)  time: 11.3497  data: 10.8025  max mem: 31081
Val:  [ 10/728]  eta: 0:18:34  loss: 0.4016 (0.4554)  acc: 79.7619 (79.2208)  time: 1.5515  data: 1.0300  max mem: 31081
Val:  [ 20/728]  eta: 0:14:23  loss: 0.4051 (0.4416)  acc: 79.7619 (79.5351)  time: 0.7134  data: 0.1941  max mem: 31081
Val:  [ 30/728]  eta: 0:12:52  loss: 0.4332 (0.4538)  acc: 79.7619 (78.4178)  time: 0.8631  data: 0.3419  max mem: 31081
Val:  [ 40/728]  eta: 0:12:00  loss: 0.3681 (0.4567)  acc: 79.7619 (78.1359)  time: 0.8665  data: 0.3442  max mem: 31081
Val:  [ 50/728]  eta: 0:11:25  loss: 0.3463 (0.4312)  acc: 82.1429 (79.5518)  time: 0.8613  data: 0.3383  max mem: 31081
Val:  [ 60/728]  eta: 0:10:30  loss: 0.3952 (0.4403)  acc: 78.5714 (79.3326)  time: 0.7303  data: 0.2076  max mem: 31081
Val:  [ 70/728]  eta: 0:09:59  loss: 0.5340 (0.4400)  acc: 77.3810 (79.0074)  time: 0.6545  data: 0.1340  max mem: 31081
Val:  [ 80/728]  eta: 0:09:44  loss: 0.4686 (0.4556)  acc: 78.5714 (78.4392)  time: 0.7752  data: 0.2537  max mem: 31081
Val:  [ 90/728]  eta: 0:09:30  loss: 0.5021 (0.4672)  acc: 76.1905 (77.9958)  time: 0.8335  data: 0.3100  max mem: 31081
Val:  [100/728]  eta: 0:09:18  loss: 0.3663 (0.4584)  acc: 80.9524 (78.5007)  time: 0.8386  data: 0.3178  max mem: 31081
Val:  [110/728]  eta: 0:09:09  loss: 0.3861 (0.4616)  acc: 82.1429 (78.5500)  time: 0.8695  data: 0.3495  max mem: 31081
Val:  [120/728]  eta: 0:09:00  loss: 0.3676 (0.4561)  acc: 83.3333 (79.0142)  time: 0.8842  data: 0.3649  max mem: 31081
Val:  [130/728]  eta: 0:08:34  loss: 0.3399 (0.4622)  acc: 82.1429 (78.7714)  time: 0.6997  data: 0.1823  max mem: 31081
Val:  [140/728]  eta: 0:08:22  loss: 0.4055 (0.4681)  acc: 75.0000 (78.3013)  time: 0.6524  data: 0.1341  max mem: 31081
Val:  [150/728]  eta: 0:08:09  loss: 0.4585 (0.4695)  acc: 76.1905 (78.2876)  time: 0.7624  data: 0.2416  max mem: 31081
Val:  [160/728]  eta: 0:08:00  loss: 0.4745 (0.4761)  acc: 76.1905 (78.0686)  time: 0.7856  data: 0.2624  max mem: 31081
Val:  [170/728]  eta: 0:07:51  loss: 0.4364 (0.4752)  acc: 78.5714 (78.2303)  time: 0.8300  data: 0.3067  max mem: 31081
Val:  [180/728]  eta: 0:07:41  loss: 0.4227 (0.4782)  acc: 79.7619 (78.1505)  time: 0.8045  data: 0.2808  max mem: 31081
Val:  [190/728]  eta: 0:07:25  loss: 0.4661 (0.4773)  acc: 77.3810 (78.1351)  time: 0.6783  data: 0.1545  max mem: 31081
Val:  [200/728]  eta: 0:07:20  loss: 0.4661 (0.4754)  acc: 77.3810 (78.2338)  time: 0.7695  data: 0.2460  max mem: 31081
Val:  [210/728]  eta: 0:07:12  loss: 0.4415 (0.4770)  acc: 80.9524 (78.2668)  time: 0.8932  data: 0.3709  max mem: 31081
Val:  [220/728]  eta: 0:07:04  loss: 0.3803 (0.4743)  acc: 83.3333 (78.4314)  time: 0.8356  data: 0.3129  max mem: 31081
Val:  [230/728]  eta: 0:06:55  loss: 0.3803 (0.4766)  acc: 79.7619 (78.3034)  time: 0.8314  data: 0.3096  max mem: 31081
Val:  [240/728]  eta: 0:06:44  loss: 0.3985 (0.4738)  acc: 79.7619 (78.4578)  time: 0.7633  data: 0.2430  max mem: 31081
Val:  [250/728]  eta: 0:06:32  loss: 0.4465 (0.4738)  acc: 76.1905 (78.4671)  time: 0.6652  data: 0.1442  max mem: 31081
Val:  [260/728]  eta: 0:06:25  loss: 0.4465 (0.4733)  acc: 76.1905 (78.5714)  time: 0.7528  data: 0.2310  max mem: 31081
Val:  [270/728]  eta: 0:06:17  loss: 0.3678 (0.4740)  acc: 79.7619 (78.6022)  time: 0.8605  data: 0.3370  max mem: 31081
Val:  [280/728]  eta: 0:06:11  loss: 0.4520 (0.4735)  acc: 79.7619 (78.6901)  time: 0.8986  data: 0.3759  max mem: 31081
Val:  [290/728]  eta: 0:06:02  loss: 0.3926 (0.4715)  acc: 80.9524 (78.7842)  time: 0.8869  data: 0.3642  max mem: 31081
Val:  [300/728]  eta: 0:05:49  loss: 0.3645 (0.4700)  acc: 82.1429 (78.8364)  time: 0.6667  data: 0.1432  max mem: 31081
Val:  [310/728]  eta: 0:05:42  loss: 0.4033 (0.4718)  acc: 78.5714 (78.7628)  time: 0.6814  data: 0.1584  max mem: 31081
Val:  [320/728]  eta: 0:05:33  loss: 0.4125 (0.4707)  acc: 80.9524 (78.7680)  time: 0.8176  data: 0.2961  max mem: 31081
Val:  [330/728]  eta: 0:05:26  loss: 0.4423 (0.4697)  acc: 79.7619 (78.7441)  time: 0.8472  data: 0.3253  max mem: 31081
Val:  [340/728]  eta: 0:05:19  loss: 0.3674 (0.4651)  acc: 80.9524 (78.9345)  time: 0.9052  data: 0.3803  max mem: 31081
Val:  [350/728]  eta: 0:05:10  loss: 0.3307 (0.4649)  acc: 83.3333 (78.9852)  time: 0.8544  data: 0.3302  max mem: 31081
Val:  [360/728]  eta: 0:04:59  loss: 0.3369 (0.4616)  acc: 83.3333 (79.1452)  time: 0.6580  data: 0.1375  max mem: 31081
Val:  [370/728]  eta: 0:04:51  loss: 0.4194 (0.4651)  acc: 79.7619 (78.9757)  time: 0.6990  data: 0.1760  max mem: 31081
Val:  [380/728]  eta: 0:04:43  loss: 0.4831 (0.4677)  acc: 76.1905 (78.9276)  time: 0.8461  data: 0.3240  max mem: 31081
Val:  [390/728]  eta: 0:04:35  loss: 0.3437 (0.4638)  acc: 82.1429 (79.1195)  time: 0.8333  data: 0.3123  max mem: 31081
Val:  [400/728]  eta: 0:04:28  loss: 0.3054 (0.4622)  acc: 83.3333 (79.2246)  time: 0.8857  data: 0.3620  max mem: 31081
Val:  [410/728]  eta: 0:04:20  loss: 0.3330 (0.4605)  acc: 79.7619 (79.3100)  time: 0.8994  data: 0.3779  max mem: 31081
Val:  [420/728]  eta: 0:04:10  loss: 0.4196 (0.4608)  acc: 79.7619 (79.3293)  time: 0.7027  data: 0.1803  max mem: 31081
Val:  [430/728]  eta: 0:04:02  loss: 0.4839 (0.4622)  acc: 77.3810 (79.2454)  time: 0.7084  data: 0.1845  max mem: 31081
Val:  [440/728]  eta: 0:03:54  loss: 0.4109 (0.4604)  acc: 82.1429 (79.3435)  time: 0.8523  data: 0.3292  max mem: 31081
Val:  [450/728]  eta: 0:03:47  loss: 0.3807 (0.4624)  acc: 84.5238 (79.2129)  time: 0.8504  data: 0.3278  max mem: 31081
Val:  [460/728]  eta: 0:03:39  loss: 0.3905 (0.4641)  acc: 79.7619 (79.1473)  time: 0.8663  data: 0.3443  max mem: 31081
Val:  [470/728]  eta: 0:03:30  loss: 0.3320 (0.4622)  acc: 83.3333 (79.2412)  time: 0.8219  data: 0.2998  max mem: 31081
Val:  [480/728]  eta: 0:03:21  loss: 0.3263 (0.4610)  acc: 84.5238 (79.3065)  time: 0.6593  data: 0.1386  max mem: 31081
Val:  [490/728]  eta: 0:03:13  loss: 0.3718 (0.4601)  acc: 84.5238 (79.3206)  time: 0.7003  data: 0.1773  max mem: 31081
Val:  [500/728]  eta: 0:03:05  loss: 0.3874 (0.4595)  acc: 80.9524 (79.3437)  time: 0.8651  data: 0.3388  max mem: 31081
Val:  [510/728]  eta: 0:02:57  loss: 0.3437 (0.4582)  acc: 83.3333 (79.3961)  time: 0.8920  data: 0.3684  max mem: 31081
Val:  [520/728]  eta: 0:02:49  loss: 0.3799 (0.4597)  acc: 82.1429 (79.3209)  time: 0.8708  data: 0.3489  max mem: 31081
Val:  [530/728]  eta: 0:02:41  loss: 0.5175 (0.4592)  acc: 76.1905 (79.3897)  time: 0.7641  data: 0.2428  max mem: 31081
Val:  [540/728]  eta: 0:02:31  loss: 0.4985 (0.4619)  acc: 77.3810 (79.2448)  time: 0.6234  data: 0.1008  max mem: 31081
Val:  [550/728]  eta: 0:02:23  loss: 0.4333 (0.4622)  acc: 77.3810 (79.2045)  time: 0.6774  data: 0.1550  max mem: 31081
Val:  [560/728]  eta: 0:02:15  loss: 0.4069 (0.4613)  acc: 80.9524 (79.2760)  time: 0.7668  data: 0.2473  max mem: 31081
Val:  [570/728]  eta: 0:02:07  loss: 0.4069 (0.4632)  acc: 80.9524 (79.2136)  time: 0.8198  data: 0.2981  max mem: 31081
Val:  [580/728]  eta: 0:01:59  loss: 0.4380 (0.4630)  acc: 78.5714 (79.2435)  time: 0.8724  data: 0.3482  max mem: 31081
Val:  [590/728]  eta: 0:01:51  loss: 0.4122 (0.4637)  acc: 82.1429 (79.2261)  time: 0.7423  data: 0.2167  max mem: 31081
Val:  [600/728]  eta: 0:01:43  loss: 0.3931 (0.4638)  acc: 83.3333 (79.2568)  time: 0.7277  data: 0.2031  max mem: 31081
Val:  [610/728]  eta: 0:01:34  loss: 0.4766 (0.4662)  acc: 80.9524 (79.1228)  time: 0.7011  data: 0.1803  max mem: 31081
Val:  [620/728]  eta: 0:01:26  loss: 0.4766 (0.4678)  acc: 77.3810 (79.0871)  time: 0.7451  data: 0.2269  max mem: 31081
Val:  [630/728]  eta: 0:01:18  loss: 0.4165 (0.4668)  acc: 79.7619 (79.1167)  time: 0.8547  data: 0.3346  max mem: 31081
Val:  [640/728]  eta: 0:01:10  loss: 0.3736 (0.4657)  acc: 83.3333 (79.1546)  time: 0.8018  data: 0.2775  max mem: 31081
Val:  [650/728]  eta: 0:01:02  loss: 0.4343 (0.4661)  acc: 78.5714 (79.1292)  time: 0.7936  data: 0.2707  max mem: 31081
Val:  [660/728]  eta: 0:00:54  loss: 0.4332 (0.4652)  acc: 78.5714 (79.1802)  time: 0.8328  data: 0.3123  max mem: 31081
Val:  [670/728]  eta: 0:00:46  loss: 0.3982 (0.4655)  acc: 82.1429 (79.1835)  time: 0.6842  data: 0.1617  max mem: 31081
Val:  [680/728]  eta: 0:00:38  loss: 0.4205 (0.4646)  acc: 79.7619 (79.2182)  time: 0.6857  data: 0.1633  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3492 (0.4635)  acc: 83.3333 (79.2675)  time: 0.8908  data: 0.3703  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3737 (0.4635)  acc: 83.3333 (79.2388)  time: 0.9083  data: 0.3874  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4381 (0.4639)  acc: 75.0000 (79.2362)  time: 0.8499  data: 0.3289  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4484 (0.4637)  acc: 76.1905 (79.2583)  time: 0.7759  data: 0.2618  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4394 (0.4648)  acc: 79.7619 (79.1934)  time: 0.7515  data: 0.2618  max mem: 31081
Val: Total time: 0:09:42 (0.8006 s / it)
* Acc@1 79.193 AP 0.8050660490989685 loss 0.465
Accuracy of the network on the 61096 val videos: 79.2%
[2025-03-10 22:29:35,104] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestap is about to be saved!
[2025-03-10 22:29:35,106] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt
[2025-03-10 22:29:35,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt...
[2025-03-10 22:29:35,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt.
[2025-03-10 22:29:35,366] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestap is ready now!
[2025-03-10 22:29:35,367] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestacc is about to be saved!
[2025-03-10 22:29:35,370] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt
[2025-03-10 22:29:35,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt...
[2025-03-10 22:29:35,614] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt.
[2025-03-10 22:29:35,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestacc is ready now!
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.96 GB / 503.51 GB
Epoch: [5]  [  0/893]  eta: 3:38:02  lr: 0.002188  min_lr: 0.000003  loss: 0.3672 (0.3672)  class_acc: 0.8214 (0.8214)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 14.6495  data: 13.3421  max mem: 31081
[2025-03-10 22:29:58,103] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 4465
[2025-03-10 22:29:58,103] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 22:29:58,103] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.28 GB / 503.51 GB
Epoch: [5]  [ 10/893]  eta: 0:39:29  lr: 0.002187  min_lr: 0.000003  loss: 0.3672 (0.3775)  class_acc: 0.8393 (0.8295)  loss_scale: 8192.0000 (11915.6364)  weight_decay: 0.0500 (0.0500)  time: 2.6830  data: 1.2135  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.26 GB / 503.51 GB
Epoch: [5]  [ 20/893]  eta: 0:30:53  lr: 0.002187  min_lr: 0.000003  loss: 0.4077 (0.4007)  class_acc: 0.8036 (0.8163)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  time: 1.4972  data: 0.0006  max mem: 31081
[2025-03-10 22:30:22,081] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 4481
[2025-03-10 22:30:22,081] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-10 22:30:22,081] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.29 GB / 503.51 GB
Epoch: [5]  [ 30/893]  eta: 0:27:37  lr: 0.002187  min_lr: 0.000003  loss: 0.4180 (0.4066)  class_acc: 0.7857 (0.8128)  loss_scale: 4096.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5015  data: 0.0004  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.68 GB / 503.51 GB
Epoch: [5]  [ 40/893]  eta: 0:25:53  lr: 0.002187  min_lr: 0.000003  loss: 0.3958 (0.4038)  class_acc: 0.8036 (0.8127)  loss_scale: 4096.0000 (7192.9756)  weight_decay: 0.0500 (0.0500)  time: 1.5029  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.99 GB / 503.51 GB
Epoch: [5]  [ 50/893]  eta: 0:24:39  lr: 0.002187  min_lr: 0.000003  loss: 0.3914 (0.4044)  class_acc: 0.8214 (0.8162)  loss_scale: 4096.0000 (6585.7255)  weight_decay: 0.0500 (0.0500)  time: 1.4985  data: 0.0006  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.55 GB / 503.51 GB
Epoch: [5]  [ 60/893]  eta: 0:23:47  lr: 0.002187  min_lr: 0.000003  loss: 0.3799 (0.3967)  class_acc: 0.8393 (0.8217)  loss_scale: 4096.0000 (6177.5738)  weight_decay: 0.0500 (0.0500)  time: 1.4934  data: 0.0004  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.58 GB / 503.51 GB
Epoch: [5]  [ 70/893]  eta: 0:23:00  lr: 0.002187  min_lr: 0.000003  loss: 0.3838 (0.4001)  class_acc: 0.8393 (0.8217)  loss_scale: 4096.0000 (5884.3944)  weight_decay: 0.0500 (0.0500)  time: 1.4782  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.81 GB / 503.51 GB
Epoch: [5]  [ 80/893]  eta: 0:22:22  lr: 0.002187  min_lr: 0.000003  loss: 0.3838 (0.3965)  class_acc: 0.8393 (0.8236)  loss_scale: 4096.0000 (5663.6049)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.43 GB / 503.51 GB
Epoch: [5]  [ 90/893]  eta: 0:21:49  lr: 0.002187  min_lr: 0.000003  loss: 0.3887 (0.3988)  class_acc: 0.8214 (0.8210)  loss_scale: 4096.0000 (5491.3407)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.54 GB / 503.51 GB
Epoch: [5]  [100/893]  eta: 0:21:20  lr: 0.002187  min_lr: 0.000003  loss: 0.4126 (0.4008)  class_acc: 0.8036 (0.8198)  loss_scale: 4096.0000 (5353.1881)  weight_decay: 0.0500 (0.0500)  time: 1.4685  data: 0.0002  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.92 GB / 503.51 GB
Epoch: [5]  [110/893]  eta: 0:20:54  lr: 0.002187  min_lr: 0.000003  loss: 0.4141 (0.4022)  class_acc: 0.8036 (0.8197)  loss_scale: 4096.0000 (5239.9279)  weight_decay: 0.0500 (0.0500)  time: 1.4706  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.65 GB / 503.51 GB
Epoch: [5]  [120/893]  eta: 0:20:29  lr: 0.002187  min_lr: 0.000003  loss: 0.4092 (0.4014)  class_acc: 0.8036 (0.8189)  loss_scale: 4096.0000 (5145.3884)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.41 GB / 503.51 GB
Epoch: [5]  [130/893]  eta: 0:20:06  lr: 0.002187  min_lr: 0.000003  loss: 0.4062 (0.4025)  class_acc: 0.8036 (0.8167)  loss_scale: 4096.0000 (5065.2824)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0002  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.94 GB / 503.51 GB
Epoch: [5]  [140/893]  eta: 0:19:44  lr: 0.002187  min_lr: 0.000003  loss: 0.4043 (0.4030)  class_acc: 0.8214 (0.8174)  loss_scale: 4096.0000 (4996.5390)  weight_decay: 0.0500 (0.0500)  time: 1.4669  data: 0.0003  max mem: 31081
[2025-03-10 22:33:32,456] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:33:32,456] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.24 GB / 503.51 GB
Epoch: [5]  [150/893]  eta: 0:19:23  lr: 0.002187  min_lr: 0.000003  loss: 0.4189 (0.4040)  class_acc: 0.8214 (0.8175)  loss_scale: 4096.0000 (4964.0265)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.58 GB / 503.51 GB
Epoch: [5]  [160/893]  eta: 0:19:02  lr: 0.002187  min_lr: 0.000003  loss: 0.4119 (0.4028)  class_acc: 0.8214 (0.8179)  loss_scale: 8192.0000 (5164.5217)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.71 GB / 503.51 GB
Epoch: [5]  [170/893]  eta: 0:18:43  lr: 0.002187  min_lr: 0.000003  loss: 0.3857 (0.4024)  class_acc: 0.8214 (0.8188)  loss_scale: 8192.0000 (5341.5673)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.87 GB / 503.51 GB
Epoch: [5]  [180/893]  eta: 0:18:24  lr: 0.002187  min_lr: 0.000003  loss: 0.4009 (0.4026)  class_acc: 0.8036 (0.8178)  loss_scale: 8192.0000 (5499.0497)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.34 GB / 503.51 GB
Epoch: [5]  [190/893]  eta: 0:18:05  lr: 0.002187  min_lr: 0.000003  loss: 0.4080 (0.4054)  class_acc: 0.8036 (0.8162)  loss_scale: 8192.0000 (5640.0419)  weight_decay: 0.0500 (0.0500)  time: 1.4603  data: 0.0002  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.70 GB / 503.51 GB
Epoch: [5]  [200/893]  eta: 0:17:47  lr: 0.002187  min_lr: 0.000003  loss: 0.4260 (0.4061)  class_acc: 0.8036 (0.8156)  loss_scale: 8192.0000 (5767.0050)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0002  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.30 GB / 503.51 GB
Epoch: [5]  [210/893]  eta: 0:17:29  lr: 0.002187  min_lr: 0.000003  loss: 0.4404 (0.4079)  class_acc: 0.8036 (0.8144)  loss_scale: 8192.0000 (5881.9336)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.02 GB / 503.51 GB
Epoch: [5]  [220/893]  eta: 0:17:11  lr: 0.002187  min_lr: 0.000003  loss: 0.4241 (0.4083)  class_acc: 0.8036 (0.8141)  loss_scale: 8192.0000 (5986.4615)  weight_decay: 0.0500 (0.0500)  time: 1.4539  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.39 GB / 503.51 GB
Epoch: [5]  [230/893]  eta: 0:16:54  lr: 0.002187  min_lr: 0.000003  loss: 0.4033 (0.4084)  class_acc: 0.8036 (0.8139)  loss_scale: 8192.0000 (6081.9394)  weight_decay: 0.0500 (0.0500)  time: 1.4580  data: 0.0005  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.46 GB / 503.51 GB
Epoch: [5]  [240/893]  eta: 0:16:37  lr: 0.002187  min_lr: 0.000003  loss: 0.4138 (0.4086)  class_acc: 0.8036 (0.8142)  loss_scale: 8192.0000 (6169.4938)  weight_decay: 0.0500 (0.0500)  time: 1.4713  data: 0.0005  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.67 GB / 503.51 GB
Epoch: [5]  [250/893]  eta: 0:16:20  lr: 0.002187  min_lr: 0.000003  loss: 0.3694 (0.4084)  class_acc: 0.8214 (0.8145)  loss_scale: 8192.0000 (6250.0717)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.02 GB / 503.51 GB
Epoch: [5]  [260/893]  eta: 0:16:03  lr: 0.002187  min_lr: 0.000003  loss: 0.3694 (0.4080)  class_acc: 0.8036 (0.8142)  loss_scale: 8192.0000 (6324.4751)  weight_decay: 0.0500 (0.0500)  time: 1.4568  data: 0.0004  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.16 GB / 503.51 GB
Epoch: [5]  [270/893]  eta: 0:15:47  lr: 0.002187  min_lr: 0.000003  loss: 0.3674 (0.4061)  class_acc: 0.8214 (0.8150)  loss_scale: 8192.0000 (6393.3875)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0003  max mem: 31081
[2025-03-10 22:36:39,679] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:36:39,679] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.55 GB / 503.51 GB
Epoch: [5]  [280/893]  eta: 0:15:30  lr: 0.002187  min_lr: 0.000003  loss: 0.3682 (0.4060)  class_acc: 0.8214 (0.8155)  loss_scale: 8192.0000 (6544.8541)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0002  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.20 GB / 503.51 GB
Epoch: [5]  [290/893]  eta: 0:15:14  lr: 0.002187  min_lr: 0.000003  loss: 0.4329 (0.4078)  class_acc: 0.8036 (0.8143)  loss_scale: 16384.0000 (6882.9691)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0002  max mem: 31081
[2025-03-10 22:37:08,918] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 4758
[2025-03-10 22:37:08,919] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 22:37:08,919] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.77 GB / 503.51 GB
Epoch: [5]  [300/893]  eta: 0:14:58  lr: 0.002187  min_lr: 0.000003  loss: 0.4070 (0.4077)  class_acc: 0.7857 (0.8137)  loss_scale: 16384.0000 (7116.9701)  weight_decay: 0.0500 (0.0500)  time: 1.4617  data: 0.0002  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.79 GB / 503.51 GB
Epoch: [5]  [310/893]  eta: 0:14:42  lr: 0.002187  min_lr: 0.000003  loss: 0.3945 (0.4069)  class_acc: 0.8214 (0.8143)  loss_scale: 8192.0000 (7151.5370)  weight_decay: 0.0500 (0.0500)  time: 1.4644  data: 0.0002  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.42 GB / 503.51 GB
Epoch: [5]  [320/893]  eta: 0:14:26  lr: 0.002187  min_lr: 0.000003  loss: 0.4116 (0.4077)  class_acc: 0.8214 (0.8137)  loss_scale: 8192.0000 (7183.9502)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0002  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.05 GB / 503.51 GB
Epoch: [5]  [330/893]  eta: 0:14:10  lr: 0.002187  min_lr: 0.000003  loss: 0.4763 (0.4091)  class_acc: 0.7857 (0.8130)  loss_scale: 8192.0000 (7214.4048)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.02 GB / 503.51 GB
Epoch: [5]  [340/893]  eta: 0:13:54  lr: 0.002187  min_lr: 0.000003  loss: 0.4395 (0.4093)  class_acc: 0.7857 (0.8123)  loss_scale: 8192.0000 (7243.0733)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0002  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.90 GB / 503.51 GB
Epoch: [5]  [350/893]  eta: 0:13:38  lr: 0.002187  min_lr: 0.000003  loss: 0.3975 (0.4086)  class_acc: 0.7857 (0.8126)  loss_scale: 8192.0000 (7270.1083)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0002  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.54 GB / 503.51 GB
Epoch: [5]  [360/893]  eta: 0:13:22  lr: 0.002187  min_lr: 0.000003  loss: 0.4011 (0.4093)  class_acc: 0.8214 (0.8124)  loss_scale: 8192.0000 (7295.6454)  weight_decay: 0.0500 (0.0500)  time: 1.4585  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.96 GB / 503.51 GB
Epoch: [5]  [370/893]  eta: 0:13:07  lr: 0.002187  min_lr: 0.000003  loss: 0.4360 (0.4104)  class_acc: 0.7857 (0.8120)  loss_scale: 8192.0000 (7319.8059)  weight_decay: 0.0500 (0.0500)  time: 1.4696  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.26 GB / 503.51 GB
Epoch: [5]  [380/893]  eta: 0:12:51  lr: 0.002187  min_lr: 0.000003  loss: 0.4597 (0.4118)  class_acc: 0.7857 (0.8113)  loss_scale: 8192.0000 (7342.6982)  weight_decay: 0.0500 (0.0500)  time: 1.4699  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.24 GB / 503.51 GB
Epoch: [5]  [390/893]  eta: 0:12:36  lr: 0.002187  min_lr: 0.000003  loss: 0.4419 (0.4128)  class_acc: 0.7679 (0.8106)  loss_scale: 8192.0000 (7364.4194)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.33 GB / 503.51 GB
Epoch: [5]  [400/893]  eta: 0:12:20  lr: 0.002187  min_lr: 0.000003  loss: 0.4224 (0.4125)  class_acc: 0.8036 (0.8112)  loss_scale: 8192.0000 (7385.0574)  weight_decay: 0.0500 (0.0500)  time: 1.4716  data: 0.0003  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.24 GB / 503.51 GB
Epoch: [5]  [410/893]  eta: 0:12:05  lr: 0.002187  min_lr: 0.000003  loss: 0.4204 (0.4123)  class_acc: 0.8214 (0.8111)  loss_scale: 8192.0000 (7404.6910)  weight_decay: 0.0500 (0.0500)  time: 1.4739  data: 0.0002  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.14 GB / 503.51 GB
Epoch: [5]  [420/893]  eta: 0:11:49  lr: 0.002187  min_lr: 0.000003  loss: 0.3979 (0.4120)  class_acc: 0.8036 (0.8113)  loss_scale: 8192.0000 (7423.3919)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0003  max mem: 31081
[2025-03-10 22:40:18,302] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:40:18,302] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.16 GB / 503.51 GB
Epoch: [5]  [430/893]  eta: 0:11:34  lr: 0.002187  min_lr: 0.000003  loss: 0.4033 (0.4124)  class_acc: 0.8214 (0.8112)  loss_scale: 8192.0000 (7517.2529)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0004  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.00 GB / 503.51 GB
Epoch: [5]  [440/893]  eta: 0:11:19  lr: 0.002187  min_lr: 0.000003  loss: 0.4033 (0.4120)  class_acc: 0.8214 (0.8116)  loss_scale: 16384.0000 (7718.3129)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0003  max mem: 31081
[2025-03-10 22:40:41,811] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 4903
[2025-03-10 22:40:41,811] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 22:40:41,811] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.29 GB / 503.51 GB
Epoch: [5]  [450/893]  eta: 0:11:03  lr: 0.002187  min_lr: 0.000003  loss: 0.4011 (0.4120)  class_acc: 0.8036 (0.8115)  loss_scale: 16384.0000 (7765.1441)  weight_decay: 0.0500 (0.0500)  time: 1.4677  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.02 GB / 503.51 GB
Epoch: [5]  [460/893]  eta: 0:10:48  lr: 0.002187  min_lr: 0.000003  loss: 0.4126 (0.4123)  class_acc: 0.7857 (0.8112)  loss_scale: 8192.0000 (7774.4035)  weight_decay: 0.0500 (0.0500)  time: 1.4732  data: 0.0004  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.60 GB / 503.51 GB
Epoch: [5]  [470/893]  eta: 0:10:33  lr: 0.002187  min_lr: 0.000003  loss: 0.4026 (0.4116)  class_acc: 0.8214 (0.8118)  loss_scale: 8192.0000 (7783.2696)  weight_decay: 0.0500 (0.0500)  time: 1.4737  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.87 GB / 503.51 GB
Epoch: [5]  [480/893]  eta: 0:10:18  lr: 0.002187  min_lr: 0.000003  loss: 0.3726 (0.4109)  class_acc: 0.8393 (0.8123)  loss_scale: 8192.0000 (7791.7672)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.53 GB / 503.51 GB
Epoch: [5]  [490/893]  eta: 0:10:03  lr: 0.002187  min_lr: 0.000003  loss: 0.4263 (0.4121)  class_acc: 0.8214 (0.8116)  loss_scale: 8192.0000 (7799.9185)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.45 GB / 503.51 GB
Epoch: [5]  [500/893]  eta: 0:09:47  lr: 0.002187  min_lr: 0.000003  loss: 0.4619 (0.4131)  class_acc: 0.7500 (0.8108)  loss_scale: 8192.0000 (7807.7445)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0002  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.49 GB / 503.51 GB
Epoch: [5]  [510/893]  eta: 0:09:32  lr: 0.002187  min_lr: 0.000003  loss: 0.4565 (0.4135)  class_acc: 0.7679 (0.8107)  loss_scale: 8192.0000 (7815.2642)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0002  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.84 GB / 503.51 GB
Epoch: [5]  [520/893]  eta: 0:09:17  lr: 0.002187  min_lr: 0.000003  loss: 0.3923 (0.4131)  class_acc: 0.8214 (0.8107)  loss_scale: 8192.0000 (7822.4952)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.53 GB / 503.51 GB
Epoch: [5]  [530/893]  eta: 0:09:02  lr: 0.002187  min_lr: 0.000003  loss: 0.3770 (0.4128)  class_acc: 0.8393 (0.8112)  loss_scale: 8192.0000 (7829.4539)  weight_decay: 0.0500 (0.0500)  time: 1.4760  data: 0.0003  max mem: 31081
[2025-03-10 22:43:02,778] [INFO] [logging.py:129:log_dist] [Rank 0] step=5000, skipped=26, lr=[2.8557560399349216e-06, 2.8557560399349216e-06, 4.7595933998915365e-06, 4.7595933998915365e-06, 7.932655666485894e-06, 7.932655666485894e-06, 1.322109277747649e-05, 1.322109277747649e-05, 2.2035154629127487e-05, 2.2035154629127487e-05, 3.672525771521248e-05, 3.672525771521248e-05, 6.120876285868746e-05, 6.120876285868746e-05, 0.00010201460476447911, 0.00010201460476447911, 0.00017002434127413185, 0.00017002434127413185, 0.0002833739021235531, 0.0002833739021235531, 0.0004722898368725885, 0.0004722898368725885, 0.0007871497281209809, 0.0007871497281209809, 0.0013119162135349683, 0.0013119162135349683, 0.0021865270225582805, 0.0021865270225582805], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-10 22:43:02,779] [INFO] [timer.py:264:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=61.113620832294124, CurrSamplesPerSec=61.635714768086146, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.04 GB / 503.51 GB
Epoch: [5]  [540/893]  eta: 0:08:47  lr: 0.002187  min_lr: 0.000003  loss: 0.4036 (0.4135)  class_acc: 0.8393 (0.8110)  loss_scale: 8192.0000 (7836.1553)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.93 GB / 503.51 GB
Epoch: [5]  [550/893]  eta: 0:08:32  lr: 0.002186  min_lr: 0.000003  loss: 0.4036 (0.4135)  class_acc: 0.8214 (0.8111)  loss_scale: 8192.0000 (7842.6134)  weight_decay: 0.0500 (0.0500)  time: 1.4576  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.86 GB / 503.51 GB
Epoch: [5]  [560/893]  eta: 0:08:17  lr: 0.002186  min_lr: 0.000003  loss: 0.3931 (0.4134)  class_acc: 0.8036 (0.8112)  loss_scale: 8192.0000 (7848.8414)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.04 GB / 503.51 GB
Epoch: [5]  [570/893]  eta: 0:08:02  lr: 0.002186  min_lr: 0.000003  loss: 0.3999 (0.4133)  class_acc: 0.8036 (0.8111)  loss_scale: 8192.0000 (7854.8511)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0003  max mem: 31081
[2025-03-10 22:43:51,201] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:43:51,201] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-03-10 22:44:01,393] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 5039
[2025-03-10 22:44:01,394] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 22:44:01,394] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.26 GB / 503.51 GB
Epoch: [5]  [580/893]  eta: 0:07:46  lr: 0.002186  min_lr: 0.000003  loss: 0.3831 (0.4131)  class_acc: 0.8036 (0.8113)  loss_scale: 8192.0000 (7959.3528)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.39 GB / 503.51 GB
Epoch: [5]  [590/893]  eta: 0:07:31  lr: 0.002186  min_lr: 0.000003  loss: 0.3879 (0.4136)  class_acc: 0.8036 (0.8109)  loss_scale: 8192.0000 (7963.2893)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.44 GB / 503.51 GB
Epoch: [5]  [600/893]  eta: 0:07:16  lr: 0.002186  min_lr: 0.000003  loss: 0.4004 (0.4134)  class_acc: 0.8036 (0.8109)  loss_scale: 8192.0000 (7967.0948)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.19 GB / 503.51 GB
Epoch: [5]  [610/893]  eta: 0:07:01  lr: 0.002186  min_lr: 0.000003  loss: 0.3896 (0.4127)  class_acc: 0.8214 (0.8115)  loss_scale: 8192.0000 (7970.7758)  weight_decay: 0.0500 (0.0500)  time: 1.4560  data: 0.0004  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.96 GB / 503.51 GB
Epoch: [5]  [620/893]  eta: 0:06:46  lr: 0.002186  min_lr: 0.000003  loss: 0.3718 (0.4124)  class_acc: 0.8393 (0.8116)  loss_scale: 8192.0000 (7974.3382)  weight_decay: 0.0500 (0.0500)  time: 1.4486  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.18 GB / 503.51 GB
Epoch: [5]  [630/893]  eta: 0:06:31  lr: 0.002186  min_lr: 0.000003  loss: 0.3718 (0.4122)  class_acc: 0.8214 (0.8116)  loss_scale: 8192.0000 (7977.7876)  weight_decay: 0.0500 (0.0500)  time: 1.4582  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.22 GB / 503.51 GB
Epoch: [5]  [640/893]  eta: 0:06:16  lr: 0.002186  min_lr: 0.000003  loss: 0.3967 (0.4121)  class_acc: 0.8036 (0.8116)  loss_scale: 8192.0000 (7981.1295)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.07 GB / 503.51 GB
Epoch: [5]  [650/893]  eta: 0:06:01  lr: 0.002186  min_lr: 0.000003  loss: 0.3809 (0.4118)  class_acc: 0.8214 (0.8119)  loss_scale: 8192.0000 (7984.3687)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.08 GB / 503.51 GB
Epoch: [5]  [660/893]  eta: 0:05:46  lr: 0.002186  min_lr: 0.000003  loss: 0.3833 (0.4116)  class_acc: 0.8214 (0.8121)  loss_scale: 8192.0000 (7987.5098)  weight_decay: 0.0500 (0.0500)  time: 1.4581  data: 0.0004  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.89 GB / 503.51 GB
Epoch: [5]  [670/893]  eta: 0:05:31  lr: 0.002186  min_lr: 0.000003  loss: 0.3945 (0.4117)  class_acc: 0.8036 (0.8119)  loss_scale: 8192.0000 (7990.5574)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0004  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.42 GB / 503.51 GB
Epoch: [5]  [680/893]  eta: 0:05:16  lr: 0.002186  min_lr: 0.000003  loss: 0.3945 (0.4117)  class_acc: 0.8036 (0.8121)  loss_scale: 8192.0000 (7993.5154)  weight_decay: 0.0500 (0.0500)  time: 1.4715  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.52 GB / 503.51 GB
Epoch: [5]  [690/893]  eta: 0:05:01  lr: 0.002186  min_lr: 0.000003  loss: 0.4124 (0.4121)  class_acc: 0.8036 (0.8118)  loss_scale: 8192.0000 (7996.3878)  weight_decay: 0.0500 (0.0500)  time: 1.4703  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.89 GB / 503.51 GB
Epoch: [5]  [700/893]  eta: 0:04:46  lr: 0.002186  min_lr: 0.000003  loss: 0.4121 (0.4120)  class_acc: 0.8036 (0.8118)  loss_scale: 8192.0000 (7999.1783)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0003  max mem: 31081
[2025-03-10 22:47:10,031] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:47:10,031] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.75 GB / 503.51 GB
Epoch: [5]  [710/893]  eta: 0:04:32  lr: 0.002186  min_lr: 0.000003  loss: 0.4121 (0.4120)  class_acc: 0.8036 (0.8118)  loss_scale: 8192.0000 (8036.4557)  weight_decay: 0.0500 (0.0500)  time: 1.4558  data: 0.0004  max mem: 31081
[2025-03-10 22:47:21,819] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 5176
[2025-03-10 22:47:21,819] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 22:47:21,819] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.70 GB / 503.51 GB
Epoch: [5]  [720/893]  eta: 0:04:17  lr: 0.002186  min_lr: 0.000003  loss: 0.4163 (0.4122)  class_acc: 0.8036 (0.8117)  loss_scale: 8192.0000 (8095.4230)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0003  max mem: 31081
[2025-03-10 22:47:40,905] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 5189
[2025-03-10 22:47:40,905] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-10 22:47:40,906] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.12 GB / 503.51 GB
Epoch: [5]  [730/893]  eta: 0:04:02  lr: 0.002186  min_lr: 0.000003  loss: 0.4150 (0.4121)  class_acc: 0.8036 (0.8117)  loss_scale: 8192.0000 (8085.5376)  weight_decay: 0.0500 (0.0500)  time: 1.4710  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.65 GB / 503.51 GB
Epoch: [5]  [740/893]  eta: 0:03:47  lr: 0.002186  min_lr: 0.000003  loss: 0.4001 (0.4120)  class_acc: 0.8036 (0.8118)  loss_scale: 4096.0000 (8031.6977)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.41 GB / 503.51 GB
Epoch: [5]  [750/893]  eta: 0:03:32  lr: 0.002186  min_lr: 0.000003  loss: 0.3965 (0.4120)  class_acc: 0.8214 (0.8119)  loss_scale: 4096.0000 (7979.2916)  weight_decay: 0.0500 (0.0500)  time: 1.4721  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.55 GB / 503.51 GB
Epoch: [5]  [760/893]  eta: 0:03:17  lr: 0.002186  min_lr: 0.000003  loss: 0.3770 (0.4115)  class_acc: 0.8214 (0.8122)  loss_scale: 4096.0000 (7928.2628)  weight_decay: 0.0500 (0.0500)  time: 1.4725  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.90 GB / 503.51 GB
Epoch: [5]  [770/893]  eta: 0:03:02  lr: 0.002186  min_lr: 0.000003  loss: 0.3740 (0.4115)  class_acc: 0.8036 (0.8122)  loss_scale: 4096.0000 (7878.5577)  weight_decay: 0.0500 (0.0500)  time: 1.4677  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.70 GB / 503.51 GB
Epoch: [5]  [780/893]  eta: 0:02:47  lr: 0.002185  min_lr: 0.000003  loss: 0.4363 (0.4119)  class_acc: 0.8036 (0.8119)  loss_scale: 4096.0000 (7830.1255)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.38 GB / 503.51 GB
Epoch: [5]  [790/893]  eta: 0:02:32  lr: 0.002185  min_lr: 0.000003  loss: 0.4055 (0.4117)  class_acc: 0.8036 (0.8120)  loss_scale: 4096.0000 (7782.9178)  weight_decay: 0.0500 (0.0500)  time: 1.4737  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.29 GB / 503.51 GB
Epoch: [5]  [800/893]  eta: 0:02:18  lr: 0.002185  min_lr: 0.000003  loss: 0.3796 (0.4117)  class_acc: 0.8214 (0.8121)  loss_scale: 4096.0000 (7736.8889)  weight_decay: 0.0500 (0.0500)  time: 1.4762  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.08 GB / 503.51 GB
Epoch: [5]  [810/893]  eta: 0:02:03  lr: 0.002185  min_lr: 0.000003  loss: 0.4055 (0.4120)  class_acc: 0.7857 (0.8120)  loss_scale: 4096.0000 (7691.9951)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.54 GB / 503.51 GB
Epoch: [5]  [820/893]  eta: 0:01:48  lr: 0.002185  min_lr: 0.000003  loss: 0.3862 (0.4118)  class_acc: 0.8036 (0.8122)  loss_scale: 4096.0000 (7648.1949)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0002  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.48 GB / 503.51 GB
Epoch: [5]  [830/893]  eta: 0:01:33  lr: 0.002185  min_lr: 0.000003  loss: 0.3953 (0.4120)  class_acc: 0.8036 (0.8120)  loss_scale: 4096.0000 (7605.4489)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.58 GB / 503.51 GB
Epoch: [5]  [840/893]  eta: 0:01:18  lr: 0.002185  min_lr: 0.000003  loss: 0.4104 (0.4119)  class_acc: 0.8036 (0.8122)  loss_scale: 4096.0000 (7563.7194)  weight_decay: 0.0500 (0.0500)  time: 1.4701  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.87 GB / 503.51 GB
Epoch: [5]  [850/893]  eta: 0:01:03  lr: 0.002185  min_lr: 0.000003  loss: 0.4014 (0.4119)  class_acc: 0.8214 (0.8121)  loss_scale: 4096.0000 (7522.9706)  weight_decay: 0.0500 (0.0500)  time: 1.4615  data: 0.0003  max mem: 31081
[2025-03-10 22:50:50,035] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 22:50:50,035] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.15 GB / 503.51 GB
Epoch: [5]  [860/893]  eta: 0:00:48  lr: 0.002185  min_lr: 0.000003  loss: 0.4080 (0.4119)  class_acc: 0.8036 (0.8119)  loss_scale: 4096.0000 (7497.4402)  weight_decay: 0.0500 (0.0500)  time: 1.4447  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.58 GB / 503.51 GB
Epoch: [5]  [870/893]  eta: 0:00:34  lr: 0.002185  min_lr: 0.000003  loss: 0.4080 (0.4117)  class_acc: 0.8214 (0.8121)  loss_scale: 8192.0000 (7505.4145)  weight_decay: 0.0500 (0.0500)  time: 1.4441  data: 0.0002  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.60 GB / 503.51 GB
Epoch: [5]  [880/893]  eta: 0:00:19  lr: 0.002185  min_lr: 0.000003  loss: 0.4136 (0.4119)  class_acc: 0.8214 (0.8120)  loss_scale: 8192.0000 (7513.2077)  weight_decay: 0.0500 (0.0500)  time: 1.4466  data: 0.0002  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.64 GB / 503.51 GB
Epoch: [5]  [890/893]  eta: 0:00:04  lr: 0.002185  min_lr: 0.000003  loss: 0.4312 (0.4125)  class_acc: 0.7857 (0.8116)  loss_scale: 8192.0000 (7520.8260)  weight_decay: 0.0500 (0.0500)  time: 1.4429  data: 0.0002  max mem: 31081
Epoch: [5]  [892/893]  eta: 0:00:01  lr: 0.002185  min_lr: 0.000003  loss: 0.4331 (0.4127)  class_acc: 0.7857 (0.8114)  loss_scale: 8192.0000 (7521.5785)  weight_decay: 0.0500 (0.0500)  time: 1.3895  data: 0.0002  max mem: 31081
Epoch: [5] Total time: 0:22:02 (1.4807 s / it)
Averaged stats: lr: 0.002185  min_lr: 0.000003  loss: 0.4331 (0.4127)  class_acc: 0.7857 (0.8114)  loss_scale: 8192.0000 (7521.5785)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:15:39  loss: 0.7034 (0.7034)  acc: 67.8571 (67.8571)  time: 11.1800  data: 10.6549  max mem: 31081
Val:  [ 10/728]  eta: 0:18:27  loss: 0.4537 (0.4564)  acc: 83.3333 (81.6017)  time: 1.5419  data: 1.0201  max mem: 31081
Val:  [ 20/728]  eta: 0:14:11  loss: 0.4122 (0.4429)  acc: 82.1429 (81.0658)  time: 0.7042  data: 0.1815  max mem: 31081
Val:  [ 30/728]  eta: 0:12:40  loss: 0.4122 (0.4535)  acc: 79.7619 (79.1091)  time: 0.8399  data: 0.3155  max mem: 31081
Val:  [ 40/728]  eta: 0:11:52  loss: 0.4464 (0.4742)  acc: 72.6190 (77.7875)  time: 0.8608  data: 0.3385  max mem: 31081
Val:  [ 50/728]  eta: 0:11:17  loss: 0.4279 (0.4560)  acc: 78.5714 (79.1783)  time: 0.8617  data: 0.3402  max mem: 31081
Val:  [ 60/728]  eta: 0:10:20  loss: 0.4360 (0.4622)  acc: 82.1429 (79.1179)  time: 0.7090  data: 0.1868  max mem: 31081
Val:  [ 70/728]  eta: 0:09:50  loss: 0.4275 (0.4579)  acc: 80.9524 (79.2421)  time: 0.6359  data: 0.1147  max mem: 31081
Val:  [ 80/728]  eta: 0:09:38  loss: 0.4496 (0.4634)  acc: 79.7619 (78.5714)  time: 0.7831  data: 0.2609  max mem: 31081
Val:  [ 90/728]  eta: 0:09:24  loss: 0.4496 (0.4688)  acc: 76.1905 (78.0220)  time: 0.8436  data: 0.3206  max mem: 31081
Val:  [100/728]  eta: 0:09:12  loss: 0.4162 (0.4647)  acc: 80.9524 (78.3003)  time: 0.8284  data: 0.3050  max mem: 31081
Val:  [110/728]  eta: 0:09:03  loss: 0.4162 (0.4632)  acc: 82.1429 (78.3784)  time: 0.8499  data: 0.3280  max mem: 31081
Val:  [120/728]  eta: 0:08:48  loss: 0.3673 (0.4593)  acc: 84.5238 (78.6600)  time: 0.8152  data: 0.2934  max mem: 31081
Val:  [130/728]  eta: 0:08:27  loss: 0.3703 (0.4589)  acc: 82.1429 (78.5442)  time: 0.6776  data: 0.1571  max mem: 31081
Val:  [140/728]  eta: 0:08:16  loss: 0.4753 (0.4654)  acc: 75.0000 (77.9044)  time: 0.6882  data: 0.1686  max mem: 31081
Val:  [150/728]  eta: 0:08:03  loss: 0.4753 (0.4645)  acc: 78.5714 (77.9959)  time: 0.7550  data: 0.2299  max mem: 31081
Val:  [160/728]  eta: 0:07:55  loss: 0.4600 (0.4663)  acc: 78.5714 (77.8838)  time: 0.7893  data: 0.2632  max mem: 31081
Val:  [170/728]  eta: 0:07:46  loss: 0.4411 (0.4651)  acc: 77.3810 (77.9309)  time: 0.8374  data: 0.3149  max mem: 31081
Val:  [180/728]  eta: 0:07:33  loss: 0.4065 (0.4642)  acc: 79.7619 (78.0189)  time: 0.7508  data: 0.2290  max mem: 31081
Val:  [190/728]  eta: 0:07:19  loss: 0.4766 (0.4642)  acc: 76.1905 (78.0354)  time: 0.6595  data: 0.1360  max mem: 31081
Val:  [200/728]  eta: 0:07:11  loss: 0.4196 (0.4607)  acc: 77.3810 (78.2634)  time: 0.7300  data: 0.2041  max mem: 31081
Val:  [210/728]  eta: 0:07:05  loss: 0.3986 (0.4619)  acc: 77.3810 (78.2611)  time: 0.8642  data: 0.3407  max mem: 31081
Val:  [220/728]  eta: 0:06:59  loss: 0.3986 (0.4595)  acc: 82.1429 (78.4475)  time: 0.9031  data: 0.3814  max mem: 31081
Val:  [230/728]  eta: 0:06:49  loss: 0.3789 (0.4622)  acc: 82.1429 (78.2777)  time: 0.8316  data: 0.3088  max mem: 31081
Val:  [240/728]  eta: 0:06:38  loss: 0.3874 (0.4590)  acc: 82.1429 (78.4578)  time: 0.7071  data: 0.1863  max mem: 31081
Val:  [250/728]  eta: 0:06:27  loss: 0.4208 (0.4584)  acc: 82.1429 (78.4623)  time: 0.6629  data: 0.1395  max mem: 31081
Val:  [260/728]  eta: 0:06:20  loss: 0.4208 (0.4568)  acc: 78.5714 (78.6216)  time: 0.7681  data: 0.2435  max mem: 31081
Val:  [270/728]  eta: 0:06:12  loss: 0.4073 (0.4582)  acc: 80.9524 (78.5231)  time: 0.8465  data: 0.3225  max mem: 31081
Val:  [280/728]  eta: 0:06:07  loss: 0.4416 (0.4588)  acc: 77.3810 (78.6435)  time: 0.9332  data: 0.4067  max mem: 31081
Val:  [290/728]  eta: 0:05:59  loss: 0.3949 (0.4569)  acc: 83.3333 (78.7678)  time: 0.9149  data: 0.3924  max mem: 31081
Val:  [300/728]  eta: 0:05:46  loss: 0.3962 (0.4566)  acc: 80.9524 (78.8166)  time: 0.6610  data: 0.1404  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.4683 (0.4581)  acc: 78.5714 (78.7054)  time: 0.6782  data: 0.1541  max mem: 31081
Val:  [320/728]  eta: 0:05:30  loss: 0.4704 (0.4569)  acc: 76.1905 (78.7643)  time: 0.8064  data: 0.2821  max mem: 31081
Val:  [330/728]  eta: 0:05:23  loss: 0.4210 (0.4559)  acc: 79.7619 (78.8268)  time: 0.8277  data: 0.3040  max mem: 31081
Val:  [340/728]  eta: 0:05:15  loss: 0.3966 (0.4535)  acc: 79.7619 (78.9973)  time: 0.8873  data: 0.3650  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3755 (0.4533)  acc: 82.1429 (79.0123)  time: 0.8477  data: 0.3248  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.4136 (0.4519)  acc: 79.7619 (79.0562)  time: 0.6586  data: 0.1357  max mem: 31081
Val:  [370/728]  eta: 0:04:49  loss: 0.4512 (0.4537)  acc: 79.7619 (78.9372)  time: 0.6935  data: 0.1719  max mem: 31081
Val:  [380/728]  eta: 0:04:41  loss: 0.4659 (0.4553)  acc: 76.1905 (78.8370)  time: 0.8522  data: 0.3271  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.3864 (0.4526)  acc: 79.7619 (78.9916)  time: 0.8292  data: 0.3064  max mem: 31081
Val:  [400/728]  eta: 0:04:25  loss: 0.3392 (0.4509)  acc: 85.7143 (79.1385)  time: 0.8555  data: 0.3337  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.3675 (0.4495)  acc: 83.3333 (79.2203)  time: 0.8724  data: 0.3477  max mem: 31081
Val:  [420/728]  eta: 0:04:07  loss: 0.4567 (0.4507)  acc: 77.3810 (79.2162)  time: 0.6892  data: 0.1652  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4934 (0.4517)  acc: 76.1905 (79.1239)  time: 0.6874  data: 0.1649  max mem: 31081
Val:  [440/728]  eta: 0:03:51  loss: 0.4730 (0.4513)  acc: 77.3810 (79.1491)  time: 0.8226  data: 0.3020  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4139 (0.4521)  acc: 79.7619 (79.0730)  time: 0.8255  data: 0.3041  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.4689 (0.4536)  acc: 75.0000 (79.0001)  time: 0.8379  data: 0.3163  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.4175 (0.4527)  acc: 79.7619 (79.0769)  time: 0.8012  data: 0.2794  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.4175 (0.4522)  acc: 79.7619 (79.1184)  time: 0.6511  data: 0.1302  max mem: 31081
Val:  [490/728]  eta: 0:03:10  loss: 0.4453 (0.4518)  acc: 79.7619 (79.1485)  time: 0.6870  data: 0.1646  max mem: 31081
Val:  [500/728]  eta: 0:03:02  loss: 0.4453 (0.4512)  acc: 82.1429 (79.1774)  time: 0.8412  data: 0.3149  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3487 (0.4500)  acc: 84.5238 (79.2797)  time: 0.8751  data: 0.3512  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4611 (0.4517)  acc: 78.5714 (79.1929)  time: 0.8632  data: 0.3430  max mem: 31081
Val:  [530/728]  eta: 0:02:38  loss: 0.4813 (0.4512)  acc: 76.1905 (79.2373)  time: 0.7520  data: 0.2279  max mem: 31081
Val:  [540/728]  eta: 0:02:29  loss: 0.4617 (0.4523)  acc: 76.1905 (79.1150)  time: 0.6114  data: 0.0858  max mem: 31081
Val:  [550/728]  eta: 0:02:21  loss: 0.4557 (0.4527)  acc: 76.1905 (79.0532)  time: 0.6659  data: 0.1438  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.4405 (0.4521)  acc: 77.3810 (79.1232)  time: 0.7565  data: 0.2355  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4405 (0.4538)  acc: 80.9524 (79.0093)  time: 0.8039  data: 0.2827  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.5072 (0.4543)  acc: 76.1905 (78.9956)  time: 0.8453  data: 0.3212  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.4914 (0.4544)  acc: 77.3810 (78.9965)  time: 0.7406  data: 0.2156  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4156 (0.4545)  acc: 79.7619 (79.0112)  time: 0.6824  data: 0.1611  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4473 (0.4556)  acc: 79.7619 (78.9066)  time: 0.6545  data: 0.1346  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4473 (0.4564)  acc: 78.5714 (78.9127)  time: 0.7779  data: 0.2571  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4361 (0.4565)  acc: 77.3810 (78.8846)  time: 0.8642  data: 0.3428  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.4415 (0.4559)  acc: 78.5714 (78.9057)  time: 0.7939  data: 0.2717  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4071 (0.4555)  acc: 79.7619 (78.9189)  time: 0.7954  data: 0.2722  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4070 (0.4555)  acc: 80.9524 (78.9424)  time: 0.7751  data: 0.2525  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4542 (0.4560)  acc: 79.7619 (78.9068)  time: 0.6816  data: 0.1598  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4467 (0.4556)  acc: 79.7619 (78.9071)  time: 0.6841  data: 0.1610  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3873 (0.4549)  acc: 82.1429 (78.9573)  time: 0.8318  data: 0.3064  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3989 (0.4549)  acc: 80.9524 (78.9315)  time: 0.8580  data: 0.3303  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4277 (0.4546)  acc: 77.3810 (78.9063)  time: 0.8138  data: 0.2883  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4360 (0.4548)  acc: 75.0000 (78.8752)  time: 0.7792  data: 0.2644  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4500 (0.4557)  acc: 76.1905 (78.8415)  time: 0.7549  data: 0.2644  max mem: 31081
Val: Total time: 0:09:33 (0.7881 s / it)
* Acc@1 78.841 AP 0.8067474365234375 loss 0.456
Accuracy of the network on the 61096 val videos: 78.8%
[2025-03-10 23:01:12,922] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestap is about to be saved!
[2025-03-10 23:01:12,925] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt
[2025-03-10 23:01:12,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt...
[2025-03-10 23:01:13,180] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestap/mp_rank_00_model_states.pt.
[2025-03-10 23:01:13,180] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestap is ready now!
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.22 GB / 503.51 GB
Epoch: [6]  [  0/893]  eta: 3:23:36  lr: 0.002185  min_lr: 0.000003  loss: 0.3525 (0.3525)  class_acc: 0.8393 (0.8393)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 13.6808  data: 12.3729  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.60 GB / 503.51 GB
Epoch: [6]  [ 10/893]  eta: 0:38:20  lr: 0.002185  min_lr: 0.000003  loss: 0.4236 (0.4218)  class_acc: 0.8214 (0.8117)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6050  data: 1.1251  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.73 GB / 503.51 GB
Epoch: [6]  [ 20/893]  eta: 0:30:15  lr: 0.002185  min_lr: 0.000003  loss: 0.3826 (0.3898)  class_acc: 0.8214 (0.8214)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4991  data: 0.0005  max mem: 31081
[2025-03-10 23:02:09,260] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 5380
[2025-03-10 23:02:09,260] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-10 23:02:09,260] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.94 GB / 503.51 GB
Epoch: [6]  [ 30/893]  eta: 0:27:15  lr: 0.002185  min_lr: 0.000003  loss: 0.3523 (0.3899)  class_acc: 0.8393 (0.8272)  loss_scale: 8192.0000 (7795.6129)  weight_decay: 0.0500 (0.0500)  time: 1.5044  data: 0.0006  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.30 GB / 503.51 GB
Epoch: [6]  [ 40/893]  eta: 0:25:32  lr: 0.002185  min_lr: 0.000003  loss: 0.3982 (0.3981)  class_acc: 0.8214 (0.8206)  loss_scale: 4096.0000 (6893.2683)  weight_decay: 0.0500 (0.0500)  time: 1.4992  data: 0.0006  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.26 GB / 503.51 GB
Epoch: [6]  [ 50/893]  eta: 0:24:24  lr: 0.002185  min_lr: 0.000003  loss: 0.3933 (0.3958)  class_acc: 0.8214 (0.8214)  loss_scale: 4096.0000 (6344.7843)  weight_decay: 0.0500 (0.0500)  time: 1.4914  data: 0.0005  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.89 GB / 503.51 GB
Epoch: [6]  [ 60/893]  eta: 0:23:28  lr: 0.002184  min_lr: 0.000003  loss: 0.3743 (0.3948)  class_acc: 0.8214 (0.8206)  loss_scale: 4096.0000 (5976.1311)  weight_decay: 0.0500 (0.0500)  time: 1.4765  data: 0.0004  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.18 GB / 503.51 GB
Epoch: [6]  [ 70/893]  eta: 0:22:44  lr: 0.002184  min_lr: 0.000003  loss: 0.3606 (0.3889)  class_acc: 0.8393 (0.8249)  loss_scale: 4096.0000 (5711.3239)  weight_decay: 0.0500 (0.0500)  time: 1.4580  data: 0.0004  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.32 GB / 503.51 GB
Epoch: [6]  [ 80/893]  eta: 0:22:08  lr: 0.002184  min_lr: 0.000003  loss: 0.3616 (0.3872)  class_acc: 0.8571 (0.8258)  loss_scale: 4096.0000 (5511.9012)  weight_decay: 0.0500 (0.0500)  time: 1.4599  data: 0.0004  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.26 GB / 503.51 GB
Epoch: [6]  [ 90/893]  eta: 0:21:36  lr: 0.002184  min_lr: 0.000003  loss: 0.3779 (0.3884)  class_acc: 0.8214 (0.8250)  loss_scale: 4096.0000 (5356.3077)  weight_decay: 0.0500 (0.0500)  time: 1.4585  data: 0.0004  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.65 GB / 503.51 GB
Epoch: [6]  [100/893]  eta: 0:21:08  lr: 0.002184  min_lr: 0.000003  loss: 0.4387 (0.3957)  class_acc: 0.7857 (0.8207)  loss_scale: 4096.0000 (5231.5248)  weight_decay: 0.0500 (0.0500)  time: 1.4595  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.58 GB / 503.51 GB
Epoch: [6]  [110/893]  eta: 0:20:43  lr: 0.002184  min_lr: 0.000003  loss: 0.3982 (0.3946)  class_acc: 0.8036 (0.8213)  loss_scale: 4096.0000 (5129.2252)  weight_decay: 0.0500 (0.0500)  time: 1.4696  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.53 GB / 503.51 GB
Epoch: [6]  [120/893]  eta: 0:20:19  lr: 0.002184  min_lr: 0.000003  loss: 0.3796 (0.3936)  class_acc: 0.8214 (0.8217)  loss_scale: 4096.0000 (5043.8347)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.83 GB / 503.51 GB
Epoch: [6]  [130/893]  eta: 0:19:56  lr: 0.002184  min_lr: 0.000003  loss: 0.3762 (0.3963)  class_acc: 0.7857 (0.8190)  loss_scale: 4096.0000 (4971.4809)  weight_decay: 0.0500 (0.0500)  time: 1.4566  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.03 GB / 503.51 GB
Epoch: [6]  [140/893]  eta: 0:19:35  lr: 0.002184  min_lr: 0.000003  loss: 0.4216 (0.3983)  class_acc: 0.7857 (0.8180)  loss_scale: 4096.0000 (4909.3901)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.04 GB / 503.51 GB
Epoch: [6]  [150/893]  eta: 0:19:15  lr: 0.002184  min_lr: 0.000003  loss: 0.4155 (0.4001)  class_acc: 0.8036 (0.8175)  loss_scale: 4096.0000 (4855.5232)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0003  max mem: 31081
[2025-03-10 23:05:18,643] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 23:05:18,643] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.40 GB / 503.51 GB
Epoch: [6]  [160/893]  eta: 0:18:55  lr: 0.002184  min_lr: 0.000003  loss: 0.3906 (0.4002)  class_acc: 0.8214 (0.8174)  loss_scale: 4096.0000 (4910.1118)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.82 GB / 503.51 GB
Epoch: [6]  [170/893]  eta: 0:18:36  lr: 0.002184  min_lr: 0.000003  loss: 0.3613 (0.3993)  class_acc: 0.8214 (0.8179)  loss_scale: 8192.0000 (5102.0351)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.12 GB / 503.51 GB
Epoch: [6]  [180/893]  eta: 0:18:17  lr: 0.002184  min_lr: 0.000003  loss: 0.3989 (0.4003)  class_acc: 0.8393 (0.8177)  loss_scale: 8192.0000 (5272.7514)  weight_decay: 0.0500 (0.0500)  time: 1.4542  data: 0.0002  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.05 GB / 503.51 GB
Epoch: [6]  [190/893]  eta: 0:17:58  lr: 0.002184  min_lr: 0.000003  loss: 0.3777 (0.3986)  class_acc: 0.8393 (0.8182)  loss_scale: 8192.0000 (5425.5916)  weight_decay: 0.0500 (0.0500)  time: 1.4530  data: 0.0002  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.73 GB / 503.51 GB
Epoch: [6]  [200/893]  eta: 0:17:41  lr: 0.002184  min_lr: 0.000003  loss: 0.3777 (0.4000)  class_acc: 0.8036 (0.8182)  loss_scale: 8192.0000 (5563.2239)  weight_decay: 0.0500 (0.0500)  time: 1.4611  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.16 GB / 503.51 GB
Epoch: [6]  [210/893]  eta: 0:17:23  lr: 0.002183  min_lr: 0.000003  loss: 0.3794 (0.4002)  class_acc: 0.8036 (0.8180)  loss_scale: 8192.0000 (5687.8104)  weight_decay: 0.0500 (0.0500)  time: 1.4663  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.43 GB / 503.51 GB
Epoch: [6]  [220/893]  eta: 0:17:06  lr: 0.002183  min_lr: 0.000003  loss: 0.3774 (0.4017)  class_acc: 0.8036 (0.8176)  loss_scale: 8192.0000 (5801.1222)  weight_decay: 0.0500 (0.0500)  time: 1.4606  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.09 GB / 503.51 GB
Epoch: [6]  [230/893]  eta: 0:16:48  lr: 0.002183  min_lr: 0.000003  loss: 0.4224 (0.4026)  class_acc: 0.8036 (0.8174)  loss_scale: 8192.0000 (5904.6234)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0002  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.53 GB / 503.51 GB
Epoch: [6]  [240/893]  eta: 0:16:32  lr: 0.002183  min_lr: 0.000003  loss: 0.3894 (0.4018)  class_acc: 0.8393 (0.8184)  loss_scale: 8192.0000 (5999.5353)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0002  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.22 GB / 503.51 GB
Epoch: [6]  [250/893]  eta: 0:16:15  lr: 0.002183  min_lr: 0.000003  loss: 0.3748 (0.4015)  class_acc: 0.8393 (0.8187)  loss_scale: 8192.0000 (6086.8845)  weight_decay: 0.0500 (0.0500)  time: 1.4650  data: 0.0003  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.44 GB / 503.51 GB
Epoch: [6]  [260/893]  eta: 0:15:59  lr: 0.002183  min_lr: 0.000003  loss: 0.4065 (0.4020)  class_acc: 0.8036 (0.8183)  loss_scale: 8192.0000 (6167.5402)  weight_decay: 0.0500 (0.0500)  time: 1.4632  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.80 GB / 503.51 GB
Epoch: [6]  [270/893]  eta: 0:15:42  lr: 0.002183  min_lr: 0.000003  loss: 0.3916 (0.4014)  class_acc: 0.8036 (0.8185)  loss_scale: 8192.0000 (6242.2435)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.66 GB / 503.51 GB
Epoch: [6]  [280/893]  eta: 0:15:26  lr: 0.002183  min_lr: 0.000003  loss: 0.3706 (0.4009)  class_acc: 0.8393 (0.8189)  loss_scale: 8192.0000 (6311.6299)  weight_decay: 0.0500 (0.0500)  time: 1.4644  data: 0.0003  max mem: 31081
[2025-03-10 23:08:25,647] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 23:08:25,647] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.99 GB / 503.51 GB
Epoch: [6]  [290/893]  eta: 0:15:10  lr: 0.002183  min_lr: 0.000003  loss: 0.3706 (0.4005)  class_acc: 0.8393 (0.8193)  loss_scale: 8192.0000 (6545.1546)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
[2025-03-10 23:08:34,413] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 5643
[2025-03-10 23:08:34,413] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 23:08:34,413] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.14 GB / 503.51 GB
Epoch: [6]  [300/893]  eta: 0:14:54  lr: 0.002183  min_lr: 0.000003  loss: 0.3706 (0.4002)  class_acc: 0.8214 (0.8194)  loss_scale: 8192.0000 (6599.8671)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.68 GB / 503.51 GB
Epoch: [6]  [310/893]  eta: 0:14:38  lr: 0.002183  min_lr: 0.000003  loss: 0.3662 (0.3999)  class_acc: 0.8214 (0.8201)  loss_scale: 8192.0000 (6651.0611)  weight_decay: 0.0500 (0.0500)  time: 1.4619  data: 0.0004  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.05 GB / 503.51 GB
Epoch: [6]  [320/893]  eta: 0:14:22  lr: 0.002183  min_lr: 0.000003  loss: 0.3572 (0.3996)  class_acc: 0.8571 (0.8205)  loss_scale: 8192.0000 (6699.0654)  weight_decay: 0.0500 (0.0500)  time: 1.4572  data: 0.0003  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.54 GB / 503.51 GB
Epoch: [6]  [330/893]  eta: 0:14:06  lr: 0.002183  min_lr: 0.000003  loss: 0.3772 (0.3998)  class_acc: 0.8214 (0.8201)  loss_scale: 8192.0000 (6744.1692)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.07 GB / 503.51 GB
Epoch: [6]  [340/893]  eta: 0:13:51  lr: 0.002182  min_lr: 0.000003  loss: 0.3484 (0.3988)  class_acc: 0.8214 (0.8207)  loss_scale: 8192.0000 (6786.6276)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.11 GB / 503.51 GB
Epoch: [6]  [350/893]  eta: 0:13:35  lr: 0.002182  min_lr: 0.000003  loss: 0.3865 (0.3992)  class_acc: 0.8214 (0.8209)  loss_scale: 8192.0000 (6826.6667)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.95 GB / 503.51 GB
Epoch: [6]  [360/893]  eta: 0:13:19  lr: 0.002182  min_lr: 0.000003  loss: 0.4170 (0.4000)  class_acc: 0.8214 (0.8207)  loss_scale: 8192.0000 (6864.4875)  weight_decay: 0.0500 (0.0500)  time: 1.4619  data: 0.0002  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.30 GB / 503.51 GB
Epoch: [6]  [370/893]  eta: 0:13:04  lr: 0.002182  min_lr: 0.000003  loss: 0.3926 (0.3992)  class_acc: 0.8214 (0.8210)  loss_scale: 8192.0000 (6900.2695)  weight_decay: 0.0500 (0.0500)  time: 1.4602  data: 0.0002  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.33 GB / 503.51 GB
Epoch: [6]  [380/893]  eta: 0:12:48  lr: 0.002182  min_lr: 0.000003  loss: 0.3926 (0.4007)  class_acc: 0.8214 (0.8199)  loss_scale: 8192.0000 (6934.1732)  weight_decay: 0.0500 (0.0500)  time: 1.4616  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.64 GB / 503.51 GB
Epoch: [6]  [390/893]  eta: 0:12:33  lr: 0.002182  min_lr: 0.000003  loss: 0.4167 (0.4007)  class_acc: 0.8214 (0.8201)  loss_scale: 8192.0000 (6966.3427)  weight_decay: 0.0500 (0.0500)  time: 1.4591  data: 0.0002  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.64 GB / 503.51 GB
Epoch: [6]  [400/893]  eta: 0:12:17  lr: 0.002182  min_lr: 0.000003  loss: 0.3804 (0.4004)  class_acc: 0.8214 (0.8197)  loss_scale: 8192.0000 (6996.9077)  weight_decay: 0.0500 (0.0500)  time: 1.4627  data: 0.0002  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.13 GB / 503.51 GB
Epoch: [6]  [410/893]  eta: 0:12:02  lr: 0.002182  min_lr: 0.000003  loss: 0.3772 (0.3999)  class_acc: 0.8214 (0.8199)  loss_scale: 8192.0000 (7025.9854)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0002  max mem: 31081
[2025-03-10 23:11:42,968] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 23:11:42,969] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.42 GB / 503.51 GB
Epoch: [6]  [420/893]  eta: 0:11:47  lr: 0.002182  min_lr: 0.000003  loss: 0.3809 (0.4001)  class_acc: 0.8214 (0.8197)  loss_scale: 8192.0000 (7073.1401)  weight_decay: 0.0500 (0.0500)  time: 1.4578  data: 0.0003  max mem: 31081
[2025-03-10 23:11:48,782] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 5776
[2025-03-10 23:11:48,782] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 23:11:48,782] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.40 GB / 503.51 GB
Epoch: [6]  [430/893]  eta: 0:11:31  lr: 0.002182  min_lr: 0.000003  loss: 0.3757 (0.3996)  class_acc: 0.8214 (0.8200)  loss_scale: 8192.0000 (7156.1206)  weight_decay: 0.0500 (0.0500)  time: 1.4580  data: 0.0003  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.55 GB / 503.51 GB
Epoch: [6]  [440/893]  eta: 0:11:16  lr: 0.002182  min_lr: 0.000003  loss: 0.3728 (0.3993)  class_acc: 0.8214 (0.8202)  loss_scale: 8192.0000 (7179.6100)  weight_decay: 0.0500 (0.0500)  time: 1.4490  data: 0.0002  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.48 GB / 503.51 GB
Epoch: [6]  [450/893]  eta: 0:11:01  lr: 0.002181  min_lr: 0.000003  loss: 0.3728 (0.3993)  class_acc: 0.8214 (0.8202)  loss_scale: 8192.0000 (7202.0576)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.58 GB / 503.51 GB
Epoch: [6]  [460/893]  eta: 0:10:45  lr: 0.002181  min_lr: 0.000003  loss: 0.3535 (0.3982)  class_acc: 0.8393 (0.8209)  loss_scale: 8192.0000 (7223.5315)  weight_decay: 0.0500 (0.0500)  time: 1.4721  data: 0.0004  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.53 GB / 503.51 GB
Epoch: [6]  [470/893]  eta: 0:10:30  lr: 0.002181  min_lr: 0.000003  loss: 0.3921 (0.3986)  class_acc: 0.8036 (0.8204)  loss_scale: 8192.0000 (7244.0934)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.72 GB / 503.51 GB
Epoch: [6]  [480/893]  eta: 0:10:15  lr: 0.002181  min_lr: 0.000003  loss: 0.3899 (0.3980)  class_acc: 0.8214 (0.8206)  loss_scale: 8192.0000 (7263.8004)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.43 GB / 503.51 GB
Epoch: [6]  [490/893]  eta: 0:10:00  lr: 0.002181  min_lr: 0.000003  loss: 0.3674 (0.3974)  class_acc: 0.8214 (0.8206)  loss_scale: 8192.0000 (7282.7047)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.84 GB / 503.51 GB
Epoch: [6]  [500/893]  eta: 0:09:45  lr: 0.002181  min_lr: 0.000003  loss: 0.3813 (0.3973)  class_acc: 0.8214 (0.8206)  loss_scale: 8192.0000 (7300.8543)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.07 GB / 503.51 GB
Epoch: [6]  [510/893]  eta: 0:09:30  lr: 0.002181  min_lr: 0.000003  loss: 0.3906 (0.3972)  class_acc: 0.8393 (0.8209)  loss_scale: 8192.0000 (7318.2935)  weight_decay: 0.0500 (0.0500)  time: 1.4658  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.64 GB / 503.51 GB
Epoch: [6]  [520/893]  eta: 0:09:15  lr: 0.002181  min_lr: 0.000003  loss: 0.3716 (0.3967)  class_acc: 0.8393 (0.8211)  loss_scale: 8192.0000 (7335.0633)  weight_decay: 0.0500 (0.0500)  time: 1.4596  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.46 GB / 503.51 GB
Epoch: [6]  [530/893]  eta: 0:09:00  lr: 0.002181  min_lr: 0.000003  loss: 0.3701 (0.3962)  class_acc: 0.8393 (0.8212)  loss_scale: 8192.0000 (7351.2015)  weight_decay: 0.0500 (0.0500)  time: 1.4575  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.82 GB / 503.51 GB
Epoch: [6]  [540/893]  eta: 0:08:45  lr: 0.002181  min_lr: 0.000003  loss: 0.3918 (0.3967)  class_acc: 0.8214 (0.8210)  loss_scale: 8192.0000 (7366.7431)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0002  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.58 GB / 503.51 GB
Epoch: [6]  [550/893]  eta: 0:08:30  lr: 0.002181  min_lr: 0.000003  loss: 0.3918 (0.3969)  class_acc: 0.8214 (0.8210)  loss_scale: 8192.0000 (7381.7205)  weight_decay: 0.0500 (0.0500)  time: 1.4564  data: 0.0002  max mem: 31081
[2025-03-10 23:14:57,296] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 23:14:57,296] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-03-10 23:15:01,683] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 5908
[2025-03-10 23:15:01,683] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 23:15:01,683] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.40 GB / 503.51 GB
Epoch: [6]  [560/893]  eta: 0:08:15  lr: 0.002180  min_lr: 0.000003  loss: 0.3867 (0.3974)  class_acc: 0.8036 (0.8207)  loss_scale: 8192.0000 (7439.9715)  weight_decay: 0.0500 (0.0500)  time: 1.4588  data: 0.0002  max mem: 31081
[2025-03-10 23:15:17,754] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 5919
[2025-03-10 23:15:17,754] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-10 23:15:17,754] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.72 GB / 503.51 GB
Epoch: [6]  [570/893]  eta: 0:08:00  lr: 0.002180  min_lr: 0.000003  loss: 0.4082 (0.3978)  class_acc: 0.7857 (0.8201)  loss_scale: 8192.0000 (7424.4483)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.08 GB / 503.51 GB
Epoch: [6]  [580/893]  eta: 0:07:45  lr: 0.002180  min_lr: 0.000003  loss: 0.4136 (0.3980)  class_acc: 0.8214 (0.8201)  loss_scale: 4096.0000 (7367.1601)  weight_decay: 0.0500 (0.0500)  time: 1.4608  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.81 GB / 503.51 GB
Epoch: [6]  [590/893]  eta: 0:07:30  lr: 0.002180  min_lr: 0.000003  loss: 0.4243 (0.3987)  class_acc: 0.8036 (0.8197)  loss_scale: 4096.0000 (7311.8105)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.62 GB / 503.51 GB
Epoch: [6]  [600/893]  eta: 0:07:15  lr: 0.002180  min_lr: 0.000003  loss: 0.4136 (0.3995)  class_acc: 0.7857 (0.8192)  loss_scale: 4096.0000 (7258.3028)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.16 GB / 503.51 GB
Epoch: [6]  [610/893]  eta: 0:07:00  lr: 0.002180  min_lr: 0.000003  loss: 0.4136 (0.3999)  class_acc: 0.7857 (0.8188)  loss_scale: 4096.0000 (7206.5466)  weight_decay: 0.0500 (0.0500)  time: 1.4706  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.70 GB / 503.51 GB
Epoch: [6]  [620/893]  eta: 0:06:45  lr: 0.002180  min_lr: 0.000003  loss: 0.3984 (0.4000)  class_acc: 0.8036 (0.8188)  loss_scale: 4096.0000 (7156.4573)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0002  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.95 GB / 503.51 GB
Epoch: [6]  [630/893]  eta: 0:06:30  lr: 0.002180  min_lr: 0.000003  loss: 0.3914 (0.4004)  class_acc: 0.8036 (0.8183)  loss_scale: 4096.0000 (7107.9556)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0002  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.62 GB / 503.51 GB
Epoch: [6]  [640/893]  eta: 0:06:15  lr: 0.002180  min_lr: 0.000003  loss: 0.3843 (0.4002)  class_acc: 0.8036 (0.8184)  loss_scale: 4096.0000 (7060.9672)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0002  max mem: 31081
[2025-03-10 23:17:15,044] [INFO] [logging.py:129:log_dist] [Rank 0] step=6000, skipped=34, lr=[2.8466775869431924e-06, 2.8466775869431924e-06, 4.744462644905321e-06, 4.744462644905321e-06, 7.907437741508869e-06, 7.907437741508869e-06, 1.3179062902514781e-05, 1.3179062902514781e-05, 2.1965104837524638e-05, 2.1965104837524638e-05, 3.660850806254106e-05, 3.660850806254106e-05, 6.101418010423511e-05, 6.101418010423511e-05, 0.00010169030017372519, 0.00010169030017372519, 0.0001694838336228753, 0.0001694838336228753, 0.0002824730560381256, 0.0002824730560381256, 0.00047078842673020925, 0.00047078842673020925, 0.0007846473778836821, 0.0007846473778836821, 0.0013077456298061368, 0.0013077456298061368, 0.002179576049676895, 0.002179576049676895], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-10 23:17:15,044] [INFO] [timer.py:264:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=61.108175039031075, CurrSamplesPerSec=61.711972278369224, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.97 GB / 503.51 GB
Epoch: [6]  [650/893]  eta: 0:06:00  lr: 0.002180  min_lr: 0.000003  loss: 0.3303 (0.3996)  class_acc: 0.8571 (0.8188)  loss_scale: 4096.0000 (7015.4224)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.64 GB / 503.51 GB
Epoch: [6]  [660/893]  eta: 0:05:45  lr: 0.002179  min_lr: 0.000003  loss: 0.3691 (0.4001)  class_acc: 0.8214 (0.8186)  loss_scale: 4096.0000 (6971.2557)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.79 GB / 503.51 GB
Epoch: [6]  [670/893]  eta: 0:05:30  lr: 0.002179  min_lr: 0.000003  loss: 0.4077 (0.4000)  class_acc: 0.8036 (0.8186)  loss_scale: 4096.0000 (6928.4054)  weight_decay: 0.0500 (0.0500)  time: 1.4580  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.78 GB / 503.51 GB
Epoch: [6]  [680/893]  eta: 0:05:15  lr: 0.002179  min_lr: 0.000003  loss: 0.3938 (0.3999)  class_acc: 0.8036 (0.8187)  loss_scale: 4096.0000 (6886.8135)  weight_decay: 0.0500 (0.0500)  time: 1.4568  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.00 GB / 503.51 GB
Epoch: [6]  [690/893]  eta: 0:05:00  lr: 0.002179  min_lr: 0.000003  loss: 0.3918 (0.3997)  class_acc: 0.8036 (0.8187)  loss_scale: 4096.0000 (6846.4255)  weight_decay: 0.0500 (0.0500)  time: 1.4584  data: 0.0003  max mem: 31081
[2025-03-10 23:18:26,568] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 23:18:26,568] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.34 GB / 503.51 GB
Epoch: [6]  [700/893]  eta: 0:04:45  lr: 0.002179  min_lr: 0.000003  loss: 0.3784 (0.3992)  class_acc: 0.8214 (0.8189)  loss_scale: 4096.0000 (6836.4051)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0003  max mem: 31081
[2025-03-10 23:18:35,328] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 6054
[2025-03-10 23:18:35,328] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-10 23:18:35,328] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.03 GB / 503.51 GB
Epoch: [6]  [710/893]  eta: 0:04:31  lr: 0.002179  min_lr: 0.000003  loss: 0.3989 (0.3992)  class_acc: 0.8214 (0.8190)  loss_scale: 4096.0000 (6803.6231)  weight_decay: 0.0500 (0.0500)  time: 1.4644  data: 0.0004  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.96 GB / 503.51 GB
Epoch: [6]  [720/893]  eta: 0:04:16  lr: 0.002179  min_lr: 0.000003  loss: 0.3784 (0.3989)  class_acc: 0.8393 (0.8194)  loss_scale: 4096.0000 (6766.0693)  weight_decay: 0.0500 (0.0500)  time: 1.4562  data: 0.0004  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.21 GB / 503.51 GB
Epoch: [6]  [730/893]  eta: 0:04:01  lr: 0.002179  min_lr: 0.000003  loss: 0.3586 (0.3984)  class_acc: 0.8393 (0.8195)  loss_scale: 4096.0000 (6729.5431)  weight_decay: 0.0500 (0.0500)  time: 1.4561  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.71 GB / 503.51 GB
Epoch: [6]  [740/893]  eta: 0:03:46  lr: 0.002179  min_lr: 0.000003  loss: 0.3586 (0.3988)  class_acc: 0.8393 (0.8193)  loss_scale: 4096.0000 (6694.0027)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0002  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.89 GB / 503.51 GB
Epoch: [6]  [750/893]  eta: 0:03:31  lr: 0.002178  min_lr: 0.000003  loss: 0.3621 (0.3984)  class_acc: 0.8214 (0.8195)  loss_scale: 4096.0000 (6659.4088)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.88 GB / 503.51 GB
Epoch: [6]  [760/893]  eta: 0:03:16  lr: 0.002178  min_lr: 0.000003  loss: 0.3542 (0.3981)  class_acc: 0.8393 (0.8196)  loss_scale: 4096.0000 (6625.7240)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.94 GB / 503.51 GB
Epoch: [6]  [770/893]  eta: 0:03:02  lr: 0.002178  min_lr: 0.000003  loss: 0.4417 (0.3990)  class_acc: 0.8036 (0.8191)  loss_scale: 4096.0000 (6592.9131)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.78 GB / 503.51 GB
Epoch: [6]  [780/893]  eta: 0:02:47  lr: 0.002178  min_lr: 0.000003  loss: 0.4656 (0.3995)  class_acc: 0.7857 (0.8190)  loss_scale: 4096.0000 (6560.9424)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.01 GB / 503.51 GB
Epoch: [6]  [790/893]  eta: 0:02:32  lr: 0.002178  min_lr: 0.000003  loss: 0.4165 (0.3992)  class_acc: 0.8214 (0.8193)  loss_scale: 4096.0000 (6529.7800)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.83 GB / 503.51 GB
Epoch: [6]  [800/893]  eta: 0:02:17  lr: 0.002178  min_lr: 0.000003  loss: 0.3574 (0.3991)  class_acc: 0.8393 (0.8196)  loss_scale: 4096.0000 (6499.3958)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.13 GB / 503.51 GB
Epoch: [6]  [810/893]  eta: 0:02:02  lr: 0.002178  min_lr: 0.000003  loss: 0.3999 (0.3992)  class_acc: 0.8214 (0.8196)  loss_scale: 4096.0000 (6469.7608)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0002  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.47 GB / 503.51 GB
Epoch: [6]  [820/893]  eta: 0:01:47  lr: 0.002178  min_lr: 0.000003  loss: 0.3711 (0.3988)  class_acc: 0.8393 (0.8198)  loss_scale: 4096.0000 (6440.8477)  weight_decay: 0.0500 (0.0500)  time: 1.4584  data: 0.0002  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.85 GB / 503.51 GB
Epoch: [6]  [830/893]  eta: 0:01:33  lr: 0.002178  min_lr: 0.000003  loss: 0.3796 (0.3991)  class_acc: 0.8393 (0.8198)  loss_scale: 4096.0000 (6412.6306)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
[2025-03-10 23:21:44,240] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 23:21:44,240] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.57 GB / 503.51 GB
Epoch: [6]  [840/893]  eta: 0:01:18  lr: 0.002177  min_lr: 0.000003  loss: 0.4231 (0.3996)  class_acc: 0.8036 (0.8195)  loss_scale: 4096.0000 (6433.7883)  weight_decay: 0.0500 (0.0500)  time: 1.4673  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.46 GB / 503.51 GB
Epoch: [6]  [850/893]  eta: 0:01:03  lr: 0.002177  min_lr: 0.000003  loss: 0.4053 (0.3998)  class_acc: 0.8036 (0.8192)  loss_scale: 8192.0000 (6454.4489)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.30 GB / 503.51 GB
Epoch: [6]  [860/893]  eta: 0:00:48  lr: 0.002177  min_lr: 0.000003  loss: 0.3767 (0.3995)  class_acc: 0.8214 (0.8194)  loss_scale: 8192.0000 (6474.6295)  weight_decay: 0.0500 (0.0500)  time: 1.4528  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.16 GB / 503.51 GB
Epoch: [6]  [870/893]  eta: 0:00:33  lr: 0.002177  min_lr: 0.000003  loss: 0.3643 (0.4000)  class_acc: 0.8214 (0.8190)  loss_scale: 8192.0000 (6494.3467)  weight_decay: 0.0500 (0.0500)  time: 1.4415  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.43 GB / 503.51 GB
Epoch: [6]  [880/893]  eta: 0:00:19  lr: 0.002177  min_lr: 0.000003  loss: 0.4087 (0.4001)  class_acc: 0.8214 (0.8190)  loss_scale: 8192.0000 (6513.6163)  weight_decay: 0.0500 (0.0500)  time: 1.4379  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.13 GB / 503.51 GB
Epoch: [6]  [890/893]  eta: 0:00:04  lr: 0.002177  min_lr: 0.000003  loss: 0.3667 (0.3998)  class_acc: 0.8214 (0.8192)  loss_scale: 8192.0000 (6532.4534)  weight_decay: 0.0500 (0.0500)  time: 1.4335  data: 0.0001  max mem: 31081
Epoch: [6]  [892/893]  eta: 0:00:01  lr: 0.002177  min_lr: 0.000003  loss: 0.3638 (0.3997)  class_acc: 0.8393 (0.8193)  loss_scale: 8192.0000 (6534.3139)  weight_decay: 0.0500 (0.0500)  time: 1.3829  data: 0.0001  max mem: 31081
Epoch: [6] Total time: 0:21:57 (1.4759 s / it)
Averaged stats: lr: 0.002177  min_lr: 0.000003  loss: 0.3638 (0.3997)  class_acc: 0.8393 (0.8193)  loss_scale: 8192.0000 (6534.3139)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:16:01  loss: 0.7069 (0.7069)  acc: 73.8095 (73.8095)  time: 11.2108  data: 10.6851  max mem: 31081
Val:  [ 10/728]  eta: 0:18:25  loss: 0.4475 (0.4538)  acc: 79.7619 (81.6017)  time: 1.5402  data: 1.0215  max mem: 31081
Val:  [ 20/728]  eta: 0:14:13  loss: 0.4454 (0.4508)  acc: 79.7619 (80.1587)  time: 0.7053  data: 0.1850  max mem: 31081
Val:  [ 30/728]  eta: 0:12:47  loss: 0.3600 (0.4484)  acc: 79.7619 (79.2627)  time: 0.8571  data: 0.3328  max mem: 31081
Val:  [ 40/728]  eta: 0:11:54  loss: 0.3600 (0.4572)  acc: 78.5714 (78.2230)  time: 0.8638  data: 0.3407  max mem: 31081
Val:  [ 50/728]  eta: 0:11:17  loss: 0.3709 (0.4358)  acc: 82.1429 (79.4351)  time: 0.8445  data: 0.3221  max mem: 31081
Val:  [ 60/728]  eta: 0:10:17  loss: 0.4381 (0.4472)  acc: 78.5714 (79.0984)  time: 0.6901  data: 0.1673  max mem: 31081
Val:  [ 70/728]  eta: 0:09:52  loss: 0.4381 (0.4391)  acc: 78.5714 (79.2589)  time: 0.6505  data: 0.1278  max mem: 31081
Val:  [ 80/728]  eta: 0:09:39  loss: 0.5068 (0.4556)  acc: 77.3810 (78.7772)  time: 0.8049  data: 0.2815  max mem: 31081
Val:  [ 90/728]  eta: 0:09:23  loss: 0.5019 (0.4721)  acc: 78.5714 (78.3360)  time: 0.8169  data: 0.2951  max mem: 31081
Val:  [100/728]  eta: 0:09:12  loss: 0.4214 (0.4664)  acc: 80.9524 (78.6422)  time: 0.8161  data: 0.2957  max mem: 31081
Val:  [110/728]  eta: 0:09:03  loss: 0.3787 (0.4667)  acc: 80.9524 (78.6143)  time: 0.8642  data: 0.3416  max mem: 31081
Val:  [120/728]  eta: 0:08:46  loss: 0.3607 (0.4596)  acc: 83.3333 (79.0240)  time: 0.7947  data: 0.2719  max mem: 31081
Val:  [130/728]  eta: 0:08:27  loss: 0.3754 (0.4694)  acc: 82.1429 (78.7532)  time: 0.6757  data: 0.1570  max mem: 31081
Val:  [140/728]  eta: 0:08:18  loss: 0.4698 (0.4709)  acc: 75.0000 (78.4363)  time: 0.7408  data: 0.2222  max mem: 31081
Val:  [150/728]  eta: 0:08:02  loss: 0.4796 (0.4757)  acc: 76.1905 (78.1615)  time: 0.7434  data: 0.2205  max mem: 31081
Val:  [160/728]  eta: 0:07:55  loss: 0.4172 (0.4811)  acc: 80.9524 (78.1574)  time: 0.7632  data: 0.2387  max mem: 31081
Val:  [170/728]  eta: 0:07:48  loss: 0.4208 (0.4797)  acc: 80.9524 (78.1885)  time: 0.8742  data: 0.3513  max mem: 31081
Val:  [180/728]  eta: 0:07:34  loss: 0.4321 (0.4830)  acc: 80.9524 (78.1176)  time: 0.7675  data: 0.2471  max mem: 31081
Val:  [190/728]  eta: 0:07:22  loss: 0.4321 (0.4827)  acc: 76.1905 (78.0416)  time: 0.6865  data: 0.1660  max mem: 31081
Val:  [200/728]  eta: 0:07:14  loss: 0.4292 (0.4808)  acc: 73.8095 (78.0384)  time: 0.7554  data: 0.2339  max mem: 31081
Val:  [210/728]  eta: 0:07:08  loss: 0.4337 (0.4848)  acc: 73.8095 (78.0241)  time: 0.8608  data: 0.3396  max mem: 31081
Val:  [220/728]  eta: 0:07:01  loss: 0.3879 (0.4831)  acc: 80.9524 (78.1997)  time: 0.9105  data: 0.3867  max mem: 31081
Val:  [230/728]  eta: 0:06:52  loss: 0.4274 (0.4850)  acc: 79.7619 (78.1334)  time: 0.8427  data: 0.3177  max mem: 31081
Val:  [240/728]  eta: 0:06:39  loss: 0.4466 (0.4830)  acc: 82.1429 (78.3541)  time: 0.6863  data: 0.1630  max mem: 31081
Val:  [250/728]  eta: 0:06:29  loss: 0.3871 (0.4825)  acc: 82.1429 (78.3675)  time: 0.6666  data: 0.1443  max mem: 31081
Val:  [260/728]  eta: 0:06:22  loss: 0.3413 (0.4813)  acc: 84.5238 (78.5213)  time: 0.8131  data: 0.2887  max mem: 31081
Val:  [270/728]  eta: 0:06:14  loss: 0.4084 (0.4818)  acc: 83.3333 (78.5319)  time: 0.8562  data: 0.3341  max mem: 31081
Val:  [280/728]  eta: 0:06:10  loss: 0.4660 (0.4802)  acc: 77.3810 (78.5630)  time: 0.9321  data: 0.4120  max mem: 31081
Val:  [290/728]  eta: 0:06:01  loss: 0.4070 (0.4780)  acc: 80.9524 (78.6451)  time: 0.9289  data: 0.4052  max mem: 31081
Val:  [300/728]  eta: 0:05:49  loss: 0.3915 (0.4774)  acc: 80.9524 (78.7217)  time: 0.6725  data: 0.1485  max mem: 31081
Val:  [310/728]  eta: 0:05:41  loss: 0.4607 (0.4786)  acc: 77.3810 (78.6824)  time: 0.6786  data: 0.1570  max mem: 31081
Val:  [320/728]  eta: 0:05:32  loss: 0.4267 (0.4762)  acc: 77.3810 (78.7420)  time: 0.8149  data: 0.2940  max mem: 31081
Val:  [330/728]  eta: 0:05:25  loss: 0.4096 (0.4734)  acc: 82.1429 (78.8771)  time: 0.8451  data: 0.3248  max mem: 31081
Val:  [340/728]  eta: 0:05:18  loss: 0.3241 (0.4699)  acc: 84.5238 (79.0532)  time: 0.8959  data: 0.3769  max mem: 31081
Val:  [350/728]  eta: 0:05:09  loss: 0.3164 (0.4689)  acc: 84.5238 (79.1548)  time: 0.8436  data: 0.3261  max mem: 31081
Val:  [360/728]  eta: 0:04:58  loss: 0.3447 (0.4665)  acc: 83.3333 (79.2475)  time: 0.6566  data: 0.1371  max mem: 31081
Val:  [370/728]  eta: 0:04:51  loss: 0.4057 (0.4685)  acc: 80.9524 (79.1843)  time: 0.6968  data: 0.1714  max mem: 31081
Val:  [380/728]  eta: 0:04:43  loss: 0.4495 (0.4715)  acc: 76.1905 (79.0714)  time: 0.8595  data: 0.3346  max mem: 31081
Val:  [390/728]  eta: 0:04:35  loss: 0.4064 (0.4676)  acc: 78.5714 (79.2047)  time: 0.8444  data: 0.3210  max mem: 31081
Val:  [400/728]  eta: 0:04:27  loss: 0.3770 (0.4673)  acc: 83.3333 (79.3017)  time: 0.8733  data: 0.3488  max mem: 31081
Val:  [410/728]  eta: 0:04:20  loss: 0.3519 (0.4661)  acc: 82.1429 (79.3100)  time: 0.8849  data: 0.3623  max mem: 31081
Val:  [420/728]  eta: 0:04:09  loss: 0.3519 (0.4666)  acc: 82.1429 (79.3236)  time: 0.6924  data: 0.1714  max mem: 31081
Val:  [430/728]  eta: 0:04:02  loss: 0.4795 (0.4671)  acc: 75.0000 (79.2233)  time: 0.6953  data: 0.1743  max mem: 31081
Val:  [440/728]  eta: 0:03:53  loss: 0.4584 (0.4659)  acc: 78.5714 (79.3030)  time: 0.8333  data: 0.3118  max mem: 31081
Val:  [450/728]  eta: 0:03:46  loss: 0.3702 (0.4653)  acc: 85.7143 (79.3396)  time: 0.8342  data: 0.3122  max mem: 31081
Val:  [460/728]  eta: 0:03:37  loss: 0.4132 (0.4671)  acc: 79.7619 (79.2583)  time: 0.8406  data: 0.3199  max mem: 31081
Val:  [470/728]  eta: 0:03:29  loss: 0.3346 (0.4660)  acc: 82.1429 (79.3171)  time: 0.7983  data: 0.2790  max mem: 31081
Val:  [480/728]  eta: 0:03:20  loss: 0.3346 (0.4653)  acc: 83.3333 (79.3412)  time: 0.6555  data: 0.1337  max mem: 31081
Val:  [490/728]  eta: 0:03:12  loss: 0.4795 (0.4658)  acc: 76.1905 (79.2697)  time: 0.6999  data: 0.1753  max mem: 31081
Val:  [500/728]  eta: 0:03:04  loss: 0.4459 (0.4654)  acc: 76.1905 (79.3294)  time: 0.8511  data: 0.3282  max mem: 31081
Val:  [510/728]  eta: 0:02:56  loss: 0.3341 (0.4630)  acc: 84.5238 (79.4311)  time: 0.8767  data: 0.3570  max mem: 31081
Val:  [520/728]  eta: 0:02:48  loss: 0.4113 (0.4651)  acc: 79.7619 (79.3255)  time: 0.8683  data: 0.3486  max mem: 31081
Val:  [530/728]  eta: 0:02:40  loss: 0.5046 (0.4644)  acc: 77.3810 (79.3718)  time: 0.7542  data: 0.2303  max mem: 31081
Val:  [540/728]  eta: 0:02:31  loss: 0.4641 (0.4662)  acc: 77.3810 (79.2294)  time: 0.6107  data: 0.0858  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.4775 (0.4666)  acc: 76.1905 (79.2045)  time: 0.6631  data: 0.1404  max mem: 31081
Val:  [560/728]  eta: 0:02:14  loss: 0.4333 (0.4667)  acc: 78.5714 (79.2250)  time: 0.7562  data: 0.2342  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4328 (0.4709)  acc: 79.7619 (79.1072)  time: 0.8068  data: 0.2823  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4671 (0.4715)  acc: 79.7619 (79.1124)  time: 0.8408  data: 0.3157  max mem: 31081
Val:  [590/728]  eta: 0:01:50  loss: 0.4706 (0.4720)  acc: 80.9524 (79.1052)  time: 0.7469  data: 0.2256  max mem: 31081
Val:  [600/728]  eta: 0:01:42  loss: 0.4692 (0.4717)  acc: 82.1429 (79.1479)  time: 0.6396  data: 0.1185  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4572 (0.4736)  acc: 78.5714 (79.0468)  time: 0.6518  data: 0.1294  max mem: 31081
Val:  [620/728]  eta: 0:01:26  loss: 0.4574 (0.4759)  acc: 78.5714 (79.0066)  time: 0.8210  data: 0.2976  max mem: 31081
Val:  [630/728]  eta: 0:01:18  loss: 0.4553 (0.4757)  acc: 78.5714 (79.0167)  time: 0.8628  data: 0.3389  max mem: 31081
Val:  [640/728]  eta: 0:01:10  loss: 0.4222 (0.4748)  acc: 79.7619 (79.0543)  time: 0.8054  data: 0.2798  max mem: 31081
Val:  [650/728]  eta: 0:01:02  loss: 0.4484 (0.4752)  acc: 78.5714 (79.0396)  time: 0.8105  data: 0.2834  max mem: 31081
Val:  [660/728]  eta: 0:00:54  loss: 0.4484 (0.4751)  acc: 78.5714 (79.0127)  time: 0.7533  data: 0.2273  max mem: 31081
Val:  [670/728]  eta: 0:00:46  loss: 0.4202 (0.4759)  acc: 78.5714 (78.9759)  time: 0.6786  data: 0.1544  max mem: 31081
Val:  [680/728]  eta: 0:00:38  loss: 0.3772 (0.4749)  acc: 80.9524 (79.0032)  time: 0.7203  data: 0.1948  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3391 (0.4733)  acc: 82.1429 (79.0624)  time: 0.8319  data: 0.3095  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3893 (0.4738)  acc: 82.1429 (79.0249)  time: 0.8490  data: 0.3280  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4230 (0.4747)  acc: 76.1905 (78.9783)  time: 0.8118  data: 0.2876  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4348 (0.4751)  acc: 76.1905 (78.9611)  time: 0.7718  data: 0.2543  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4348 (0.4755)  acc: 76.1905 (78.9512)  time: 0.7471  data: 0.2542  max mem: 31081
Val: Total time: 0:09:37 (0.7927 s / it)
* Acc@1 78.951 AP 0.8025919198989868 loss 0.476
Accuracy of the network on the 61096 val videos: 79.0%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.59 GB / 503.51 GB
Epoch: [7]  [  0/893]  eta: 3:31:20  lr: 0.002177  min_lr: 0.000003  loss: 0.4771 (0.4771)  class_acc: 0.7679 (0.7679)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 14.2002  data: 12.8942  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.66 GB / 503.51 GB
Epoch: [7]  [ 10/893]  eta: 0:39:00  lr: 0.002177  min_lr: 0.000003  loss: 0.3462 (0.3596)  class_acc: 0.8571 (0.8442)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6509  data: 1.1726  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.24 GB / 503.51 GB
Epoch: [7]  [ 20/893]  eta: 0:30:39  lr: 0.002177  min_lr: 0.000003  loss: 0.3381 (0.3522)  class_acc: 0.8571 (0.8486)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5028  data: 0.0005  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.60 GB / 503.51 GB
Epoch: [7]  [ 30/893]  eta: 0:27:29  lr: 0.002176  min_lr: 0.000003  loss: 0.4026 (0.3918)  class_acc: 0.8214 (0.8249)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5052  data: 0.0005  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.59 GB / 503.51 GB
Epoch: [7]  [ 40/893]  eta: 0:25:44  lr: 0.002176  min_lr: 0.000003  loss: 0.4204 (0.3915)  class_acc: 0.7857 (0.8240)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4981  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.02 GB / 503.51 GB
Epoch: [7]  [ 50/893]  eta: 0:24:33  lr: 0.002176  min_lr: 0.000003  loss: 0.3757 (0.3912)  class_acc: 0.8214 (0.8249)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4950  data: 0.0005  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.71 GB / 503.51 GB
Epoch: [7]  [ 60/893]  eta: 0:23:37  lr: 0.002176  min_lr: 0.000003  loss: 0.3757 (0.3893)  class_acc: 0.8214 (0.8229)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4801  data: 0.0003  max mem: 31081
[2025-03-10 23:34:43,992] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 23:34:43,992] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.88 GB / 503.51 GB
Epoch: [7]  [ 70/893]  eta: 0:22:54  lr: 0.002176  min_lr: 0.000003  loss: 0.3960 (0.3917)  class_acc: 0.8393 (0.8217)  loss_scale: 8192.0000 (8653.5211)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.81 GB / 503.51 GB
Epoch: [7]  [ 80/893]  eta: 0:22:16  lr: 0.002176  min_lr: 0.000003  loss: 0.3999 (0.3905)  class_acc: 0.8214 (0.8225)  loss_scale: 16384.0000 (9607.9012)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0004  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.40 GB / 503.51 GB
Epoch: [7]  [ 90/893]  eta: 0:21:44  lr: 0.002176  min_lr: 0.000003  loss: 0.3735 (0.3862)  class_acc: 0.8214 (0.8246)  loss_scale: 16384.0000 (10352.5275)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0004  max mem: 31081
[2025-03-10 23:35:26,540] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 6340
[2025-03-10 23:35:26,540] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 23:35:26,540] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.69 GB / 503.51 GB
Epoch: [7]  [100/893]  eta: 0:21:16  lr: 0.002176  min_lr: 0.000003  loss: 0.3533 (0.3888)  class_acc: 0.8214 (0.8218)  loss_scale: 16384.0000 (10544.1584)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0005  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.53 GB / 503.51 GB
Epoch: [7]  [110/893]  eta: 0:20:50  lr: 0.002176  min_lr: 0.000003  loss: 0.3931 (0.3908)  class_acc: 0.8036 (0.8213)  loss_scale: 8192.0000 (10332.2523)  weight_decay: 0.0500 (0.0500)  time: 1.4726  data: 0.0004  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.33 GB / 503.51 GB
Epoch: [7]  [120/893]  eta: 0:20:26  lr: 0.002175  min_lr: 0.000003  loss: 0.4180 (0.3945)  class_acc: 0.8036 (0.8191)  loss_scale: 8192.0000 (10155.3719)  weight_decay: 0.0500 (0.0500)  time: 1.4698  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.87 GB / 503.51 GB
Epoch: [7]  [130/893]  eta: 0:20:03  lr: 0.002175  min_lr: 0.000003  loss: 0.4072 (0.3950)  class_acc: 0.8036 (0.8195)  loss_scale: 8192.0000 (10005.4962)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.26 GB / 503.51 GB
Epoch: [7]  [140/893]  eta: 0:19:40  lr: 0.002175  min_lr: 0.000003  loss: 0.3892 (0.3962)  class_acc: 0.8214 (0.8193)  loss_scale: 8192.0000 (9876.8794)  weight_decay: 0.0500 (0.0500)  time: 1.4599  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.20 GB / 503.51 GB
Epoch: [7]  [150/893]  eta: 0:19:19  lr: 0.002175  min_lr: 0.000003  loss: 0.4109 (0.3986)  class_acc: 0.8214 (0.8186)  loss_scale: 8192.0000 (9765.2980)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0002  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.09 GB / 503.51 GB
Epoch: [7]  [160/893]  eta: 0:18:59  lr: 0.002175  min_lr: 0.000003  loss: 0.4055 (0.3980)  class_acc: 0.8214 (0.8197)  loss_scale: 8192.0000 (9667.5776)  weight_decay: 0.0500 (0.0500)  time: 1.4602  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.80 GB / 503.51 GB
Epoch: [7]  [170/893]  eta: 0:18:40  lr: 0.002175  min_lr: 0.000003  loss: 0.3774 (0.3970)  class_acc: 0.8393 (0.8206)  loss_scale: 8192.0000 (9581.2865)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0004  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.12 GB / 503.51 GB
Epoch: [7]  [180/893]  eta: 0:18:21  lr: 0.002175  min_lr: 0.000003  loss: 0.3572 (0.3952)  class_acc: 0.8393 (0.8217)  loss_scale: 8192.0000 (9504.5304)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0004  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.02 GB / 503.51 GB
Epoch: [7]  [190/893]  eta: 0:18:02  lr: 0.002174  min_lr: 0.000003  loss: 0.3743 (0.3950)  class_acc: 0.8393 (0.8224)  loss_scale: 8192.0000 (9435.8115)  weight_decay: 0.0500 (0.0500)  time: 1.4596  data: 0.0003  max mem: 31081
[2025-03-10 23:37:57,178] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 6443
[2025-03-10 23:37:57,178] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-10 23:37:57,178] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.23 GB / 503.51 GB
Epoch: [7]  [200/893]  eta: 0:17:44  lr: 0.002174  min_lr: 0.000003  loss: 0.3936 (0.3954)  class_acc: 0.8214 (0.8221)  loss_scale: 8192.0000 (9333.1741)  weight_decay: 0.0500 (0.0500)  time: 1.4550  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.03 GB / 503.51 GB
Epoch: [7]  [210/893]  eta: 0:17:26  lr: 0.002174  min_lr: 0.000003  loss: 0.3965 (0.3948)  class_acc: 0.8214 (0.8223)  loss_scale: 4096.0000 (9084.9668)  weight_decay: 0.0500 (0.0500)  time: 1.4593  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.48 GB / 503.51 GB
Epoch: [7]  [220/893]  eta: 0:17:08  lr: 0.002174  min_lr: 0.000003  loss: 0.3479 (0.3940)  class_acc: 0.8393 (0.8226)  loss_scale: 4096.0000 (8859.2217)  weight_decay: 0.0500 (0.0500)  time: 1.4580  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.68 GB / 503.51 GB
Epoch: [7]  [230/893]  eta: 0:16:52  lr: 0.002174  min_lr: 0.000003  loss: 0.4065 (0.3959)  class_acc: 0.8214 (0.8221)  loss_scale: 4096.0000 (8653.0216)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.78 GB / 503.51 GB
Epoch: [7]  [240/893]  eta: 0:16:35  lr: 0.002174  min_lr: 0.000003  loss: 0.4189 (0.3955)  class_acc: 0.8214 (0.8222)  loss_scale: 4096.0000 (8463.9336)  weight_decay: 0.0500 (0.0500)  time: 1.4712  data: 0.0002  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.07 GB / 503.51 GB
Epoch: [7]  [250/893]  eta: 0:16:18  lr: 0.002174  min_lr: 0.000003  loss: 0.3953 (0.3955)  class_acc: 0.8214 (0.8219)  loss_scale: 4096.0000 (8289.9124)  weight_decay: 0.0500 (0.0500)  time: 1.4632  data: 0.0002  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.19 GB / 503.51 GB
Epoch: [7]  [260/893]  eta: 0:16:01  lr: 0.002174  min_lr: 0.000003  loss: 0.3960 (0.3951)  class_acc: 0.8036 (0.8217)  loss_scale: 4096.0000 (8129.2261)  weight_decay: 0.0500 (0.0500)  time: 1.4673  data: 0.0002  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.09 GB / 503.51 GB
Epoch: [7]  [270/893]  eta: 0:15:45  lr: 0.002173  min_lr: 0.000003  loss: 0.3958 (0.3942)  class_acc: 0.8214 (0.8223)  loss_scale: 4096.0000 (7980.3985)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0004  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.76 GB / 503.51 GB
Epoch: [7]  [280/893]  eta: 0:15:29  lr: 0.002173  min_lr: 0.000003  loss: 0.3538 (0.3935)  class_acc: 0.8214 (0.8226)  loss_scale: 4096.0000 (7842.1637)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0004  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.13 GB / 503.51 GB
Epoch: [7]  [290/893]  eta: 0:15:12  lr: 0.002173  min_lr: 0.000003  loss: 0.3538 (0.3933)  class_acc: 0.8214 (0.8226)  loss_scale: 4096.0000 (7713.4296)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0005  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.08 GB / 503.51 GB
Epoch: [7]  [300/893]  eta: 0:14:56  lr: 0.002173  min_lr: 0.000003  loss: 0.3823 (0.3935)  class_acc: 0.8036 (0.8225)  loss_scale: 4096.0000 (7593.2492)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0004  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.33 GB / 503.51 GB
Epoch: [7]  [310/893]  eta: 0:14:40  lr: 0.002173  min_lr: 0.000003  loss: 0.3379 (0.3923)  class_acc: 0.8393 (0.8233)  loss_scale: 4096.0000 (7480.7974)  weight_decay: 0.0500 (0.0500)  time: 1.4598  data: 0.0002  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.41 GB / 503.51 GB
Epoch: [7]  [320/893]  eta: 0:14:24  lr: 0.002173  min_lr: 0.000003  loss: 0.3596 (0.3935)  class_acc: 0.8393 (0.8229)  loss_scale: 4096.0000 (7375.3520)  weight_decay: 0.0500 (0.0500)  time: 1.4628  data: 0.0002  max mem: 31081
[2025-03-10 23:41:06,118] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 23:41:06,119] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.97 GB / 503.51 GB
Epoch: [7]  [330/893]  eta: 0:14:08  lr: 0.002173  min_lr: 0.000003  loss: 0.3838 (0.3928)  class_acc: 0.8393 (0.8234)  loss_scale: 4096.0000 (7313.4018)  weight_decay: 0.0500 (0.0500)  time: 1.4652  data: 0.0002  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.34 GB / 503.51 GB
Epoch: [7]  [340/893]  eta: 0:13:53  lr: 0.002172  min_lr: 0.000003  loss: 0.3599 (0.3925)  class_acc: 0.8393 (0.8235)  loss_scale: 8192.0000 (7339.1672)  weight_decay: 0.0500 (0.0500)  time: 1.4730  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.01 GB / 503.51 GB
Epoch: [7]  [350/893]  eta: 0:13:37  lr: 0.002172  min_lr: 0.000003  loss: 0.3887 (0.3935)  class_acc: 0.8393 (0.8233)  loss_scale: 8192.0000 (7363.4644)  weight_decay: 0.0500 (0.0500)  time: 1.4770  data: 0.0004  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.83 GB / 503.51 GB
Epoch: [7]  [360/893]  eta: 0:13:22  lr: 0.002172  min_lr: 0.000003  loss: 0.4033 (0.3934)  class_acc: 0.8036 (0.8234)  loss_scale: 8192.0000 (7386.4155)  weight_decay: 0.0500 (0.0500)  time: 1.4698  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.77 GB / 503.51 GB
Epoch: [7]  [370/893]  eta: 0:13:06  lr: 0.002172  min_lr: 0.000003  loss: 0.3889 (0.3931)  class_acc: 0.8393 (0.8235)  loss_scale: 8192.0000 (7408.1294)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.98 GB / 503.51 GB
Epoch: [7]  [380/893]  eta: 0:12:51  lr: 0.002172  min_lr: 0.000003  loss: 0.4023 (0.3935)  class_acc: 0.8036 (0.8231)  loss_scale: 8192.0000 (7428.7034)  weight_decay: 0.0500 (0.0500)  time: 1.4716  data: 0.0004  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.03 GB / 503.51 GB
Epoch: [7]  [390/893]  eta: 0:12:35  lr: 0.002172  min_lr: 0.000003  loss: 0.4109 (0.3930)  class_acc: 0.8214 (0.8237)  loss_scale: 8192.0000 (7448.2251)  weight_decay: 0.0500 (0.0500)  time: 1.4706  data: 0.0003  max mem: 31081
[2025-03-10 23:42:38,756] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 6635
[2025-03-10 23:42:38,756] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-10 23:42:38,756] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.16 GB / 503.51 GB
Epoch: [7]  [400/893]  eta: 0:12:20  lr: 0.002172  min_lr: 0.000003  loss: 0.3774 (0.3931)  class_acc: 0.8393 (0.8238)  loss_scale: 4096.0000 (7364.6284)  weight_decay: 0.0500 (0.0500)  time: 1.4609  data: 0.0003  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.43 GB / 503.51 GB
Epoch: [7]  [410/893]  eta: 0:12:04  lr: 0.002171  min_lr: 0.000003  loss: 0.3870 (0.3929)  class_acc: 0.8214 (0.8237)  loss_scale: 4096.0000 (7285.0998)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0004  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.58 GB / 503.51 GB
Epoch: [7]  [420/893]  eta: 0:11:49  lr: 0.002171  min_lr: 0.000003  loss: 0.3618 (0.3918)  class_acc: 0.8393 (0.8245)  loss_scale: 4096.0000 (7209.3492)  weight_decay: 0.0500 (0.0500)  time: 1.4558  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.37 GB / 503.51 GB
Epoch: [7]  [430/893]  eta: 0:11:33  lr: 0.002171  min_lr: 0.000003  loss: 0.3618 (0.3924)  class_acc: 0.8393 (0.8244)  loss_scale: 4096.0000 (7137.1137)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0002  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.97 GB / 503.51 GB
Epoch: [7]  [440/893]  eta: 0:11:18  lr: 0.002171  min_lr: 0.000003  loss: 0.4026 (0.3925)  class_acc: 0.8214 (0.8245)  loss_scale: 4096.0000 (7068.1542)  weight_decay: 0.0500 (0.0500)  time: 1.4723  data: 0.0002  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.69 GB / 503.51 GB
Epoch: [7]  [450/893]  eta: 0:11:03  lr: 0.002171  min_lr: 0.000003  loss: 0.3953 (0.3926)  class_acc: 0.8214 (0.8241)  loss_scale: 4096.0000 (7002.2528)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.05 GB / 503.51 GB
Epoch: [7]  [460/893]  eta: 0:10:47  lr: 0.002171  min_lr: 0.000003  loss: 0.3657 (0.3923)  class_acc: 0.8036 (0.8244)  loss_scale: 4096.0000 (6939.2104)  weight_decay: 0.0500 (0.0500)  time: 1.4650  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.21 GB / 503.51 GB
Epoch: [7]  [470/893]  eta: 0:10:32  lr: 0.002171  min_lr: 0.000003  loss: 0.3657 (0.3921)  class_acc: 0.8393 (0.8245)  loss_scale: 4096.0000 (6878.8450)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0002  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.46 GB / 503.51 GB
Epoch: [7]  [480/893]  eta: 0:10:17  lr: 0.002170  min_lr: 0.000003  loss: 0.3699 (0.3922)  class_acc: 0.8393 (0.8245)  loss_scale: 4096.0000 (6820.9896)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0002  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.08 GB / 503.51 GB
Epoch: [7]  [490/893]  eta: 0:10:02  lr: 0.002170  min_lr: 0.000003  loss: 0.3711 (0.3917)  class_acc: 0.8393 (0.8247)  loss_scale: 4096.0000 (6765.4908)  weight_decay: 0.0500 (0.0500)  time: 1.4611  data: 0.0002  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.84 GB / 503.51 GB
Epoch: [7]  [500/893]  eta: 0:09:46  lr: 0.002170  min_lr: 0.000003  loss: 0.3765 (0.3914)  class_acc: 0.8393 (0.8247)  loss_scale: 4096.0000 (6712.2076)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.43 GB / 503.51 GB
Epoch: [7]  [510/893]  eta: 0:09:31  lr: 0.002170  min_lr: 0.000003  loss: 0.3894 (0.3914)  class_acc: 0.8393 (0.8250)  loss_scale: 4096.0000 (6661.0098)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0003  max mem: 31081
[2025-03-10 23:45:47,507] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 23:45:47,507] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.29 GB / 503.51 GB
Epoch: [7]  [520/893]  eta: 0:09:16  lr: 0.002170  min_lr: 0.000003  loss: 0.3582 (0.3903)  class_acc: 0.8571 (0.8256)  loss_scale: 4096.0000 (6619.6392)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.50 GB / 503.51 GB
Epoch: [7]  [530/893]  eta: 0:09:01  lr: 0.002170  min_lr: 0.000003  loss: 0.3567 (0.3904)  class_acc: 0.8393 (0.8254)  loss_scale: 8192.0000 (6649.2505)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.85 GB / 503.51 GB
Epoch: [7]  [540/893]  eta: 0:08:46  lr: 0.002169  min_lr: 0.000003  loss: 0.3740 (0.3898)  class_acc: 0.8393 (0.8256)  loss_scale: 8192.0000 (6677.7671)  weight_decay: 0.0500 (0.0500)  time: 1.4715  data: 0.0002  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.39 GB / 503.51 GB
Epoch: [7]  [550/893]  eta: 0:08:31  lr: 0.002169  min_lr: 0.000003  loss: 0.3247 (0.3897)  class_acc: 0.8393 (0.8256)  loss_scale: 8192.0000 (6705.2486)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0002  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.84 GB / 503.51 GB
Epoch: [7]  [560/893]  eta: 0:08:16  lr: 0.002169  min_lr: 0.000003  loss: 0.3982 (0.3896)  class_acc: 0.8393 (0.8259)  loss_scale: 8192.0000 (6731.7504)  weight_decay: 0.0500 (0.0500)  time: 1.4533  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.96 GB / 503.51 GB
Epoch: [7]  [570/893]  eta: 0:08:01  lr: 0.002169  min_lr: 0.000003  loss: 0.3982 (0.3901)  class_acc: 0.8393 (0.8257)  loss_scale: 8192.0000 (6757.3240)  weight_decay: 0.0500 (0.0500)  time: 1.4609  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.20 GB / 503.51 GB
Epoch: [7]  [580/893]  eta: 0:07:46  lr: 0.002169  min_lr: 0.000003  loss: 0.4204 (0.3905)  class_acc: 0.8036 (0.8253)  loss_scale: 8192.0000 (6782.0172)  weight_decay: 0.0500 (0.0500)  time: 1.4614  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.37 GB / 503.51 GB
Epoch: [7]  [590/893]  eta: 0:07:31  lr: 0.002169  min_lr: 0.000003  loss: 0.3701 (0.3899)  class_acc: 0.8036 (0.8252)  loss_scale: 8192.0000 (6805.8748)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.51 GB / 503.51 GB
Epoch: [7]  [600/893]  eta: 0:07:16  lr: 0.002169  min_lr: 0.000003  loss: 0.3701 (0.3899)  class_acc: 0.8036 (0.8250)  loss_scale: 8192.0000 (6828.9384)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.07 GB / 503.51 GB
Epoch: [7]  [610/893]  eta: 0:07:01  lr: 0.002168  min_lr: 0.000003  loss: 0.3938 (0.3900)  class_acc: 0.8036 (0.8248)  loss_scale: 8192.0000 (6851.2471)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.66 GB / 503.51 GB
Epoch: [7]  [620/893]  eta: 0:06:46  lr: 0.002168  min_lr: 0.000003  loss: 0.4141 (0.3907)  class_acc: 0.8036 (0.8245)  loss_scale: 8192.0000 (6872.8374)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.73 GB / 503.51 GB
Epoch: [7]  [630/893]  eta: 0:06:31  lr: 0.002168  min_lr: 0.000003  loss: 0.3789 (0.3900)  class_acc: 0.8214 (0.8247)  loss_scale: 8192.0000 (6893.7433)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.76 GB / 503.51 GB
Epoch: [7]  [640/893]  eta: 0:06:16  lr: 0.002168  min_lr: 0.000003  loss: 0.3577 (0.3896)  class_acc: 0.8393 (0.8251)  loss_scale: 8192.0000 (6913.9969)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
[2025-03-10 23:48:54,792] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-10 23:48:54,792] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-03-10 23:48:57,728] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 6894
[2025-03-10 23:48:57,728] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-10 23:48:57,728] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.80 GB / 503.51 GB
Epoch: [7]  [650/893]  eta: 0:06:01  lr: 0.002168  min_lr: 0.000003  loss: 0.3630 (0.3896)  class_acc: 0.8393 (0.8251)  loss_scale: 8192.0000 (6958.7957)  weight_decay: 0.0500 (0.0500)  time: 1.4573  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.74 GB / 503.51 GB
Epoch: [7]  [660/893]  eta: 0:05:46  lr: 0.002168  min_lr: 0.000003  loss: 0.3713 (0.3892)  class_acc: 0.8214 (0.8253)  loss_scale: 8192.0000 (6977.4523)  weight_decay: 0.0500 (0.0500)  time: 1.4644  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.70 GB / 503.51 GB
Epoch: [7]  [670/893]  eta: 0:05:31  lr: 0.002167  min_lr: 0.000003  loss: 0.3735 (0.3895)  class_acc: 0.8393 (0.8252)  loss_scale: 8192.0000 (6995.5529)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.05 GB / 503.51 GB
Epoch: [7]  [680/893]  eta: 0:05:16  lr: 0.002167  min_lr: 0.000003  loss: 0.3804 (0.3898)  class_acc: 0.8393 (0.8252)  loss_scale: 8192.0000 (7013.1219)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.34 GB / 503.51 GB
Epoch: [7]  [690/893]  eta: 0:05:01  lr: 0.002167  min_lr: 0.000003  loss: 0.3823 (0.3892)  class_acc: 0.8393 (0.8256)  loss_scale: 8192.0000 (7030.1823)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.19 GB / 503.51 GB
Epoch: [7]  [700/893]  eta: 0:04:46  lr: 0.002167  min_lr: 0.000003  loss: 0.3723 (0.3893)  class_acc: 0.8393 (0.8256)  loss_scale: 8192.0000 (7046.7561)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0002  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.31 GB / 503.51 GB
Epoch: [7]  [710/893]  eta: 0:04:31  lr: 0.002167  min_lr: 0.000003  loss: 0.3442 (0.3886)  class_acc: 0.8393 (0.8262)  loss_scale: 8192.0000 (7062.8636)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.72 GB / 503.51 GB
Epoch: [7]  [720/893]  eta: 0:04:16  lr: 0.002167  min_lr: 0.000003  loss: 0.3621 (0.3888)  class_acc: 0.8393 (0.8262)  loss_scale: 8192.0000 (7078.5243)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.34 GB / 503.51 GB
Epoch: [7]  [730/893]  eta: 0:04:01  lr: 0.002166  min_lr: 0.000003  loss: 0.3733 (0.3888)  class_acc: 0.8393 (0.8262)  loss_scale: 8192.0000 (7093.7565)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0002  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.64 GB / 503.51 GB
Epoch: [7]  [740/893]  eta: 0:03:47  lr: 0.002166  min_lr: 0.000003  loss: 0.3618 (0.3885)  class_acc: 0.8393 (0.8263)  loss_scale: 8192.0000 (7108.5776)  weight_decay: 0.0500 (0.0500)  time: 1.4609  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.49 GB / 503.51 GB
Epoch: [7]  [750/893]  eta: 0:03:32  lr: 0.002166  min_lr: 0.000003  loss: 0.3455 (0.3879)  class_acc: 0.8750 (0.8267)  loss_scale: 8192.0000 (7123.0040)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0004  max mem: 31081
[2025-03-10 23:51:31,681] [INFO] [logging.py:129:log_dist] [Rank 0] step=7000, skipped=39, lr=[2.8289174080620055e-06, 2.8289174080620055e-06, 4.71486234677001e-06, 4.71486234677001e-06, 7.858103911283349e-06, 7.858103911283349e-06, 1.3096839852138914e-05, 1.3096839852138914e-05, 2.1828066420231526e-05, 2.1828066420231526e-05, 3.638011070038588e-05, 3.638011070038588e-05, 6.063351783397647e-05, 6.063351783397647e-05, 0.00010105586305662745, 0.00010105586305662745, 0.0001684264384277124, 0.0001684264384277124, 0.0002807107307128541, 0.0002807107307128541, 0.0004678512178547567, 0.0004678512178547567, 0.000779752029757928, 0.000779752029757928, 0.0012995867162632134, 0.0012995867162632134, 0.002165977860438689, 0.002165977860438689], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-10 23:51:31,682] [INFO] [timer.py:264:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=61.08643908483175, CurrSamplesPerSec=61.57541252455771, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.91 GB / 503.51 GB
Epoch: [7]  [760/893]  eta: 0:03:17  lr: 0.002166  min_lr: 0.000003  loss: 0.3411 (0.3879)  class_acc: 0.8571 (0.8268)  loss_scale: 8192.0000 (7137.0512)  weight_decay: 0.0500 (0.0500)  time: 1.4764  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.37 GB / 503.51 GB
Epoch: [7]  [770/893]  eta: 0:03:02  lr: 0.002166  min_lr: 0.000003  loss: 0.3701 (0.3883)  class_acc: 0.8214 (0.8265)  loss_scale: 8192.0000 (7150.7341)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
[2025-03-10 23:51:55,106] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 7015
[2025-03-10 23:51:55,106] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-10 23:51:55,106] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.63 GB / 503.51 GB
Epoch: [7]  [780/893]  eta: 0:02:47  lr: 0.002166  min_lr: 0.000003  loss: 0.3923 (0.3884)  class_acc: 0.8214 (0.8266)  loss_scale: 4096.0000 (7111.6210)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.47 GB / 503.51 GB
Epoch: [7]  [790/893]  eta: 0:02:32  lr: 0.002165  min_lr: 0.000003  loss: 0.3923 (0.3888)  class_acc: 0.8214 (0.8265)  loss_scale: 4096.0000 (7073.4968)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0004  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.62 GB / 503.51 GB
Epoch: [7]  [800/893]  eta: 0:02:17  lr: 0.002165  min_lr: 0.000003  loss: 0.3562 (0.3884)  class_acc: 0.8214 (0.8265)  loss_scale: 4096.0000 (7036.3246)  weight_decay: 0.0500 (0.0500)  time: 1.4766  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.37 GB / 503.51 GB
Epoch: [7]  [810/893]  eta: 0:02:03  lr: 0.002165  min_lr: 0.000003  loss: 0.3701 (0.3883)  class_acc: 0.8214 (0.8266)  loss_scale: 4096.0000 (7000.0691)  weight_decay: 0.0500 (0.0500)  time: 1.4729  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.13 GB / 503.51 GB
Epoch: [7]  [820/893]  eta: 0:01:48  lr: 0.002165  min_lr: 0.000003  loss: 0.3809 (0.3882)  class_acc: 0.8393 (0.8267)  loss_scale: 4096.0000 (6964.6967)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.45 GB / 503.51 GB
Epoch: [7]  [830/893]  eta: 0:01:33  lr: 0.002165  min_lr: 0.000003  loss: 0.3809 (0.3880)  class_acc: 0.8214 (0.8267)  loss_scale: 4096.0000 (6930.1757)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.22 GB / 503.51 GB
Epoch: [7]  [840/893]  eta: 0:01:18  lr: 0.002165  min_lr: 0.000003  loss: 0.3743 (0.3880)  class_acc: 0.8214 (0.8268)  loss_scale: 4096.0000 (6896.4756)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.18 GB / 503.51 GB
Epoch: [7]  [850/893]  eta: 0:01:03  lr: 0.002164  min_lr: 0.000003  loss: 0.4065 (0.3887)  class_acc: 0.8214 (0.8264)  loss_scale: 4096.0000 (6863.5676)  weight_decay: 0.0500 (0.0500)  time: 1.4658  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.01 GB / 503.51 GB
Epoch: [7]  [860/893]  eta: 0:00:48  lr: 0.002164  min_lr: 0.000003  loss: 0.4065 (0.3885)  class_acc: 0.8036 (0.8264)  loss_scale: 4096.0000 (6831.4239)  weight_decay: 0.0500 (0.0500)  time: 1.4566  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.86 GB / 503.51 GB
Epoch: [7]  [870/893]  eta: 0:00:34  lr: 0.002164  min_lr: 0.000003  loss: 0.3696 (0.3886)  class_acc: 0.8036 (0.8262)  loss_scale: 4096.0000 (6800.0184)  weight_decay: 0.0500 (0.0500)  time: 1.4464  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.95 GB / 503.51 GB
Epoch: [7]  [880/893]  eta: 0:00:19  lr: 0.002164  min_lr: 0.000003  loss: 0.3916 (0.3888)  class_acc: 0.8036 (0.8261)  loss_scale: 4096.0000 (6769.3258)  weight_decay: 0.0500 (0.0500)  time: 1.4483  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.55 GB / 503.51 GB
Epoch: [7]  [890/893]  eta: 0:00:04  lr: 0.002164  min_lr: 0.000003  loss: 0.3809 (0.3885)  class_acc: 0.8214 (0.8262)  loss_scale: 4096.0000 (6739.3221)  weight_decay: 0.0500 (0.0500)  time: 1.4474  data: 0.0001  max mem: 31081
Epoch: [7]  [892/893]  eta: 0:00:01  lr: 0.002164  min_lr: 0.000003  loss: 0.4041 (0.3886)  class_acc: 0.8214 (0.8261)  loss_scale: 4096.0000 (6736.3587)  weight_decay: 0.0500 (0.0500)  time: 1.3950  data: 0.0001  max mem: 31081
Epoch: [7] Total time: 0:22:01 (1.4795 s / it)
Averaged stats: lr: 0.002164  min_lr: 0.000003  loss: 0.4041 (0.3886)  class_acc: 0.8214 (0.8261)  loss_scale: 4096.0000 (6736.3587)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:14:35  loss: 0.6771 (0.6771)  acc: 72.6190 (72.6190)  time: 11.0931  data: 10.5651  max mem: 31081
Val:  [ 10/728]  eta: 0:18:18  loss: 0.4142 (0.4851)  acc: 80.9524 (79.3290)  time: 1.5299  data: 1.0099  max mem: 31081
Val:  [ 20/728]  eta: 0:14:09  loss: 0.4142 (0.4661)  acc: 80.9524 (79.3084)  time: 0.7052  data: 0.1842  max mem: 31081
Val:  [ 30/728]  eta: 0:12:37  loss: 0.4446 (0.4656)  acc: 80.9524 (78.3026)  time: 0.8397  data: 0.3170  max mem: 31081
Val:  [ 40/728]  eta: 0:11:48  loss: 0.4336 (0.4747)  acc: 77.3810 (77.8746)  time: 0.8505  data: 0.3279  max mem: 31081
Val:  [ 50/728]  eta: 0:11:10  loss: 0.3567 (0.4500)  acc: 84.5238 (79.1083)  time: 0.8417  data: 0.3190  max mem: 31081
Val:  [ 60/728]  eta: 0:10:19  loss: 0.3900 (0.4690)  acc: 78.5714 (78.5909)  time: 0.7174  data: 0.1943  max mem: 31081
Val:  [ 70/728]  eta: 0:09:47  loss: 0.4823 (0.4746)  acc: 76.1905 (78.2696)  time: 0.6477  data: 0.1238  max mem: 31081
Val:  [ 80/728]  eta: 0:09:36  loss: 0.4584 (0.4851)  acc: 72.6190 (77.7190)  time: 0.7772  data: 0.2514  max mem: 31081
Val:  [ 90/728]  eta: 0:09:20  loss: 0.4441 (0.4933)  acc: 77.3810 (77.4987)  time: 0.8261  data: 0.3000  max mem: 31081
Val:  [100/728]  eta: 0:09:08  loss: 0.4013 (0.4826)  acc: 80.9524 (78.0410)  time: 0.8054  data: 0.2829  max mem: 31081
Val:  [110/728]  eta: 0:08:58  loss: 0.3761 (0.4814)  acc: 80.9524 (77.9065)  time: 0.8407  data: 0.3203  max mem: 31081
Val:  [120/728]  eta: 0:08:44  loss: 0.3809 (0.4747)  acc: 83.3333 (78.4435)  time: 0.8127  data: 0.2923  max mem: 31081
Val:  [130/728]  eta: 0:08:23  loss: 0.3820 (0.4836)  acc: 83.3333 (78.2079)  time: 0.6785  data: 0.1557  max mem: 31081
Val:  [140/728]  eta: 0:08:13  loss: 0.4498 (0.4893)  acc: 77.3810 (77.8960)  time: 0.6964  data: 0.1729  max mem: 31081
Val:  [150/728]  eta: 0:07:59  loss: 0.4948 (0.4919)  acc: 75.0000 (77.8382)  time: 0.7489  data: 0.2261  max mem: 31081
Val:  [160/728]  eta: 0:07:52  loss: 0.5075 (0.4984)  acc: 72.6190 (77.5658)  time: 0.7709  data: 0.2462  max mem: 31081
Val:  [170/728]  eta: 0:07:43  loss: 0.4642 (0.4968)  acc: 78.5714 (77.6594)  time: 0.8364  data: 0.3123  max mem: 31081
Val:  [180/728]  eta: 0:07:31  loss: 0.4322 (0.4976)  acc: 80.9524 (77.6243)  time: 0.7656  data: 0.2446  max mem: 31081
Val:  [190/728]  eta: 0:07:18  loss: 0.4424 (0.4947)  acc: 78.5714 (77.6988)  time: 0.6772  data: 0.1572  max mem: 31081
Val:  [200/728]  eta: 0:07:09  loss: 0.4049 (0.4902)  acc: 80.9524 (77.8844)  time: 0.7251  data: 0.2044  max mem: 31081
Val:  [210/728]  eta: 0:07:04  loss: 0.3904 (0.4910)  acc: 80.9524 (77.9395)  time: 0.8638  data: 0.3410  max mem: 31081
Val:  [220/728]  eta: 0:06:57  loss: 0.3831 (0.4874)  acc: 82.1429 (78.1459)  time: 0.9058  data: 0.3825  max mem: 31081
Val:  [230/728]  eta: 0:06:48  loss: 0.3675 (0.4891)  acc: 82.1429 (78.1488)  time: 0.8235  data: 0.3027  max mem: 31081
Val:  [240/728]  eta: 0:06:37  loss: 0.3675 (0.4845)  acc: 83.3333 (78.4183)  time: 0.7226  data: 0.2010  max mem: 31081
Val:  [250/728]  eta: 0:06:25  loss: 0.3881 (0.4833)  acc: 82.1429 (78.4813)  time: 0.6630  data: 0.1412  max mem: 31081
Val:  [260/728]  eta: 0:06:18  loss: 0.4098 (0.4821)  acc: 79.7619 (78.6034)  time: 0.7540  data: 0.2339  max mem: 31081
Val:  [270/728]  eta: 0:06:11  loss: 0.3672 (0.4812)  acc: 82.1429 (78.6637)  time: 0.8608  data: 0.3384  max mem: 31081
Val:  [280/728]  eta: 0:06:06  loss: 0.4872 (0.4811)  acc: 76.1905 (78.6477)  time: 0.9311  data: 0.4074  max mem: 31081
Val:  [290/728]  eta: 0:05:58  loss: 0.3639 (0.4781)  acc: 78.5714 (78.7351)  time: 0.9101  data: 0.3865  max mem: 31081
Val:  [300/728]  eta: 0:05:45  loss: 0.3685 (0.4769)  acc: 78.5714 (78.8404)  time: 0.6690  data: 0.1428  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.4489 (0.4787)  acc: 78.5714 (78.8088)  time: 0.6811  data: 0.1557  max mem: 31081
Val:  [320/728]  eta: 0:05:29  loss: 0.4144 (0.4785)  acc: 78.5714 (78.7754)  time: 0.8113  data: 0.2863  max mem: 31081
Val:  [330/728]  eta: 0:05:22  loss: 0.3717 (0.4764)  acc: 83.3333 (78.8987)  time: 0.8290  data: 0.3036  max mem: 31081
Val:  [340/728]  eta: 0:05:15  loss: 0.3711 (0.4725)  acc: 83.3333 (79.0742)  time: 0.8792  data: 0.3579  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.2943 (0.4727)  acc: 85.7143 (79.1175)  time: 0.8468  data: 0.3268  max mem: 31081
Val:  [360/728]  eta: 0:04:55  loss: 0.3651 (0.4707)  acc: 80.9524 (79.1815)  time: 0.6635  data: 0.1419  max mem: 31081
Val:  [370/728]  eta: 0:04:48  loss: 0.4086 (0.4733)  acc: 80.9524 (79.0592)  time: 0.7013  data: 0.1783  max mem: 31081
Val:  [380/728]  eta: 0:04:40  loss: 0.4623 (0.4756)  acc: 77.3810 (79.0276)  time: 0.8565  data: 0.3324  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.3347 (0.4716)  acc: 82.1429 (79.1743)  time: 0.8346  data: 0.3084  max mem: 31081
Val:  [400/728]  eta: 0:04:25  loss: 0.2638 (0.4703)  acc: 85.7143 (79.2988)  time: 0.8653  data: 0.3383  max mem: 31081
Val:  [410/728]  eta: 0:04:17  loss: 0.3380 (0.4686)  acc: 83.3333 (79.3911)  time: 0.8698  data: 0.3455  max mem: 31081
Val:  [420/728]  eta: 0:04:07  loss: 0.4486 (0.4689)  acc: 82.1429 (79.3830)  time: 0.6864  data: 0.1614  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4486 (0.4692)  acc: 78.5714 (79.3559)  time: 0.6982  data: 0.1750  max mem: 31081
Val:  [440/728]  eta: 0:03:51  loss: 0.4132 (0.4688)  acc: 79.7619 (79.3435)  time: 0.8361  data: 0.3170  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.3894 (0.4693)  acc: 83.3333 (79.3000)  time: 0.8361  data: 0.3145  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.3742 (0.4709)  acc: 83.3333 (79.2428)  time: 0.8439  data: 0.3219  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.3681 (0.4691)  acc: 80.9524 (79.3246)  time: 0.8001  data: 0.2760  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.3681 (0.4679)  acc: 80.9524 (79.3412)  time: 0.6519  data: 0.1267  max mem: 31081
Val:  [490/728]  eta: 0:03:10  loss: 0.4708 (0.4683)  acc: 77.3810 (79.3085)  time: 0.6850  data: 0.1627  max mem: 31081
Val:  [500/728]  eta: 0:03:02  loss: 0.4819 (0.4696)  acc: 77.3810 (79.2700)  time: 0.8333  data: 0.3114  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3196 (0.4674)  acc: 85.7143 (79.3612)  time: 0.8716  data: 0.3495  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.3436 (0.4698)  acc: 78.5714 (79.2227)  time: 0.8556  data: 0.3351  max mem: 31081
Val:  [530/728]  eta: 0:02:38  loss: 0.6063 (0.4693)  acc: 75.0000 (79.2776)  time: 0.7379  data: 0.2168  max mem: 31081
Val:  [540/728]  eta: 0:02:29  loss: 0.4663 (0.4718)  acc: 78.5714 (79.1414)  time: 0.6074  data: 0.0825  max mem: 31081
Val:  [550/728]  eta: 0:02:21  loss: 0.4874 (0.4726)  acc: 78.5714 (79.1094)  time: 0.6613  data: 0.1374  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.3986 (0.4712)  acc: 83.3333 (79.1783)  time: 0.7564  data: 0.2327  max mem: 31081
Val:  [570/728]  eta: 0:02:05  loss: 0.3977 (0.4749)  acc: 79.7619 (79.0843)  time: 0.8057  data: 0.2810  max mem: 31081
Val:  [580/728]  eta: 0:01:57  loss: 0.4159 (0.4747)  acc: 79.7619 (79.1390)  time: 0.8363  data: 0.3125  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.4159 (0.4752)  acc: 82.1429 (79.1435)  time: 0.7280  data: 0.2048  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4267 (0.4760)  acc: 79.7619 (79.1003)  time: 0.7005  data: 0.1754  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4497 (0.4786)  acc: 76.1905 (79.0059)  time: 0.6512  data: 0.1273  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4483 (0.4818)  acc: 79.7619 (78.9280)  time: 0.7387  data: 0.2169  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4151 (0.4806)  acc: 80.9524 (78.9940)  time: 0.8479  data: 0.3258  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3845 (0.4796)  acc: 82.1429 (79.0302)  time: 0.7882  data: 0.2652  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4436 (0.4793)  acc: 82.1429 (79.0597)  time: 0.8027  data: 0.2773  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4082 (0.4783)  acc: 82.1429 (79.0865)  time: 0.8118  data: 0.2851  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4216 (0.4786)  acc: 80.9524 (79.0913)  time: 0.6845  data: 0.1572  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4189 (0.4776)  acc: 80.9524 (79.1413)  time: 0.6455  data: 0.1193  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3652 (0.4762)  acc: 85.7143 (79.2347)  time: 0.8376  data: 0.3107  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3822 (0.4763)  acc: 84.5238 (79.2236)  time: 0.8577  data: 0.3319  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.5289 (0.4774)  acc: 77.3810 (79.2043)  time: 0.8424  data: 0.3203  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.3943 (0.4770)  acc: 82.1429 (79.2203)  time: 0.8190  data: 0.3053  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.3928 (0.4775)  acc: 82.1429 (79.1836)  time: 0.7944  data: 0.3052  max mem: 31081
Val: Total time: 0:09:33 (0.7879 s / it)
* Acc@1 79.184 AP 0.8015035390853882 loss 0.477
Accuracy of the network on the 61096 val videos: 79.2%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.86 GB / 503.51 GB
Epoch: [8]  [  0/893]  eta: 3:34:25  lr: 0.002164  min_lr: 0.000003  loss: 0.3965 (0.3965)  class_acc: 0.7857 (0.7857)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 14.4066  data: 13.0838  max mem: 31081
[2025-03-11 00:04:52,323] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 00:04:52,323] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.98 GB / 503.51 GB
Epoch: [8]  [ 10/893]  eta: 0:39:22  lr: 0.002163  min_lr: 0.000003  loss: 0.3657 (0.3691)  class_acc: 0.8393 (0.8425)  loss_scale: 4096.0000 (5213.0909)  weight_decay: 0.0500 (0.0500)  time: 2.6754  data: 1.1898  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.19 GB / 503.51 GB
Epoch: [8]  [ 20/893]  eta: 0:30:49  lr: 0.002163  min_lr: 0.000003  loss: 0.3657 (0.3840)  class_acc: 0.8393 (0.8274)  loss_scale: 8192.0000 (6631.6190)  weight_decay: 0.0500 (0.0500)  time: 1.5044  data: 0.0005  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.52 GB / 503.51 GB
Epoch: [8]  [ 30/893]  eta: 0:27:38  lr: 0.002163  min_lr: 0.000003  loss: 0.4041 (0.3880)  class_acc: 0.8036 (0.8220)  loss_scale: 8192.0000 (7134.9677)  weight_decay: 0.0500 (0.0500)  time: 1.5079  data: 0.0005  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.12 GB / 503.51 GB
Epoch: [8]  [ 40/893]  eta: 0:25:51  lr: 0.002163  min_lr: 0.000003  loss: 0.4016 (0.3898)  class_acc: 0.8036 (0.8223)  loss_scale: 8192.0000 (7392.7805)  weight_decay: 0.0500 (0.0500)  time: 1.5038  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.08 GB / 503.51 GB
Epoch: [8]  [ 50/893]  eta: 0:24:38  lr: 0.002163  min_lr: 0.000003  loss: 0.3904 (0.3897)  class_acc: 0.8214 (0.8232)  loss_scale: 8192.0000 (7549.4902)  weight_decay: 0.0500 (0.0500)  time: 1.4943  data: 0.0005  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.72 GB / 503.51 GB
Epoch: [8]  [ 60/893]  eta: 0:23:41  lr: 0.002163  min_lr: 0.000003  loss: 0.3401 (0.3841)  class_acc: 0.8393 (0.8261)  loss_scale: 8192.0000 (7654.8197)  weight_decay: 0.0500 (0.0500)  time: 1.4771  data: 0.0003  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.79 GB / 503.51 GB
Epoch: [8]  [ 70/893]  eta: 0:22:57  lr: 0.002162  min_lr: 0.000003  loss: 0.3472 (0.3837)  class_acc: 0.8214 (0.8265)  loss_scale: 8192.0000 (7730.4789)  weight_decay: 0.0500 (0.0500)  time: 1.4679  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.86 GB / 503.51 GB
Epoch: [8]  [ 80/893]  eta: 0:22:19  lr: 0.002162  min_lr: 0.000003  loss: 0.4082 (0.3889)  class_acc: 0.8036 (0.8239)  loss_scale: 8192.0000 (7787.4568)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.39 GB / 503.51 GB
Epoch: [8]  [ 90/893]  eta: 0:21:48  lr: 0.002162  min_lr: 0.000003  loss: 0.4524 (0.3961)  class_acc: 0.8036 (0.8203)  loss_scale: 8192.0000 (7831.9121)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.21 GB / 503.51 GB
Epoch: [8]  [100/893]  eta: 0:21:18  lr: 0.002162  min_lr: 0.000003  loss: 0.4265 (0.3978)  class_acc: 0.8036 (0.8190)  loss_scale: 8192.0000 (7867.5644)  weight_decay: 0.0500 (0.0500)  time: 1.4695  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.20 GB / 503.51 GB
Epoch: [8]  [110/893]  eta: 0:20:51  lr: 0.002162  min_lr: 0.000003  loss: 0.3848 (0.3945)  class_acc: 0.8214 (0.8216)  loss_scale: 8192.0000 (7896.7928)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.63 GB / 503.51 GB
Epoch: [8]  [120/893]  eta: 0:20:27  lr: 0.002161  min_lr: 0.000003  loss: 0.3667 (0.3943)  class_acc: 0.8214 (0.8217)  loss_scale: 8192.0000 (7921.1901)  weight_decay: 0.0500 (0.0500)  time: 1.4596  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.58 GB / 503.51 GB
Epoch: [8]  [130/893]  eta: 0:20:03  lr: 0.002161  min_lr: 0.000003  loss: 0.3845 (0.3942)  class_acc: 0.8214 (0.8220)  loss_scale: 8192.0000 (7941.8626)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0003  max mem: 31081
[2025-03-11 00:08:01,426] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 00:08:01,426] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.42 GB / 503.51 GB
Epoch: [8]  [140/893]  eta: 0:19:41  lr: 0.002161  min_lr: 0.000003  loss: 0.3845 (0.3949)  class_acc: 0.8393 (0.8222)  loss_scale: 8192.0000 (8250.0993)  weight_decay: 0.0500 (0.0500)  time: 1.4617  data: 0.0003  max mem: 31081
[2025-03-11 00:08:14,603] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 7281
[2025-03-11 00:08:14,603] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 00:08:14,603] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.32 GB / 503.51 GB
Epoch: [8]  [150/893]  eta: 0:19:20  lr: 0.002161  min_lr: 0.000003  loss: 0.4124 (0.3965)  class_acc: 0.8393 (0.8220)  loss_scale: 8192.0000 (8463.2583)  weight_decay: 0.0500 (0.0500)  time: 1.4611  data: 0.0002  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.34 GB / 503.51 GB
Epoch: [8]  [160/893]  eta: 0:19:00  lr: 0.002161  min_lr: 0.000003  loss: 0.3960 (0.3953)  class_acc: 0.8393 (0.8225)  loss_scale: 8192.0000 (8446.4099)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.69 GB / 503.51 GB
Epoch: [8]  [170/893]  eta: 0:18:40  lr: 0.002160  min_lr: 0.000003  loss: 0.3911 (0.3950)  class_acc: 0.8393 (0.8235)  loss_scale: 8192.0000 (8431.5322)  weight_decay: 0.0500 (0.0500)  time: 1.4574  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.79 GB / 503.51 GB
Epoch: [8]  [180/893]  eta: 0:18:21  lr: 0.002160  min_lr: 0.000003  loss: 0.3911 (0.3946)  class_acc: 0.8393 (0.8248)  loss_scale: 8192.0000 (8418.2983)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.02 GB / 503.51 GB
Epoch: [8]  [190/893]  eta: 0:18:03  lr: 0.002160  min_lr: 0.000003  loss: 0.3513 (0.3929)  class_acc: 0.8393 (0.8258)  loss_scale: 8192.0000 (8406.4503)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0004  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.13 GB / 503.51 GB
Epoch: [8]  [200/893]  eta: 0:17:45  lr: 0.002160  min_lr: 0.000003  loss: 0.3564 (0.3925)  class_acc: 0.8214 (0.8254)  loss_scale: 8192.0000 (8395.7811)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.69 GB / 503.51 GB
Epoch: [8]  [210/893]  eta: 0:17:27  lr: 0.002160  min_lr: 0.000003  loss: 0.3760 (0.3936)  class_acc: 0.8214 (0.8252)  loss_scale: 8192.0000 (8386.1232)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.72 GB / 503.51 GB
Epoch: [8]  [220/893]  eta: 0:17:09  lr: 0.002160  min_lr: 0.000003  loss: 0.3726 (0.3928)  class_acc: 0.8214 (0.8260)  loss_scale: 8192.0000 (8377.3394)  weight_decay: 0.0500 (0.0500)  time: 1.4617  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.15 GB / 503.51 GB
Epoch: [8]  [230/893]  eta: 0:16:53  lr: 0.002159  min_lr: 0.000003  loss: 0.3726 (0.3923)  class_acc: 0.8393 (0.8265)  loss_scale: 8192.0000 (8369.3160)  weight_decay: 0.0500 (0.0500)  time: 1.4730  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.18 GB / 503.51 GB
Epoch: [8]  [240/893]  eta: 0:16:35  lr: 0.002159  min_lr: 0.000003  loss: 0.3555 (0.3910)  class_acc: 0.8393 (0.8270)  loss_scale: 8192.0000 (8361.9585)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.97 GB / 503.51 GB
Epoch: [8]  [250/893]  eta: 0:16:19  lr: 0.002159  min_lr: 0.000003  loss: 0.3528 (0.3902)  class_acc: 0.8393 (0.8271)  loss_scale: 8192.0000 (8355.1873)  weight_decay: 0.0500 (0.0500)  time: 1.4572  data: 0.0003  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.34 GB / 503.51 GB
Epoch: [8]  [260/893]  eta: 0:16:02  lr: 0.002159  min_lr: 0.000003  loss: 0.3357 (0.3887)  class_acc: 0.8393 (0.8280)  loss_scale: 8192.0000 (8348.9349)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0003  max mem: 31081
[2025-03-11 00:11:11,507] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 7402
[2025-03-11 00:11:11,508] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 00:11:11,508] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.83 GB / 503.51 GB
Epoch: [8]  [270/893]  eta: 0:15:45  lr: 0.002159  min_lr: 0.000003  loss: 0.3691 (0.3895)  class_acc: 0.8393 (0.8274)  loss_scale: 8192.0000 (8267.5720)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.40 GB / 503.51 GB
Epoch: [8]  [280/893]  eta: 0:15:29  lr: 0.002158  min_lr: 0.000003  loss: 0.3640 (0.3883)  class_acc: 0.8214 (0.8282)  loss_scale: 4096.0000 (8119.1174)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0004  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.51 GB / 503.51 GB
Epoch: [8]  [290/893]  eta: 0:15:13  lr: 0.002158  min_lr: 0.000003  loss: 0.3467 (0.3877)  class_acc: 0.8393 (0.8290)  loss_scale: 4096.0000 (7980.8660)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0004  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.85 GB / 503.51 GB
Epoch: [8]  [300/893]  eta: 0:14:57  lr: 0.002158  min_lr: 0.000003  loss: 0.3394 (0.3865)  class_acc: 0.8214 (0.8295)  loss_scale: 4096.0000 (7851.8007)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.77 GB / 503.51 GB
Epoch: [8]  [310/893]  eta: 0:14:41  lr: 0.002158  min_lr: 0.000003  loss: 0.3394 (0.3857)  class_acc: 0.8214 (0.8299)  loss_scale: 4096.0000 (7731.0354)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.60 GB / 503.51 GB
Epoch: [8]  [320/893]  eta: 0:14:25  lr: 0.002158  min_lr: 0.000003  loss: 0.3672 (0.3857)  class_acc: 0.8393 (0.8298)  loss_scale: 4096.0000 (7617.7944)  weight_decay: 0.0500 (0.0500)  time: 1.4596  data: 0.0002  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.18 GB / 503.51 GB
Epoch: [8]  [330/893]  eta: 0:14:09  lr: 0.002157  min_lr: 0.000003  loss: 0.3645 (0.3840)  class_acc: 0.8571 (0.8314)  loss_scale: 4096.0000 (7511.3958)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.40 GB / 503.51 GB
Epoch: [8]  [340/893]  eta: 0:13:53  lr: 0.002157  min_lr: 0.000003  loss: 0.3611 (0.3846)  class_acc: 0.8571 (0.8312)  loss_scale: 4096.0000 (7411.2375)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.49 GB / 503.51 GB
Epoch: [8]  [350/893]  eta: 0:13:37  lr: 0.002157  min_lr: 0.000003  loss: 0.3750 (0.3848)  class_acc: 0.8036 (0.8312)  loss_scale: 4096.0000 (7316.7863)  weight_decay: 0.0500 (0.0500)  time: 1.4714  data: 0.0002  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.82 GB / 503.51 GB
Epoch: [8]  [360/893]  eta: 0:13:22  lr: 0.002157  min_lr: 0.000003  loss: 0.3650 (0.3841)  class_acc: 0.8571 (0.8319)  loss_scale: 4096.0000 (7227.5679)  weight_decay: 0.0500 (0.0500)  time: 1.4734  data: 0.0002  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.69 GB / 503.51 GB
Epoch: [8]  [370/893]  eta: 0:13:06  lr: 0.002157  min_lr: 0.000003  loss: 0.3936 (0.3848)  class_acc: 0.8214 (0.8313)  loss_scale: 4096.0000 (7143.1590)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.72 GB / 503.51 GB
Epoch: [8]  [380/893]  eta: 0:12:51  lr: 0.002156  min_lr: 0.000003  loss: 0.4167 (0.3851)  class_acc: 0.8393 (0.8314)  loss_scale: 4096.0000 (7063.1811)  weight_decay: 0.0500 (0.0500)  time: 1.4734  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.58 GB / 503.51 GB
Epoch: [8]  [390/893]  eta: 0:12:35  lr: 0.002156  min_lr: 0.000003  loss: 0.3906 (0.3853)  class_acc: 0.8393 (0.8310)  loss_scale: 4096.0000 (6987.2941)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0004  max mem: 31081
[2025-03-11 00:14:20,903] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 00:14:20,903] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.49 GB / 503.51 GB
Epoch: [8]  [400/893]  eta: 0:12:20  lr: 0.002156  min_lr: 0.000003  loss: 0.3943 (0.3857)  class_acc: 0.8214 (0.8305)  loss_scale: 4096.0000 (6976.4788)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0003  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.33 GB / 503.51 GB
Epoch: [8]  [410/893]  eta: 0:12:04  lr: 0.002156  min_lr: 0.000003  loss: 0.3945 (0.3859)  class_acc: 0.8214 (0.8304)  loss_scale: 8192.0000 (7006.0535)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.15 GB / 503.51 GB
Epoch: [8]  [420/893]  eta: 0:11:49  lr: 0.002156  min_lr: 0.000003  loss: 0.4075 (0.3867)  class_acc: 0.8214 (0.8298)  loss_scale: 8192.0000 (7034.2233)  weight_decay: 0.0500 (0.0500)  time: 1.4649  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.51 GB / 503.51 GB
Epoch: [8]  [430/893]  eta: 0:11:34  lr: 0.002155  min_lr: 0.000003  loss: 0.4043 (0.3872)  class_acc: 0.8036 (0.8294)  loss_scale: 8192.0000 (7061.0858)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0004  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.14 GB / 503.51 GB
Epoch: [8]  [440/893]  eta: 0:11:18  lr: 0.002155  min_lr: 0.000003  loss: 0.3931 (0.3878)  class_acc: 0.8214 (0.8291)  loss_scale: 8192.0000 (7086.7302)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0002  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.17 GB / 503.51 GB
Epoch: [8]  [450/893]  eta: 0:11:03  lr: 0.002155  min_lr: 0.000003  loss: 0.4001 (0.3881)  class_acc: 0.8036 (0.8290)  loss_scale: 8192.0000 (7111.2373)  weight_decay: 0.0500 (0.0500)  time: 1.4579  data: 0.0002  max mem: 31081
[2025-03-11 00:15:44,318] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 7588
[2025-03-11 00:15:44,319] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 00:15:44,319] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.23 GB / 503.51 GB
Epoch: [8]  [460/893]  eta: 0:10:48  lr: 0.002155  min_lr: 0.000003  loss: 0.4065 (0.3886)  class_acc: 0.8214 (0.8289)  loss_scale: 8192.0000 (7054.7158)  weight_decay: 0.0500 (0.0500)  time: 1.4598  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.59 GB / 503.51 GB
Epoch: [8]  [470/893]  eta: 0:10:32  lr: 0.002155  min_lr: 0.000003  loss: 0.4050 (0.3884)  class_acc: 0.8214 (0.8293)  loss_scale: 4096.0000 (6991.8981)  weight_decay: 0.0500 (0.0500)  time: 1.4706  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.67 GB / 503.51 GB
Epoch: [8]  [480/893]  eta: 0:10:17  lr: 0.002154  min_lr: 0.000003  loss: 0.3730 (0.3884)  class_acc: 0.8393 (0.8291)  loss_scale: 4096.0000 (6931.6923)  weight_decay: 0.0500 (0.0500)  time: 1.4770  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.72 GB / 503.51 GB
Epoch: [8]  [490/893]  eta: 0:10:02  lr: 0.002154  min_lr: 0.000003  loss: 0.3918 (0.3885)  class_acc: 0.8214 (0.8288)  loss_scale: 4096.0000 (6873.9389)  weight_decay: 0.0500 (0.0500)  time: 1.4716  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.82 GB / 503.51 GB
Epoch: [8]  [500/893]  eta: 0:09:47  lr: 0.002154  min_lr: 0.000003  loss: 0.3918 (0.3884)  class_acc: 0.8214 (0.8288)  loss_scale: 4096.0000 (6818.4910)  weight_decay: 0.0500 (0.0500)  time: 1.4615  data: 0.0002  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.91 GB / 503.51 GB
Epoch: [8]  [510/893]  eta: 0:09:32  lr: 0.002154  min_lr: 0.000003  loss: 0.3718 (0.3874)  class_acc: 0.8214 (0.8292)  loss_scale: 4096.0000 (6765.2133)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.05 GB / 503.51 GB
Epoch: [8]  [520/893]  eta: 0:09:17  lr: 0.002153  min_lr: 0.000003  loss: 0.3718 (0.3873)  class_acc: 0.8214 (0.8294)  loss_scale: 4096.0000 (6713.9808)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.91 GB / 503.51 GB
Epoch: [8]  [530/893]  eta: 0:09:02  lr: 0.002153  min_lr: 0.000003  loss: 0.3943 (0.3875)  class_acc: 0.8214 (0.8292)  loss_scale: 4096.0000 (6664.6780)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.29 GB / 503.51 GB
Epoch: [8]  [540/893]  eta: 0:08:46  lr: 0.002153  min_lr: 0.000003  loss: 0.3801 (0.3873)  class_acc: 0.8036 (0.8290)  loss_scale: 4096.0000 (6617.1978)  weight_decay: 0.0500 (0.0500)  time: 1.4614  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.14 GB / 503.51 GB
Epoch: [8]  [550/893]  eta: 0:08:31  lr: 0.002153  min_lr: 0.000003  loss: 0.3801 (0.3877)  class_acc: 0.8214 (0.8286)  loss_scale: 4096.0000 (6571.4410)  weight_decay: 0.0500 (0.0500)  time: 1.4742  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.38 GB / 503.51 GB
Epoch: [8]  [560/893]  eta: 0:08:16  lr: 0.002153  min_lr: 0.000003  loss: 0.3838 (0.3875)  class_acc: 0.8214 (0.8287)  loss_scale: 4096.0000 (6527.3155)  weight_decay: 0.0500 (0.0500)  time: 1.4747  data: 0.0004  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.76 GB / 503.51 GB
Epoch: [8]  [570/893]  eta: 0:08:01  lr: 0.002152  min_lr: 0.000003  loss: 0.3552 (0.3869)  class_acc: 0.8393 (0.8289)  loss_scale: 4096.0000 (6484.7356)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.01 GB / 503.51 GB
Epoch: [8]  [580/893]  eta: 0:07:46  lr: 0.002152  min_lr: 0.000003  loss: 0.3755 (0.3868)  class_acc: 0.8393 (0.8288)  loss_scale: 4096.0000 (6443.6213)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0003  max mem: 31081
[2025-03-11 00:18:53,746] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 00:18:53,746] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.97 GB / 503.51 GB
Epoch: [8]  [590/893]  eta: 0:07:31  lr: 0.002152  min_lr: 0.000003  loss: 0.3843 (0.3868)  class_acc: 0.8214 (0.8286)  loss_scale: 4096.0000 (6473.2047)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.12 GB / 503.51 GB
Epoch: [8]  [600/893]  eta: 0:07:16  lr: 0.002152  min_lr: 0.000003  loss: 0.3889 (0.3869)  class_acc: 0.8214 (0.8285)  loss_scale: 8192.0000 (6501.8037)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.01 GB / 503.51 GB
Epoch: [8]  [610/893]  eta: 0:07:01  lr: 0.002152  min_lr: 0.000003  loss: 0.3799 (0.3863)  class_acc: 0.8214 (0.8288)  loss_scale: 8192.0000 (6529.4664)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0002  max mem: 31081
[2025-03-11 00:19:40,695] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 7749
[2025-03-11 00:19:40,695] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 00:19:40,695] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.86 GB / 503.51 GB
Epoch: [8]  [620/893]  eta: 0:06:46  lr: 0.002151  min_lr: 0.000003  loss: 0.3621 (0.3863)  class_acc: 0.8393 (0.8288)  loss_scale: 8192.0000 (6503.4718)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.45 GB / 503.51 GB
Epoch: [8]  [630/893]  eta: 0:06:31  lr: 0.002151  min_lr: 0.000003  loss: 0.3816 (0.3863)  class_acc: 0.8393 (0.8290)  loss_scale: 4096.0000 (6465.3185)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.93 GB / 503.51 GB
Epoch: [8]  [640/893]  eta: 0:06:16  lr: 0.002151  min_lr: 0.000003  loss: 0.3599 (0.3860)  class_acc: 0.8393 (0.8292)  loss_scale: 4096.0000 (6428.3557)  weight_decay: 0.0500 (0.0500)  time: 1.4736  data: 0.0002  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.92 GB / 503.51 GB
Epoch: [8]  [650/893]  eta: 0:06:01  lr: 0.002151  min_lr: 0.000003  loss: 0.3523 (0.3854)  class_acc: 0.8393 (0.8295)  loss_scale: 4096.0000 (6392.5284)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0002  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.17 GB / 503.51 GB
Epoch: [8]  [660/893]  eta: 0:05:46  lr: 0.002150  min_lr: 0.000003  loss: 0.3589 (0.3855)  class_acc: 0.8214 (0.8293)  loss_scale: 4096.0000 (6357.7852)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.11 GB / 503.51 GB
Epoch: [8]  [670/893]  eta: 0:05:31  lr: 0.002150  min_lr: 0.000003  loss: 0.3943 (0.3857)  class_acc: 0.8036 (0.8291)  loss_scale: 4096.0000 (6324.0775)  weight_decay: 0.0500 (0.0500)  time: 1.4608  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.28 GB / 503.51 GB
Epoch: [8]  [680/893]  eta: 0:05:16  lr: 0.002150  min_lr: 0.000003  loss: 0.3948 (0.3860)  class_acc: 0.8036 (0.8289)  loss_scale: 4096.0000 (6291.3598)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.32 GB / 503.51 GB
Epoch: [8]  [690/893]  eta: 0:05:01  lr: 0.002150  min_lr: 0.000003  loss: 0.3906 (0.3859)  class_acc: 0.8214 (0.8291)  loss_scale: 4096.0000 (6259.5890)  weight_decay: 0.0500 (0.0500)  time: 1.4679  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.08 GB / 503.51 GB
Epoch: [8]  [700/893]  eta: 0:04:46  lr: 0.002150  min_lr: 0.000003  loss: 0.3740 (0.3859)  class_acc: 0.8393 (0.8290)  loss_scale: 4096.0000 (6228.7247)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.91 GB / 503.51 GB
Epoch: [8]  [710/893]  eta: 0:04:32  lr: 0.002149  min_lr: 0.000003  loss: 0.3845 (0.3859)  class_acc: 0.8214 (0.8287)  loss_scale: 4096.0000 (6198.7286)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0003  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.19 GB / 503.51 GB
Epoch: [8]  [720/893]  eta: 0:04:17  lr: 0.002149  min_lr: 0.000003  loss: 0.3523 (0.3856)  class_acc: 0.8214 (0.8289)  loss_scale: 4096.0000 (6169.5645)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.15 GB / 503.51 GB
Epoch: [8]  [730/893]  eta: 0:04:02  lr: 0.002149  min_lr: 0.000003  loss: 0.3499 (0.3852)  class_acc: 0.8393 (0.8292)  loss_scale: 4096.0000 (6141.1984)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.82 GB / 503.51 GB
Epoch: [8]  [740/893]  eta: 0:03:47  lr: 0.002149  min_lr: 0.000003  loss: 0.3545 (0.3853)  class_acc: 0.8393 (0.8292)  loss_scale: 4096.0000 (6113.5978)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0003  max mem: 31081
[2025-03-11 00:22:49,904] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 00:22:49,904] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.83 GB / 503.51 GB
Epoch: [8]  [750/893]  eta: 0:03:32  lr: 0.002148  min_lr: 0.000003  loss: 0.3672 (0.3854)  class_acc: 0.8393 (0.8289)  loss_scale: 4096.0000 (6135.8189)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.97 GB / 503.51 GB
Epoch: [8]  [760/893]  eta: 0:03:17  lr: 0.002148  min_lr: 0.000003  loss: 0.4080 (0.3860)  class_acc: 0.8214 (0.8287)  loss_scale: 8192.0000 (6162.8384)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.96 GB / 503.51 GB
Epoch: [8]  [770/893]  eta: 0:03:02  lr: 0.002148  min_lr: 0.000003  loss: 0.4080 (0.3861)  class_acc: 0.8214 (0.8287)  loss_scale: 8192.0000 (6189.1569)  weight_decay: 0.0500 (0.0500)  time: 1.4599  data: 0.0002  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.23 GB / 503.51 GB
Epoch: [8]  [780/893]  eta: 0:02:47  lr: 0.002148  min_lr: 0.000003  loss: 0.3643 (0.3860)  class_acc: 0.8214 (0.8289)  loss_scale: 8192.0000 (6214.8015)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.30 GB / 503.51 GB
Epoch: [8]  [790/893]  eta: 0:02:32  lr: 0.002148  min_lr: 0.000003  loss: 0.3577 (0.3856)  class_acc: 0.8214 (0.8290)  loss_scale: 8192.0000 (6239.7977)  weight_decay: 0.0500 (0.0500)  time: 1.4701  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.14 GB / 503.51 GB
Epoch: [8]  [800/893]  eta: 0:02:18  lr: 0.002147  min_lr: 0.000003  loss: 0.3728 (0.3860)  class_acc: 0.8214 (0.8288)  loss_scale: 8192.0000 (6264.1698)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.52 GB / 503.51 GB
Epoch: [8]  [810/893]  eta: 0:02:03  lr: 0.002147  min_lr: 0.000003  loss: 0.3513 (0.3859)  class_acc: 0.8214 (0.8290)  loss_scale: 8192.0000 (6287.9408)  weight_decay: 0.0500 (0.0500)  time: 1.4595  data: 0.0004  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.29 GB / 503.51 GB
Epoch: [8]  [820/893]  eta: 0:01:48  lr: 0.002147  min_lr: 0.000003  loss: 0.3508 (0.3858)  class_acc: 0.8214 (0.8288)  loss_scale: 8192.0000 (6311.1328)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0004  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.08 GB / 503.51 GB
Epoch: [8]  [830/893]  eta: 0:01:33  lr: 0.002147  min_lr: 0.000003  loss: 0.3787 (0.3857)  class_acc: 0.8214 (0.8288)  loss_scale: 8192.0000 (6333.7665)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.45 GB / 503.51 GB
Epoch: [8]  [840/893]  eta: 0:01:18  lr: 0.002146  min_lr: 0.000003  loss: 0.3682 (0.3856)  class_acc: 0.8214 (0.8288)  loss_scale: 8192.0000 (6355.8621)  weight_decay: 0.0500 (0.0500)  time: 1.4595  data: 0.0004  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.62 GB / 503.51 GB
Epoch: [8]  [850/893]  eta: 0:01:03  lr: 0.002146  min_lr: 0.000003  loss: 0.3730 (0.3858)  class_acc: 0.8214 (0.8286)  loss_scale: 8192.0000 (6377.4383)  weight_decay: 0.0500 (0.0500)  time: 1.4597  data: 0.0004  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.59 GB / 503.51 GB
Epoch: [8]  [860/893]  eta: 0:00:48  lr: 0.002146  min_lr: 0.000003  loss: 0.3730 (0.3859)  class_acc: 0.8214 (0.8287)  loss_scale: 8192.0000 (6398.5134)  weight_decay: 0.0500 (0.0500)  time: 1.4525  data: 0.0003  max mem: 31081
[2025-03-11 00:25:46,740] [INFO] [logging.py:129:log_dist] [Rank 0] step=8000, skipped=44, lr=[2.8025842386127524e-06, 2.8025842386127524e-06, 4.670973731021254e-06, 4.670973731021254e-06, 7.784956218368757e-06, 7.784956218368757e-06, 1.2974927030614595e-05, 1.2974927030614595e-05, 2.162487838435766e-05, 2.162487838435766e-05, 3.604146397392943e-05, 3.604146397392943e-05, 6.0069106623215726e-05, 6.0069106623215726e-05, 0.00010011517770535954, 0.00010011517770535954, 0.00016685862950893256, 0.00016685862950893256, 0.000278097715848221, 0.000278097715848221, 0.0004634961930803683, 0.0004634961930803683, 0.0007724936551339473, 0.0007724936551339473, 0.0012874894252232455, 0.0012874894252232455, 0.002145815708705409, 0.002145815708705409], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 00:25:46,740] [INFO] [timer.py:264:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=61.06134213932395, CurrSamplesPerSec=59.03958347923651, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
[2025-03-11 00:25:56,867] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 00:25:56,868] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.84 GB / 503.51 GB
Epoch: [8]  [870/893]  eta: 0:00:34  lr: 0.002146  min_lr: 0.000003  loss: 0.3325 (0.3856)  class_acc: 0.8571 (0.8291)  loss_scale: 8192.0000 (6428.5098)  weight_decay: 0.0500 (0.0500)  time: 1.4453  data: 0.0001  max mem: 31081
[2025-03-11 00:26:04,007] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 8011
[2025-03-11 00:26:04,008] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 00:26:04,008] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.56 GB / 503.51 GB
Epoch: [8]  [880/893]  eta: 0:00:19  lr: 0.002145  min_lr: 0.000003  loss: 0.3420 (0.3856)  class_acc: 0.8393 (0.8290)  loss_scale: 8192.0000 (6485.7208)  weight_decay: 0.0500 (0.0500)  time: 1.4395  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.08 GB / 503.51 GB
Epoch: [8]  [890/893]  eta: 0:00:04  lr: 0.002145  min_lr: 0.000003  loss: 0.3562 (0.3853)  class_acc: 0.8214 (0.8291)  loss_scale: 8192.0000 (6504.8709)  weight_decay: 0.0500 (0.0500)  time: 1.4395  data: 0.0002  max mem: 31081
Epoch: [8]  [892/893]  eta: 0:00:01  lr: 0.002145  min_lr: 0.000003  loss: 0.3840 (0.3853)  class_acc: 0.8214 (0.8291)  loss_scale: 8192.0000 (6506.7623)  weight_decay: 0.0500 (0.0500)  time: 1.3900  data: 0.0002  max mem: 31081
Epoch: [8] Total time: 0:22:01 (1.4801 s / it)
Averaged stats: lr: 0.002145  min_lr: 0.000003  loss: 0.3840 (0.3853)  class_acc: 0.8214 (0.8291)  loss_scale: 8192.0000 (6506.7623)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:16:03  loss: 0.4319 (0.4319)  acc: 83.3333 (83.3333)  time: 11.2129  data: 10.6729  max mem: 31081
Val:  [ 10/728]  eta: 0:18:31  loss: 0.4319 (0.4354)  acc: 83.3333 (81.9264)  time: 1.5476  data: 1.0203  max mem: 31081
Val:  [ 20/728]  eta: 0:14:11  loss: 0.4163 (0.4372)  acc: 83.3333 (81.2925)  time: 0.7026  data: 0.1777  max mem: 31081
Val:  [ 30/728]  eta: 0:12:39  loss: 0.4163 (0.4421)  acc: 80.9524 (80.2227)  time: 0.8354  data: 0.3136  max mem: 31081
Val:  [ 40/728]  eta: 0:11:52  loss: 0.4846 (0.4769)  acc: 76.1905 (78.6585)  time: 0.8587  data: 0.3379  max mem: 31081
Val:  [ 50/728]  eta: 0:11:16  loss: 0.4016 (0.4543)  acc: 80.9524 (79.6219)  time: 0.8571  data: 0.3359  max mem: 31081
Val:  [ 60/728]  eta: 0:10:19  loss: 0.4105 (0.4639)  acc: 79.7619 (79.5472)  time: 0.7061  data: 0.1834  max mem: 31081
Val:  [ 70/728]  eta: 0:09:51  loss: 0.4543 (0.4689)  acc: 79.7619 (79.1080)  time: 0.6474  data: 0.1239  max mem: 31081
Val:  [ 80/728]  eta: 0:09:40  loss: 0.4320 (0.4732)  acc: 77.3810 (78.7919)  time: 0.8024  data: 0.2807  max mem: 31081
Val:  [ 90/728]  eta: 0:09:24  loss: 0.4389 (0.4781)  acc: 75.0000 (78.3360)  time: 0.8349  data: 0.3106  max mem: 31081
Val:  [100/728]  eta: 0:09:12  loss: 0.4267 (0.4700)  acc: 82.1429 (78.8425)  time: 0.8101  data: 0.2875  max mem: 31081
Val:  [110/728]  eta: 0:09:03  loss: 0.3724 (0.4665)  acc: 83.3333 (79.0326)  time: 0.8553  data: 0.3367  max mem: 31081
Val:  [120/728]  eta: 0:08:46  loss: 0.3753 (0.4604)  acc: 83.3333 (79.4667)  time: 0.7953  data: 0.2738  max mem: 31081
Val:  [130/728]  eta: 0:08:27  loss: 0.3885 (0.4674)  acc: 83.3333 (79.1258)  time: 0.6772  data: 0.1538  max mem: 31081
Val:  [140/728]  eta: 0:08:16  loss: 0.5122 (0.4741)  acc: 73.8095 (78.6390)  time: 0.7116  data: 0.1889  max mem: 31081
Val:  [150/728]  eta: 0:08:00  loss: 0.5122 (0.4738)  acc: 73.8095 (78.5478)  time: 0.7231  data: 0.2000  max mem: 31081
Val:  [160/728]  eta: 0:07:55  loss: 0.4276 (0.4778)  acc: 78.5714 (78.4605)  time: 0.7926  data: 0.2673  max mem: 31081
Val:  [170/728]  eta: 0:07:44  loss: 0.4743 (0.4795)  acc: 78.5714 (78.2999)  time: 0.8411  data: 0.3173  max mem: 31081
Val:  [180/728]  eta: 0:07:32  loss: 0.4457 (0.4769)  acc: 77.3810 (78.3873)  time: 0.7308  data: 0.2085  max mem: 31081
Val:  [190/728]  eta: 0:07:20  loss: 0.4843 (0.4790)  acc: 76.1905 (78.3096)  time: 0.6927  data: 0.1704  max mem: 31081
Val:  [200/728]  eta: 0:07:11  loss: 0.4383 (0.4756)  acc: 80.9524 (78.5063)  time: 0.7499  data: 0.2258  max mem: 31081
Val:  [210/728]  eta: 0:07:05  loss: 0.4196 (0.4764)  acc: 80.9524 (78.4417)  time: 0.8596  data: 0.3339  max mem: 31081
Val:  [220/728]  eta: 0:06:58  loss: 0.3973 (0.4735)  acc: 80.9524 (78.5553)  time: 0.8848  data: 0.3630  max mem: 31081
Val:  [230/728]  eta: 0:06:49  loss: 0.3846 (0.4777)  acc: 82.1429 (78.4529)  time: 0.8178  data: 0.2941  max mem: 31081
Val:  [240/728]  eta: 0:06:37  loss: 0.3740 (0.4746)  acc: 82.1429 (78.6356)  time: 0.7166  data: 0.1913  max mem: 31081
Val:  [250/728]  eta: 0:06:26  loss: 0.4022 (0.4733)  acc: 80.9524 (78.7137)  time: 0.6619  data: 0.1401  max mem: 31081
Val:  [260/728]  eta: 0:06:19  loss: 0.4627 (0.4738)  acc: 78.5714 (78.7812)  time: 0.7728  data: 0.2515  max mem: 31081
Val:  [270/728]  eta: 0:06:11  loss: 0.4627 (0.4746)  acc: 79.7619 (78.7779)  time: 0.8486  data: 0.3268  max mem: 31081
Val:  [280/728]  eta: 0:06:07  loss: 0.4590 (0.4757)  acc: 79.7619 (78.8172)  time: 0.9175  data: 0.3944  max mem: 31081
Val:  [290/728]  eta: 0:05:58  loss: 0.4200 (0.4745)  acc: 79.7619 (78.8292)  time: 0.9155  data: 0.3913  max mem: 31081
Val:  [300/728]  eta: 0:05:46  loss: 0.4020 (0.4736)  acc: 80.9524 (78.8957)  time: 0.6681  data: 0.1446  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.4214 (0.4752)  acc: 80.9524 (78.8279)  time: 0.6775  data: 0.1533  max mem: 31081
Val:  [320/728]  eta: 0:05:30  loss: 0.4214 (0.4748)  acc: 80.9524 (78.7532)  time: 0.8083  data: 0.2848  max mem: 31081
Val:  [330/728]  eta: 0:05:23  loss: 0.3809 (0.4728)  acc: 80.9524 (78.8520)  time: 0.8466  data: 0.3225  max mem: 31081
Val:  [340/728]  eta: 0:05:16  loss: 0.3809 (0.4699)  acc: 80.9524 (78.9973)  time: 0.9105  data: 0.3876  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3760 (0.4696)  acc: 82.1429 (79.0327)  time: 0.8530  data: 0.3297  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.4072 (0.4681)  acc: 82.1429 (79.1254)  time: 0.6566  data: 0.1331  max mem: 31081
Val:  [370/728]  eta: 0:04:49  loss: 0.4438 (0.4712)  acc: 78.5714 (78.9886)  time: 0.7023  data: 0.1821  max mem: 31081
Val:  [380/728]  eta: 0:04:41  loss: 0.4717 (0.4729)  acc: 76.1905 (78.9026)  time: 0.8604  data: 0.3403  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.3879 (0.4692)  acc: 80.9524 (79.0525)  time: 0.8438  data: 0.3206  max mem: 31081
Val:  [400/728]  eta: 0:04:26  loss: 0.3281 (0.4675)  acc: 83.3333 (79.1236)  time: 0.8830  data: 0.3563  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.3256 (0.4654)  acc: 83.3333 (79.2145)  time: 0.8808  data: 0.3556  max mem: 31081
Val:  [420/728]  eta: 0:04:08  loss: 0.5170 (0.4669)  acc: 77.3810 (79.1850)  time: 0.6841  data: 0.1616  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4955 (0.4671)  acc: 76.1905 (79.1487)  time: 0.6939  data: 0.1718  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.3922 (0.4663)  acc: 80.9524 (79.2058)  time: 0.8371  data: 0.3142  max mem: 31081
Val:  [450/728]  eta: 0:03:45  loss: 0.4351 (0.4674)  acc: 78.5714 (79.1231)  time: 0.8374  data: 0.3146  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.4720 (0.4689)  acc: 76.1905 (79.0595)  time: 0.8379  data: 0.3168  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.4230 (0.4675)  acc: 80.9524 (79.1452)  time: 0.7916  data: 0.2687  max mem: 31081
Val:  [480/728]  eta: 0:03:19  loss: 0.3962 (0.4664)  acc: 83.3333 (79.1778)  time: 0.6502  data: 0.1241  max mem: 31081
Val:  [490/728]  eta: 0:03:11  loss: 0.3962 (0.4656)  acc: 82.1429 (79.2164)  time: 0.6967  data: 0.1709  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.4257 (0.4646)  acc: 79.7619 (79.2439)  time: 0.8517  data: 0.3264  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3842 (0.4634)  acc: 83.3333 (79.3006)  time: 0.8847  data: 0.3615  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4785 (0.4659)  acc: 75.0000 (79.1747)  time: 0.8624  data: 0.3417  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.4884 (0.4659)  acc: 76.1905 (79.2014)  time: 0.7463  data: 0.2223  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.4663 (0.4678)  acc: 77.3810 (79.1040)  time: 0.6124  data: 0.0868  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.4850 (0.4689)  acc: 73.8095 (79.0079)  time: 0.6710  data: 0.1469  max mem: 31081
Val:  [560/728]  eta: 0:02:14  loss: 0.4710 (0.4684)  acc: 78.5714 (79.0510)  time: 0.7596  data: 0.2364  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4507 (0.4700)  acc: 79.7619 (79.0030)  time: 0.8130  data: 0.2880  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4645 (0.4707)  acc: 76.1905 (78.9403)  time: 0.8511  data: 0.3249  max mem: 31081
Val:  [590/728]  eta: 0:01:50  loss: 0.4900 (0.4708)  acc: 76.1905 (78.8937)  time: 0.7337  data: 0.2106  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4098 (0.4708)  acc: 77.3810 (78.8765)  time: 0.6441  data: 0.1203  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4104 (0.4725)  acc: 77.3810 (78.7702)  time: 0.6717  data: 0.1513  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4702 (0.4745)  acc: 78.5714 (78.7210)  time: 0.8015  data: 0.2824  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4836 (0.4741)  acc: 78.5714 (78.7261)  time: 0.8390  data: 0.3173  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.4558 (0.4734)  acc: 78.5714 (78.7497)  time: 0.8023  data: 0.2818  max mem: 31081
Val:  [650/728]  eta: 0:01:02  loss: 0.4078 (0.4724)  acc: 80.9524 (78.8000)  time: 0.8055  data: 0.2830  max mem: 31081
Val:  [660/728]  eta: 0:00:54  loss: 0.4275 (0.4731)  acc: 80.9524 (78.7623)  time: 0.7683  data: 0.2431  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4424 (0.4743)  acc: 77.3810 (78.7009)  time: 0.6786  data: 0.1549  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4334 (0.4738)  acc: 77.3810 (78.7148)  time: 0.6914  data: 0.1708  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3959 (0.4732)  acc: 80.9524 (78.7506)  time: 0.8277  data: 0.3069  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4042 (0.4729)  acc: 80.9524 (78.7514)  time: 0.8550  data: 0.3334  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.3768 (0.4731)  acc: 82.1429 (78.7271)  time: 0.7986  data: 0.2783  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4522 (0.4734)  acc: 79.7619 (78.7299)  time: 0.7698  data: 0.2575  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4323 (0.4737)  acc: 79.7619 (78.7547)  time: 0.7463  data: 0.2575  max mem: 31081
Val: Total time: 0:09:34 (0.7897 s / it)
* Acc@1 78.755 AP 0.8066076636314392 loss 0.474
Accuracy of the network on the 61096 val videos: 78.8%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.32 GB / 503.51 GB
Epoch: [9]  [  0/893]  eta: 3:26:22  lr: 0.002145  min_lr: 0.000003  loss: 0.3911 (0.3911)  class_acc: 0.8214 (0.8214)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 13.8663  data: 12.6214  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.84 GB / 503.51 GB
Epoch: [9]  [ 10/893]  eta: 0:38:35  lr: 0.002145  min_lr: 0.000003  loss: 0.4485 (0.4501)  class_acc: 0.8214 (0.8101)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6227  data: 1.1480  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.76 GB / 503.51 GB
Epoch: [9]  [ 20/893]  eta: 0:30:19  lr: 0.002145  min_lr: 0.000003  loss: 0.4001 (0.4189)  class_acc: 0.8214 (0.8112)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4946  data: 0.0006  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.91 GB / 503.51 GB
Epoch: [9]  [ 30/893]  eta: 0:27:17  lr: 0.002144  min_lr: 0.000003  loss: 0.3513 (0.3966)  class_acc: 0.8214 (0.8214)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4983  data: 0.0005  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.18 GB / 503.51 GB
Epoch: [9]  [ 40/893]  eta: 0:25:39  lr: 0.002144  min_lr: 0.000003  loss: 0.3599 (0.4065)  class_acc: 0.8036 (0.8166)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5120  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.80 GB / 503.51 GB
Epoch: [9]  [ 50/893]  eta: 0:24:31  lr: 0.002144  min_lr: 0.000003  loss: 0.3811 (0.4007)  class_acc: 0.8214 (0.8207)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5101  data: 0.0005  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.00 GB / 503.51 GB
Epoch: [9]  [ 60/893]  eta: 0:23:37  lr: 0.002144  min_lr: 0.000003  loss: 0.3665 (0.3944)  class_acc: 0.8393 (0.8249)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4900  data: 0.0004  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.12 GB / 503.51 GB
Epoch: [9]  [ 70/893]  eta: 0:22:52  lr: 0.002143  min_lr: 0.000003  loss: 0.3689 (0.3946)  class_acc: 0.8393 (0.8237)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.24 GB / 503.51 GB
Epoch: [9]  [ 80/893]  eta: 0:22:16  lr: 0.002143  min_lr: 0.000003  loss: 0.3823 (0.3925)  class_acc: 0.8214 (0.8243)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.61 GB / 503.51 GB
Epoch: [9]  [ 90/893]  eta: 0:21:43  lr: 0.002143  min_lr: 0.000003  loss: 0.3911 (0.3916)  class_acc: 0.8214 (0.8246)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.03 GB / 503.51 GB
Epoch: [9]  [100/893]  eta: 0:21:15  lr: 0.002143  min_lr: 0.000003  loss: 0.3660 (0.3894)  class_acc: 0.8214 (0.8257)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.32 GB / 503.51 GB
Epoch: [9]  [110/893]  eta: 0:20:48  lr: 0.002143  min_lr: 0.000003  loss: 0.3594 (0.3878)  class_acc: 0.8393 (0.8275)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4679  data: 0.0003  max mem: 31081
[2025-03-11 00:39:03,846] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 00:39:03,846] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.92 GB / 503.51 GB
Epoch: [9]  [120/893]  eta: 0:20:24  lr: 0.002142  min_lr: 0.000003  loss: 0.3599 (0.3904)  class_acc: 0.8214 (0.8262)  loss_scale: 8192.0000 (8801.3223)  weight_decay: 0.0500 (0.0500)  time: 1.4673  data: 0.0003  max mem: 31081
[2025-03-11 00:39:23,136] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 8153
[2025-03-11 00:39:23,137] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 00:39:23,137] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.08 GB / 503.51 GB
Epoch: [9]  [130/893]  eta: 0:20:03  lr: 0.002142  min_lr: 0.000003  loss: 0.3691 (0.3881)  class_acc: 0.8393 (0.8280)  loss_scale: 16384.0000 (9004.9466)  weight_decay: 0.0500 (0.0500)  time: 1.4772  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.76 GB / 503.51 GB
Epoch: [9]  [140/893]  eta: 0:19:41  lr: 0.002142  min_lr: 0.000003  loss: 0.3726 (0.3879)  class_acc: 0.8571 (0.8285)  loss_scale: 8192.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  time: 1.4737  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.64 GB / 503.51 GB
Epoch: [9]  [150/893]  eta: 0:19:20  lr: 0.002142  min_lr: 0.000003  loss: 0.3784 (0.3897)  class_acc: 0.8036 (0.8265)  loss_scale: 8192.0000 (8897.2715)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0004  max mem: 31081
[2025-03-11 00:40:04,243] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 8181
[2025-03-11 00:40:04,243] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 00:40:04,243] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.69 GB / 503.51 GB
Epoch: [9]  [160/893]  eta: 0:19:01  lr: 0.002141  min_lr: 0.000003  loss: 0.3784 (0.3884)  class_acc: 0.8036 (0.8269)  loss_scale: 8192.0000 (8649.9379)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0004  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.80 GB / 503.51 GB
Epoch: [9]  [170/893]  eta: 0:18:42  lr: 0.002141  min_lr: 0.000003  loss: 0.3804 (0.3877)  class_acc: 0.8214 (0.8265)  loss_scale: 4096.0000 (8383.6257)  weight_decay: 0.0500 (0.0500)  time: 1.4731  data: 0.0004  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.63 GB / 503.51 GB
Epoch: [9]  [180/893]  eta: 0:18:23  lr: 0.002141  min_lr: 0.000003  loss: 0.3777 (0.3866)  class_acc: 0.8214 (0.8260)  loss_scale: 4096.0000 (8146.7403)  weight_decay: 0.0500 (0.0500)  time: 1.4734  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.60 GB / 503.51 GB
Epoch: [9]  [190/893]  eta: 0:18:04  lr: 0.002141  min_lr: 0.000003  loss: 0.3904 (0.3896)  class_acc: 0.8214 (0.8239)  loss_scale: 4096.0000 (7934.6597)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0002  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.67 GB / 503.51 GB
Epoch: [9]  [200/893]  eta: 0:17:46  lr: 0.002140  min_lr: 0.000003  loss: 0.3804 (0.3872)  class_acc: 0.8214 (0.8249)  loss_scale: 4096.0000 (7743.6816)  weight_decay: 0.0500 (0.0500)  time: 1.4554  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.01 GB / 503.51 GB
Epoch: [9]  [210/893]  eta: 0:17:28  lr: 0.002140  min_lr: 0.000003  loss: 0.3547 (0.3879)  class_acc: 0.8214 (0.8252)  loss_scale: 4096.0000 (7570.8057)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.59 GB / 503.51 GB
Epoch: [9]  [220/893]  eta: 0:17:10  lr: 0.002140  min_lr: 0.000003  loss: 0.3162 (0.3851)  class_acc: 0.8393 (0.8272)  loss_scale: 4096.0000 (7413.5747)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.68 GB / 503.51 GB
Epoch: [9]  [230/893]  eta: 0:16:52  lr: 0.002140  min_lr: 0.000003  loss: 0.3286 (0.3834)  class_acc: 0.8393 (0.8278)  loss_scale: 4096.0000 (7269.9567)  weight_decay: 0.0500 (0.0500)  time: 1.4555  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.23 GB / 503.51 GB
Epoch: [9]  [240/893]  eta: 0:16:35  lr: 0.002139  min_lr: 0.000003  loss: 0.3496 (0.3827)  class_acc: 0.8393 (0.8279)  loss_scale: 4096.0000 (7138.2573)  weight_decay: 0.0500 (0.0500)  time: 1.4568  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.48 GB / 503.51 GB
Epoch: [9]  [250/893]  eta: 0:16:18  lr: 0.002139  min_lr: 0.000003  loss: 0.3503 (0.3817)  class_acc: 0.8393 (0.8279)  loss_scale: 4096.0000 (7017.0518)  weight_decay: 0.0500 (0.0500)  time: 1.4573  data: 0.0002  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.16 GB / 503.51 GB
Epoch: [9]  [260/893]  eta: 0:16:02  lr: 0.002139  min_lr: 0.000003  loss: 0.3945 (0.3840)  class_acc: 0.8214 (0.8266)  loss_scale: 4096.0000 (6905.1341)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0002  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.30 GB / 503.51 GB
Epoch: [9]  [270/893]  eta: 0:15:45  lr: 0.002139  min_lr: 0.000003  loss: 0.3992 (0.3839)  class_acc: 0.8214 (0.8264)  loss_scale: 4096.0000 (6801.4760)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.07 GB / 503.51 GB
Epoch: [9]  [280/893]  eta: 0:15:29  lr: 0.002138  min_lr: 0.000003  loss: 0.3740 (0.3842)  class_acc: 0.8214 (0.8265)  loss_scale: 4096.0000 (6705.1957)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0003  max mem: 31081
[2025-03-11 00:43:12,875] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 00:43:12,875] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.21 GB / 503.51 GB
Epoch: [9]  [290/893]  eta: 0:15:13  lr: 0.002138  min_lr: 0.000003  loss: 0.3704 (0.3837)  class_acc: 0.8214 (0.8268)  loss_scale: 4096.0000 (6742.2131)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.03 GB / 503.51 GB
Epoch: [9]  [300/893]  eta: 0:14:57  lr: 0.002138  min_lr: 0.000003  loss: 0.3569 (0.3823)  class_acc: 0.8393 (0.8277)  loss_scale: 8192.0000 (6790.3787)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0002  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.88 GB / 503.51 GB
Epoch: [9]  [310/893]  eta: 0:14:41  lr: 0.002138  min_lr: 0.000003  loss: 0.3335 (0.3817)  class_acc: 0.8393 (0.8277)  loss_scale: 8192.0000 (6835.4469)  weight_decay: 0.0500 (0.0500)  time: 1.4673  data: 0.0002  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.65 GB / 503.51 GB
Epoch: [9]  [320/893]  eta: 0:14:25  lr: 0.002137  min_lr: 0.000003  loss: 0.3188 (0.3803)  class_acc: 0.8571 (0.8289)  loss_scale: 8192.0000 (6877.7072)  weight_decay: 0.0500 (0.0500)  time: 1.4603  data: 0.0002  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.22 GB / 503.51 GB
Epoch: [9]  [330/893]  eta: 0:14:09  lr: 0.002137  min_lr: 0.000003  loss: 0.3188 (0.3798)  class_acc: 0.8571 (0.8293)  loss_scale: 8192.0000 (6917.4139)  weight_decay: 0.0500 (0.0500)  time: 1.4742  data: 0.0002  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.01 GB / 503.51 GB
Epoch: [9]  [340/893]  eta: 0:13:53  lr: 0.002137  min_lr: 0.000003  loss: 0.3386 (0.3786)  class_acc: 0.8571 (0.8302)  loss_scale: 8192.0000 (6954.7918)  weight_decay: 0.0500 (0.0500)  time: 1.4726  data: 0.0002  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.39 GB / 503.51 GB
Epoch: [9]  [350/893]  eta: 0:13:37  lr: 0.002136  min_lr: 0.000003  loss: 0.3511 (0.3793)  class_acc: 0.8393 (0.8299)  loss_scale: 8192.0000 (6990.0399)  weight_decay: 0.0500 (0.0500)  time: 1.4580  data: 0.0003  max mem: 31081
[2025-03-11 00:44:57,006] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 8381
[2025-03-11 00:44:57,007] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 00:44:57,007] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.84 GB / 503.51 GB
Epoch: [9]  [360/893]  eta: 0:13:22  lr: 0.002136  min_lr: 0.000003  loss: 0.3713 (0.3791)  class_acc: 0.8214 (0.8297)  loss_scale: 8192.0000 (6932.5651)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0002  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.20 GB / 503.51 GB
Epoch: [9]  [370/893]  eta: 0:13:06  lr: 0.002136  min_lr: 0.000003  loss: 0.3562 (0.3785)  class_acc: 0.8393 (0.8300)  loss_scale: 4096.0000 (6856.1078)  weight_decay: 0.0500 (0.0500)  time: 1.4731  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.09 GB / 503.51 GB
Epoch: [9]  [380/893]  eta: 0:12:51  lr: 0.002136  min_lr: 0.000003  loss: 0.3669 (0.3781)  class_acc: 0.8571 (0.8306)  loss_scale: 4096.0000 (6783.6640)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.33 GB / 503.51 GB
Epoch: [9]  [390/893]  eta: 0:12:35  lr: 0.002135  min_lr: 0.000003  loss: 0.3640 (0.3778)  class_acc: 0.8214 (0.8310)  loss_scale: 4096.0000 (6714.9258)  weight_decay: 0.0500 (0.0500)  time: 1.4746  data: 0.0002  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.34 GB / 503.51 GB
Epoch: [9]  [400/893]  eta: 0:12:20  lr: 0.002135  min_lr: 0.000003  loss: 0.3811 (0.3790)  class_acc: 0.8214 (0.8306)  loss_scale: 4096.0000 (6649.6160)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0003  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.54 GB / 503.51 GB
Epoch: [9]  [410/893]  eta: 0:12:04  lr: 0.002135  min_lr: 0.000003  loss: 0.4045 (0.3792)  class_acc: 0.8214 (0.8301)  loss_scale: 4096.0000 (6587.4842)  weight_decay: 0.0500 (0.0500)  time: 1.4597  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.15 GB / 503.51 GB
Epoch: [9]  [420/893]  eta: 0:11:49  lr: 0.002135  min_lr: 0.000003  loss: 0.4060 (0.3799)  class_acc: 0.8036 (0.8296)  loss_scale: 4096.0000 (6528.3040)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.41 GB / 503.51 GB
Epoch: [9]  [430/893]  eta: 0:11:34  lr: 0.002134  min_lr: 0.000003  loss: 0.3867 (0.3799)  class_acc: 0.8214 (0.8297)  loss_scale: 4096.0000 (6471.8701)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0003  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.10 GB / 503.51 GB
Epoch: [9]  [440/893]  eta: 0:11:18  lr: 0.002134  min_lr: 0.000003  loss: 0.3662 (0.3798)  class_acc: 0.8393 (0.8301)  loss_scale: 4096.0000 (6417.9955)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.31 GB / 503.51 GB
Epoch: [9]  [450/893]  eta: 0:11:03  lr: 0.002134  min_lr: 0.000003  loss: 0.3491 (0.3798)  class_acc: 0.8571 (0.8299)  loss_scale: 4096.0000 (6366.5100)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0002  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.84 GB / 503.51 GB
Epoch: [9]  [460/893]  eta: 0:10:48  lr: 0.002134  min_lr: 0.000003  loss: 0.3882 (0.3803)  class_acc: 0.8393 (0.8297)  loss_scale: 4096.0000 (6317.2581)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.0002  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.79 GB / 503.51 GB
Epoch: [9]  [470/893]  eta: 0:10:32  lr: 0.002133  min_lr: 0.000003  loss: 0.3699 (0.3799)  class_acc: 0.8393 (0.8299)  loss_scale: 4096.0000 (6270.0977)  weight_decay: 0.0500 (0.0500)  time: 1.4669  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.14 GB / 503.51 GB
Epoch: [9]  [480/893]  eta: 0:10:17  lr: 0.002133  min_lr: 0.000003  loss: 0.3450 (0.3794)  class_acc: 0.8393 (0.8303)  loss_scale: 4096.0000 (6224.8981)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
[2025-03-11 00:48:06,155] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 00:48:06,155] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.57 GB / 503.51 GB
Epoch: [9]  [490/893]  eta: 0:10:02  lr: 0.002133  min_lr: 0.000003  loss: 0.3474 (0.3789)  class_acc: 0.8571 (0.8306)  loss_scale: 4096.0000 (6256.6191)  weight_decay: 0.0500 (0.0500)  time: 1.4608  data: 0.0002  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.40 GB / 503.51 GB
Epoch: [9]  [500/893]  eta: 0:09:47  lr: 0.002133  min_lr: 0.000003  loss: 0.3513 (0.3784)  class_acc: 0.8393 (0.8311)  loss_scale: 8192.0000 (6295.2495)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.56 GB / 503.51 GB
Epoch: [9]  [510/893]  eta: 0:09:32  lr: 0.002132  min_lr: 0.000003  loss: 0.3481 (0.3784)  class_acc: 0.8571 (0.8311)  loss_scale: 8192.0000 (6332.3679)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.70 GB / 503.51 GB
Epoch: [9]  [520/893]  eta: 0:09:16  lr: 0.002132  min_lr: 0.000003  loss: 0.3389 (0.3784)  class_acc: 0.8393 (0.8312)  loss_scale: 8192.0000 (6368.0614)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0002  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.35 GB / 503.51 GB
Epoch: [9]  [530/893]  eta: 0:09:01  lr: 0.002132  min_lr: 0.000003  loss: 0.3767 (0.3784)  class_acc: 0.8214 (0.8312)  loss_scale: 8192.0000 (6402.4105)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.62 GB / 503.51 GB
Epoch: [9]  [540/893]  eta: 0:08:46  lr: 0.002131  min_lr: 0.000003  loss: 0.3591 (0.3782)  class_acc: 0.8393 (0.8314)  loss_scale: 8192.0000 (6435.4898)  weight_decay: 0.0500 (0.0500)  time: 1.4597  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.69 GB / 503.51 GB
Epoch: [9]  [550/893]  eta: 0:08:31  lr: 0.002131  min_lr: 0.000003  loss: 0.3809 (0.3786)  class_acc: 0.8214 (0.8311)  loss_scale: 8192.0000 (6467.3684)  weight_decay: 0.0500 (0.0500)  time: 1.4580  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.51 GB / 503.51 GB
Epoch: [9]  [560/893]  eta: 0:08:16  lr: 0.002131  min_lr: 0.000003  loss: 0.3569 (0.3772)  class_acc: 0.8214 (0.8317)  loss_scale: 8192.0000 (6498.1105)  weight_decay: 0.0500 (0.0500)  time: 1.4587  data: 0.0002  max mem: 31081
[2025-03-11 00:50:04,501] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 8591
[2025-03-11 00:50:04,502] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 00:50:04,502] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.91 GB / 503.51 GB
Epoch: [9]  [570/893]  eta: 0:08:01  lr: 0.002131  min_lr: 0.000003  loss: 0.3506 (0.3773)  class_acc: 0.8393 (0.8317)  loss_scale: 8192.0000 (6470.3888)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.95 GB / 503.51 GB
Epoch: [9]  [580/893]  eta: 0:07:46  lr: 0.002130  min_lr: 0.000003  loss: 0.3506 (0.3767)  class_acc: 0.8393 (0.8319)  loss_scale: 4096.0000 (6429.5215)  weight_decay: 0.0500 (0.0500)  time: 1.4581  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.27 GB / 503.51 GB
Epoch: [9]  [590/893]  eta: 0:07:31  lr: 0.002130  min_lr: 0.000003  loss: 0.3596 (0.3768)  class_acc: 0.8214 (0.8316)  loss_scale: 4096.0000 (6390.0372)  weight_decay: 0.0500 (0.0500)  time: 1.4584  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.90 GB / 503.51 GB
Epoch: [9]  [600/893]  eta: 0:07:16  lr: 0.002130  min_lr: 0.000003  loss: 0.3655 (0.3763)  class_acc: 0.8214 (0.8319)  loss_scale: 4096.0000 (6351.8669)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.22 GB / 503.51 GB
Epoch: [9]  [610/893]  eta: 0:07:01  lr: 0.002130  min_lr: 0.000003  loss: 0.3706 (0.3773)  class_acc: 0.8214 (0.8316)  loss_scale: 4096.0000 (6314.9460)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.75 GB / 503.51 GB
Epoch: [9]  [620/893]  eta: 0:06:46  lr: 0.002129  min_lr: 0.000003  loss: 0.4282 (0.3777)  class_acc: 0.8036 (0.8314)  loss_scale: 4096.0000 (6279.2142)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.77 GB / 503.51 GB
Epoch: [9]  [630/893]  eta: 0:06:31  lr: 0.002129  min_lr: 0.000003  loss: 0.3623 (0.3771)  class_acc: 0.8214 (0.8318)  loss_scale: 4096.0000 (6244.6149)  weight_decay: 0.0500 (0.0500)  time: 1.4590  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.86 GB / 503.51 GB
Epoch: [9]  [640/893]  eta: 0:06:16  lr: 0.002129  min_lr: 0.000003  loss: 0.3418 (0.3768)  class_acc: 0.8393 (0.8319)  loss_scale: 4096.0000 (6211.0952)  weight_decay: 0.0500 (0.0500)  time: 1.4594  data: 0.0002  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.80 GB / 503.51 GB
Epoch: [9]  [650/893]  eta: 0:06:01  lr: 0.002128  min_lr: 0.000003  loss: 0.3435 (0.3766)  class_acc: 0.8393 (0.8321)  loss_scale: 4096.0000 (6178.6052)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0002  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.93 GB / 503.51 GB
Epoch: [9]  [660/893]  eta: 0:05:46  lr: 0.002128  min_lr: 0.000003  loss: 0.3687 (0.3770)  class_acc: 0.8393 (0.8318)  loss_scale: 4096.0000 (6147.0983)  weight_decay: 0.0500 (0.0500)  time: 1.4730  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.61 GB / 503.51 GB
Epoch: [9]  [670/893]  eta: 0:05:31  lr: 0.002128  min_lr: 0.000003  loss: 0.3425 (0.3766)  class_acc: 0.8214 (0.8320)  loss_scale: 4096.0000 (6116.5306)  weight_decay: 0.0500 (0.0500)  time: 1.4724  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.63 GB / 503.51 GB
Epoch: [9]  [680/893]  eta: 0:05:16  lr: 0.002128  min_lr: 0.000003  loss: 0.3911 (0.3774)  class_acc: 0.8214 (0.8315)  loss_scale: 4096.0000 (6086.8605)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0004  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.24 GB / 503.51 GB
Epoch: [9]  [690/893]  eta: 0:05:01  lr: 0.002127  min_lr: 0.000003  loss: 0.3936 (0.3773)  class_acc: 0.8036 (0.8314)  loss_scale: 4096.0000 (6058.0492)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0004  max mem: 31081
[2025-03-11 00:53:13,408] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 00:53:13,408] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.20 GB / 503.51 GB
Epoch: [9]  [700/893]  eta: 0:04:46  lr: 0.002127  min_lr: 0.000003  loss: 0.3828 (0.3774)  class_acc: 0.8214 (0.8314)  loss_scale: 4096.0000 (6082.6476)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.59 GB / 503.51 GB
Epoch: [9]  [710/893]  eta: 0:04:31  lr: 0.002127  min_lr: 0.000003  loss: 0.3777 (0.3776)  class_acc: 0.8214 (0.8310)  loss_scale: 8192.0000 (6112.3150)  weight_decay: 0.0500 (0.0500)  time: 1.4685  data: 0.0003  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.56 GB / 503.51 GB
Epoch: [9]  [720/893]  eta: 0:04:16  lr: 0.002126  min_lr: 0.000003  loss: 0.3777 (0.3780)  class_acc: 0.7857 (0.8308)  loss_scale: 8192.0000 (6141.1595)  weight_decay: 0.0500 (0.0500)  time: 1.4720  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.31 GB / 503.51 GB
Epoch: [9]  [730/893]  eta: 0:04:02  lr: 0.002126  min_lr: 0.000003  loss: 0.3740 (0.3778)  class_acc: 0.8214 (0.8309)  loss_scale: 8192.0000 (6169.2148)  weight_decay: 0.0500 (0.0500)  time: 1.4751  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.53 GB / 503.51 GB
Epoch: [9]  [740/893]  eta: 0:03:47  lr: 0.002126  min_lr: 0.000003  loss: 0.3474 (0.3770)  class_acc: 0.8393 (0.8311)  loss_scale: 8192.0000 (6196.5128)  weight_decay: 0.0500 (0.0500)  time: 1.4731  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.15 GB / 503.51 GB
Epoch: [9]  [750/893]  eta: 0:03:32  lr: 0.002126  min_lr: 0.000003  loss: 0.3303 (0.3769)  class_acc: 0.8393 (0.8313)  loss_scale: 8192.0000 (6223.0839)  weight_decay: 0.0500 (0.0500)  time: 1.4679  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.32 GB / 503.51 GB
Epoch: [9]  [760/893]  eta: 0:03:17  lr: 0.002125  min_lr: 0.000003  loss: 0.3486 (0.3766)  class_acc: 0.8393 (0.8314)  loss_scale: 8192.0000 (6248.9566)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0004  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.62 GB / 503.51 GB
Epoch: [9]  [770/893]  eta: 0:03:02  lr: 0.002125  min_lr: 0.000003  loss: 0.3618 (0.3763)  class_acc: 0.8393 (0.8316)  loss_scale: 8192.0000 (6274.1582)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
[2025-03-11 00:55:19,614] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 8806
[2025-03-11 00:55:19,614] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 00:55:19,615] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.56 GB / 503.51 GB
Epoch: [9]  [780/893]  eta: 0:02:47  lr: 0.002125  min_lr: 0.000003  loss: 0.3691 (0.3766)  class_acc: 0.8393 (0.8314)  loss_scale: 8192.0000 (6282.9808)  weight_decay: 0.0500 (0.0500)  time: 1.4614  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.84 GB / 503.51 GB
Epoch: [9]  [790/893]  eta: 0:02:32  lr: 0.002124  min_lr: 0.000003  loss: 0.3296 (0.3762)  class_acc: 0.8393 (0.8316)  loss_scale: 4096.0000 (6255.3325)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0004  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.71 GB / 503.51 GB
Epoch: [9]  [800/893]  eta: 0:02:17  lr: 0.002124  min_lr: 0.000003  loss: 0.3425 (0.3769)  class_acc: 0.7857 (0.8312)  loss_scale: 4096.0000 (6228.3745)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0004  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.10 GB / 503.51 GB
Epoch: [9]  [810/893]  eta: 0:02:03  lr: 0.002124  min_lr: 0.000003  loss: 0.4187 (0.3770)  class_acc: 0.7857 (0.8310)  loss_scale: 4096.0000 (6202.0814)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.38 GB / 503.51 GB
Epoch: [9]  [820/893]  eta: 0:01:48  lr: 0.002124  min_lr: 0.000003  loss: 0.3669 (0.3769)  class_acc: 0.8214 (0.8311)  loss_scale: 4096.0000 (6176.4287)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.11 GB / 503.51 GB
Epoch: [9]  [830/893]  eta: 0:01:33  lr: 0.002123  min_lr: 0.000003  loss: 0.3694 (0.3771)  class_acc: 0.8214 (0.8310)  loss_scale: 4096.0000 (6151.3935)  weight_decay: 0.0500 (0.0500)  time: 1.4688  data: 0.0002  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.39 GB / 503.51 GB
Epoch: [9]  [840/893]  eta: 0:01:18  lr: 0.002123  min_lr: 0.000003  loss: 0.3828 (0.3773)  class_acc: 0.8214 (0.8310)  loss_scale: 4096.0000 (6126.9536)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.68 GB / 503.51 GB
Epoch: [9]  [850/893]  eta: 0:01:03  lr: 0.002123  min_lr: 0.000003  loss: 0.3708 (0.3770)  class_acc: 0.8214 (0.8312)  loss_scale: 4096.0000 (6103.0881)  weight_decay: 0.0500 (0.0500)  time: 1.4792  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.27 GB / 503.51 GB
Epoch: [9]  [860/893]  eta: 0:00:48  lr: 0.002122  min_lr: 0.000003  loss: 0.3494 (0.3769)  class_acc: 0.8571 (0.8313)  loss_scale: 4096.0000 (6079.7770)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.74 GB / 503.51 GB
Epoch: [9]  [870/893]  eta: 0:00:34  lr: 0.002122  min_lr: 0.000003  loss: 0.3494 (0.3768)  class_acc: 0.8571 (0.8315)  loss_scale: 4096.0000 (6057.0011)  weight_decay: 0.0500 (0.0500)  time: 1.4408  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.45 GB / 503.51 GB
Epoch: [9]  [880/893]  eta: 0:00:19  lr: 0.002122  min_lr: 0.000003  loss: 0.3857 (0.3770)  class_acc: 0.8214 (0.8314)  loss_scale: 4096.0000 (6034.7423)  weight_decay: 0.0500 (0.0500)  time: 1.4373  data: 0.0002  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.65 GB / 503.51 GB
Epoch: [9]  [890/893]  eta: 0:00:04  lr: 0.002122  min_lr: 0.000003  loss: 0.3972 (0.3772)  class_acc: 0.8214 (0.8314)  loss_scale: 4096.0000 (6012.9832)  weight_decay: 0.0500 (0.0500)  time: 1.4413  data: 0.0002  max mem: 31081
Epoch: [9]  [892/893]  eta: 0:00:01  lr: 0.002122  min_lr: 0.000003  loss: 0.3877 (0.3772)  class_acc: 0.8214 (0.8314)  loss_scale: 4096.0000 (6010.8341)  weight_decay: 0.0500 (0.0500)  time: 1.3876  data: 0.0002  max mem: 31081
Epoch: [9] Total time: 0:22:01 (1.4796 s / it)
Averaged stats: lr: 0.002122  min_lr: 0.000003  loss: 0.3877 (0.3772)  class_acc: 0.8214 (0.8314)  loss_scale: 4096.0000 (6010.8341)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:17:08  loss: 0.7089 (0.7089)  acc: 70.2381 (70.2381)  time: 11.3027  data: 10.7055  max mem: 31081
Val:  [ 10/728]  eta: 0:18:27  loss: 0.3797 (0.4271)  acc: 83.3333 (81.3853)  time: 1.5428  data: 1.0154  max mem: 31081
Val:  [ 20/728]  eta: 0:14:10  loss: 0.3797 (0.4436)  acc: 83.3333 (80.1020)  time: 0.6964  data: 0.1718  max mem: 31081
Val:  [ 30/728]  eta: 0:12:40  loss: 0.3768 (0.4500)  acc: 79.7619 (79.1475)  time: 0.8395  data: 0.3142  max mem: 31081
Val:  [ 40/728]  eta: 0:11:49  loss: 0.3697 (0.4638)  acc: 80.9524 (78.3972)  time: 0.8540  data: 0.3322  max mem: 31081
Val:  [ 50/728]  eta: 0:11:15  loss: 0.3927 (0.4503)  acc: 80.9524 (79.2717)  time: 0.8515  data: 0.3327  max mem: 31081
Val:  [ 60/728]  eta: 0:10:17  loss: 0.4866 (0.4694)  acc: 77.3810 (78.3763)  time: 0.7020  data: 0.1827  max mem: 31081
Val:  [ 70/728]  eta: 0:09:48  loss: 0.4866 (0.4691)  acc: 77.3810 (78.0349)  time: 0.6371  data: 0.1174  max mem: 31081
Val:  [ 80/728]  eta: 0:09:38  loss: 0.4351 (0.4762)  acc: 78.5714 (77.7925)  time: 0.7967  data: 0.2769  max mem: 31081
Val:  [ 90/728]  eta: 0:09:23  loss: 0.4390 (0.4887)  acc: 75.0000 (77.2894)  time: 0.8438  data: 0.3216  max mem: 31081
Val:  [100/728]  eta: 0:09:12  loss: 0.4390 (0.4816)  acc: 77.3810 (77.8171)  time: 0.8247  data: 0.3039  max mem: 31081
Val:  [110/728]  eta: 0:09:03  loss: 0.3251 (0.4782)  acc: 83.3333 (77.9816)  time: 0.8625  data: 0.3417  max mem: 31081
Val:  [120/728]  eta: 0:08:47  loss: 0.3251 (0.4718)  acc: 85.7143 (78.4435)  time: 0.8057  data: 0.2840  max mem: 31081
Val:  [130/728]  eta: 0:08:29  loss: 0.3869 (0.4774)  acc: 85.7143 (78.3170)  time: 0.6977  data: 0.1770  max mem: 31081
Val:  [140/728]  eta: 0:08:17  loss: 0.3869 (0.4781)  acc: 76.1905 (78.2421)  time: 0.7200  data: 0.1978  max mem: 31081
Val:  [150/728]  eta: 0:08:05  loss: 0.4794 (0.4820)  acc: 76.1905 (78.2088)  time: 0.7594  data: 0.2327  max mem: 31081
Val:  [160/728]  eta: 0:07:57  loss: 0.4541 (0.4864)  acc: 73.8095 (78.0390)  time: 0.7989  data: 0.2746  max mem: 31081
Val:  [170/728]  eta: 0:07:45  loss: 0.3814 (0.4840)  acc: 79.7619 (78.1259)  time: 0.7980  data: 0.2770  max mem: 31081
Val:  [180/728]  eta: 0:07:33  loss: 0.3861 (0.4846)  acc: 80.9524 (77.9861)  time: 0.7215  data: 0.2004  max mem: 31081
Val:  [190/728]  eta: 0:07:20  loss: 0.4090 (0.4852)  acc: 77.3810 (77.9980)  time: 0.6779  data: 0.1581  max mem: 31081
Val:  [200/728]  eta: 0:07:13  loss: 0.4256 (0.4829)  acc: 77.3810 (78.0917)  time: 0.7629  data: 0.2405  max mem: 31081
Val:  [210/728]  eta: 0:07:06  loss: 0.4483 (0.4866)  acc: 77.3810 (78.0072)  time: 0.8740  data: 0.3492  max mem: 31081
Val:  [220/728]  eta: 0:06:59  loss: 0.4147 (0.4826)  acc: 79.7619 (78.2374)  time: 0.8864  data: 0.3623  max mem: 31081
Val:  [230/728]  eta: 0:06:49  loss: 0.3923 (0.4882)  acc: 80.9524 (78.1025)  time: 0.8129  data: 0.2891  max mem: 31081
Val:  [240/728]  eta: 0:06:37  loss: 0.4362 (0.4877)  acc: 79.7619 (78.2059)  time: 0.6843  data: 0.1623  max mem: 31081
Val:  [250/728]  eta: 0:06:27  loss: 0.5126 (0.4877)  acc: 77.3810 (78.2726)  time: 0.6692  data: 0.1479  max mem: 31081
Val:  [260/728]  eta: 0:06:19  loss: 0.4115 (0.4854)  acc: 79.7619 (78.4346)  time: 0.7746  data: 0.2508  max mem: 31081
Val:  [270/728]  eta: 0:06:12  loss: 0.4151 (0.4855)  acc: 79.7619 (78.3694)  time: 0.8342  data: 0.3100  max mem: 31081
Val:  [280/728]  eta: 0:06:06  loss: 0.4605 (0.4862)  acc: 76.1905 (78.3469)  time: 0.9143  data: 0.3928  max mem: 31081
Val:  [290/728]  eta: 0:05:58  loss: 0.4577 (0.4839)  acc: 76.1905 (78.3873)  time: 0.9131  data: 0.3904  max mem: 31081
Val:  [300/728]  eta: 0:05:46  loss: 0.4277 (0.4832)  acc: 76.1905 (78.3855)  time: 0.6753  data: 0.1495  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.4763 (0.4858)  acc: 78.5714 (78.3686)  time: 0.6768  data: 0.1506  max mem: 31081
Val:  [320/728]  eta: 0:05:30  loss: 0.3954 (0.4846)  acc: 82.1429 (78.3934)  time: 0.8049  data: 0.2822  max mem: 31081
Val:  [330/728]  eta: 0:05:23  loss: 0.3954 (0.4823)  acc: 82.1429 (78.4959)  time: 0.8368  data: 0.3158  max mem: 31081
Val:  [340/728]  eta: 0:05:15  loss: 0.3830 (0.4779)  acc: 83.3333 (78.7146)  time: 0.8867  data: 0.3653  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3154 (0.4780)  acc: 84.5238 (78.8190)  time: 0.8386  data: 0.3155  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.3438 (0.4777)  acc: 80.9524 (78.8352)  time: 0.6590  data: 0.1344  max mem: 31081
Val:  [370/728]  eta: 0:04:48  loss: 0.4473 (0.4790)  acc: 79.7619 (78.7447)  time: 0.6989  data: 0.1769  max mem: 31081
Val:  [380/728]  eta: 0:04:41  loss: 0.4685 (0.4817)  acc: 75.0000 (78.6527)  time: 0.8627  data: 0.3400  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.3442 (0.4769)  acc: 82.1429 (78.8820)  time: 0.8445  data: 0.3197  max mem: 31081
Val:  [400/728]  eta: 0:04:26  loss: 0.3051 (0.4750)  acc: 85.7143 (78.9722)  time: 0.8736  data: 0.3499  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.3547 (0.4741)  acc: 82.1429 (78.9885)  time: 0.8889  data: 0.3669  max mem: 31081
Val:  [420/728]  eta: 0:04:08  loss: 0.4295 (0.4744)  acc: 78.5714 (79.0097)  time: 0.6949  data: 0.1736  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4841 (0.4751)  acc: 76.1905 (78.9029)  time: 0.7017  data: 0.1792  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.4841 (0.4746)  acc: 73.8095 (78.8927)  time: 0.8394  data: 0.3150  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4426 (0.4756)  acc: 79.7619 (78.7984)  time: 0.8329  data: 0.3100  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.4292 (0.4777)  acc: 77.3810 (78.6902)  time: 0.8458  data: 0.3239  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.3723 (0.4764)  acc: 77.3810 (78.7484)  time: 0.8057  data: 0.2840  max mem: 31081
Val:  [480/728]  eta: 0:03:19  loss: 0.3802 (0.4744)  acc: 83.3333 (78.8239)  time: 0.6548  data: 0.1342  max mem: 31081
Val:  [490/728]  eta: 0:03:11  loss: 0.3986 (0.4738)  acc: 80.9524 (78.8406)  time: 0.6885  data: 0.1667  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.4763 (0.4727)  acc: 76.1905 (78.8851)  time: 0.8462  data: 0.3236  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3583 (0.4711)  acc: 84.5238 (78.9605)  time: 0.8785  data: 0.3576  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4025 (0.4736)  acc: 78.5714 (78.8616)  time: 0.8622  data: 0.3399  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.5404 (0.4738)  acc: 72.6190 (78.8113)  time: 0.7541  data: 0.2262  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.5359 (0.4765)  acc: 75.0000 (78.6837)  time: 0.6159  data: 0.0869  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.4750 (0.4771)  acc: 75.0000 (78.6298)  time: 0.6691  data: 0.1452  max mem: 31081
Val:  [560/728]  eta: 0:02:14  loss: 0.4685 (0.4764)  acc: 77.3810 (78.6775)  time: 0.7559  data: 0.2348  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4685 (0.4799)  acc: 78.5714 (78.5902)  time: 0.8045  data: 0.2813  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.3968 (0.4794)  acc: 80.9524 (78.5960)  time: 0.8366  data: 0.3112  max mem: 31081
Val:  [590/728]  eta: 0:01:50  loss: 0.3683 (0.4800)  acc: 82.1429 (78.5795)  time: 0.7325  data: 0.2084  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.3826 (0.4798)  acc: 83.3333 (78.6269)  time: 0.6770  data: 0.1554  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4084 (0.4814)  acc: 80.9524 (78.5675)  time: 0.6393  data: 0.1186  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4595 (0.4848)  acc: 77.3810 (78.4909)  time: 0.7751  data: 0.2533  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4595 (0.4839)  acc: 77.3810 (78.5073)  time: 0.8589  data: 0.3360  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3371 (0.4831)  acc: 82.1429 (78.5139)  time: 0.7890  data: 0.2659  max mem: 31081
Val:  [650/728]  eta: 0:01:02  loss: 0.4496 (0.4831)  acc: 79.7619 (78.4983)  time: 0.8185  data: 0.2958  max mem: 31081
Val:  [660/728]  eta: 0:00:54  loss: 0.4496 (0.4827)  acc: 79.7619 (78.5084)  time: 0.7886  data: 0.2666  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.3963 (0.4833)  acc: 78.5714 (78.4685)  time: 0.6812  data: 0.1591  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.3930 (0.4824)  acc: 78.5714 (78.5050)  time: 0.6769  data: 0.1531  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3693 (0.4811)  acc: 80.9524 (78.5542)  time: 0.8228  data: 0.2979  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3849 (0.4817)  acc: 82.1429 (78.5511)  time: 0.8510  data: 0.3274  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4183 (0.4822)  acc: 80.9524 (78.5095)  time: 0.8484  data: 0.3260  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4274 (0.4826)  acc: 79.7619 (78.5186)  time: 0.8137  data: 0.2987  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4259 (0.4825)  acc: 79.7619 (78.5207)  time: 0.7889  data: 0.2987  max mem: 31081
Val: Total time: 0:09:35 (0.7901 s / it)
* Acc@1 78.521 AP 0.8009534478187561 loss 0.483
Accuracy of the network on the 61096 val videos: 78.5%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.95 GB / 503.51 GB
Epoch: [10]  [  0/893]  eta: 3:39:38  lr: 0.002122  min_lr: 0.000003  loss: 0.4480 (0.4480)  class_acc: 0.8214 (0.8214)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 14.7574  data: 13.4683  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.73 GB / 503.51 GB
Epoch: [10]  [ 10/893]  eta: 0:39:38  lr: 0.002121  min_lr: 0.000003  loss: 0.3533 (0.3453)  class_acc: 0.8750 (0.8604)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6942  data: 1.2248  max mem: 31081
[2025-03-11 01:08:18,708] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:08:18,708] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.82 GB / 503.51 GB
Epoch: [10]  [ 20/893]  eta: 0:30:52  lr: 0.002121  min_lr: 0.000003  loss: 0.3591 (0.3644)  class_acc: 0.8393 (0.8512)  loss_scale: 4096.0000 (5266.2857)  weight_decay: 0.0500 (0.0500)  time: 1.4906  data: 0.0004  max mem: 31081
[2025-03-11 01:08:41,307] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 8950
[2025-03-11 01:08:41,307] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 01:08:41,307] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.91 GB / 503.51 GB
Epoch: [10]  [ 30/893]  eta: 0:27:42  lr: 0.002121  min_lr: 0.000003  loss: 0.3708 (0.3646)  class_acc: 0.8214 (0.8416)  loss_scale: 8192.0000 (6077.9355)  weight_decay: 0.0500 (0.0500)  time: 1.5033  data: 0.0005  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.87 GB / 503.51 GB
Epoch: [10]  [ 40/893]  eta: 0:25:55  lr: 0.002120  min_lr: 0.000003  loss: 0.3596 (0.3650)  class_acc: 0.8214 (0.8415)  loss_scale: 4096.0000 (5594.5366)  weight_decay: 0.0500 (0.0500)  time: 1.5088  data: 0.0006  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.99 GB / 503.51 GB
Epoch: [10]  [ 50/893]  eta: 0:24:42  lr: 0.002120  min_lr: 0.000003  loss: 0.3445 (0.3626)  class_acc: 0.8393 (0.8421)  loss_scale: 4096.0000 (5300.7059)  weight_decay: 0.0500 (0.0500)  time: 1.5003  data: 0.0008  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.72 GB / 503.51 GB
Epoch: [10]  [ 60/893]  eta: 0:23:44  lr: 0.002120  min_lr: 0.000003  loss: 0.3445 (0.3678)  class_acc: 0.8393 (0.8381)  loss_scale: 4096.0000 (5103.2131)  weight_decay: 0.0500 (0.0500)  time: 1.4776  data: 0.0006  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.42 GB / 503.51 GB
Epoch: [10]  [ 70/893]  eta: 0:22:59  lr: 0.002119  min_lr: 0.000003  loss: 0.3904 (0.3682)  class_acc: 0.8393 (0.8400)  loss_scale: 4096.0000 (4961.3521)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0003  max mem: 31081
[2025-03-11 01:09:53,844] [INFO] [logging.py:129:log_dist] [Rank 0] step=9000, skipped=51, lr=[2.767839301388423e-06, 2.767839301388423e-06, 4.613065502314039e-06, 4.613065502314039e-06, 7.688442503856733e-06, 7.688442503856733e-06, 1.281407083976122e-05, 1.281407083976122e-05, 2.1356784732935368e-05, 2.1356784732935368e-05, 3.559464122155895e-05, 3.559464122155895e-05, 5.932440203593159e-05, 5.932440203593159e-05, 9.88740033932193e-05, 9.88740033932193e-05, 0.00016479000565536553, 0.00016479000565536553, 0.00027465000942560925, 0.00027465000942560925, 0.0004577500157093487, 0.0004577500157093487, 0.0007629166928489145, 0.0007629166928489145, 0.0012715278214148575, 0.0012715278214148575, 0.0021192130356914294, 0.0021192130356914294], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 01:09:53,845] [INFO] [timer.py:264:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=61.05079031614024, CurrSamplesPerSec=61.74079806459308, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.55 GB / 503.51 GB
Epoch: [10]  [ 80/893]  eta: 0:22:21  lr: 0.002119  min_lr: 0.000003  loss: 0.3665 (0.3676)  class_acc: 0.8571 (0.8404)  loss_scale: 4096.0000 (4854.5185)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.80 GB / 503.51 GB
Epoch: [10]  [ 90/893]  eta: 0:21:48  lr: 0.002119  min_lr: 0.000003  loss: 0.3411 (0.3659)  class_acc: 0.8571 (0.8414)  loss_scale: 4096.0000 (4771.1648)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.83 GB / 503.51 GB
Epoch: [10]  [100/893]  eta: 0:21:18  lr: 0.002119  min_lr: 0.000003  loss: 0.3835 (0.3704)  class_acc: 0.8214 (0.8388)  loss_scale: 4096.0000 (4704.3168)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.03 GB / 503.51 GB
Epoch: [10]  [110/893]  eta: 0:20:52  lr: 0.002118  min_lr: 0.000003  loss: 0.3506 (0.3660)  class_acc: 0.8393 (0.8399)  loss_scale: 4096.0000 (4649.5135)  weight_decay: 0.0500 (0.0500)  time: 1.4594  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.08 GB / 503.51 GB
Epoch: [10]  [120/893]  eta: 0:20:27  lr: 0.002118  min_lr: 0.000003  loss: 0.3342 (0.3667)  class_acc: 0.8393 (0.8393)  loss_scale: 4096.0000 (4603.7686)  weight_decay: 0.0500 (0.0500)  time: 1.4627  data: 0.0002  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.08 GB / 503.51 GB
Epoch: [10]  [130/893]  eta: 0:20:04  lr: 0.002118  min_lr: 0.000003  loss: 0.3481 (0.3663)  class_acc: 0.8214 (0.8386)  loss_scale: 4096.0000 (4565.0076)  weight_decay: 0.0500 (0.0500)  time: 1.4652  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.41 GB / 503.51 GB
Epoch: [10]  [140/893]  eta: 0:19:43  lr: 0.002117  min_lr: 0.000003  loss: 0.3687 (0.3681)  class_acc: 0.8214 (0.8379)  loss_scale: 4096.0000 (4531.7447)  weight_decay: 0.0500 (0.0500)  time: 1.4708  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.08 GB / 503.51 GB
Epoch: [10]  [150/893]  eta: 0:19:22  lr: 0.002117  min_lr: 0.000003  loss: 0.3987 (0.3711)  class_acc: 0.8036 (0.8362)  loss_scale: 4096.0000 (4502.8874)  weight_decay: 0.0500 (0.0500)  time: 1.4693  data: 0.0002  max mem: 31081
[2025-03-11 01:11:50,995] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:11:50,995] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.49 GB / 503.51 GB
Epoch: [10]  [160/893]  eta: 0:19:02  lr: 0.002117  min_lr: 0.000003  loss: 0.4009 (0.3721)  class_acc: 0.8036 (0.8346)  loss_scale: 4096.0000 (4528.4969)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.17 GB / 503.51 GB
Epoch: [10]  [170/893]  eta: 0:18:42  lr: 0.002116  min_lr: 0.000003  loss: 0.3687 (0.3720)  class_acc: 0.8214 (0.8350)  loss_scale: 8192.0000 (4742.7368)  weight_decay: 0.0500 (0.0500)  time: 1.4706  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.98 GB / 503.51 GB
Epoch: [10]  [180/893]  eta: 0:18:24  lr: 0.002116  min_lr: 0.000003  loss: 0.3662 (0.3717)  class_acc: 0.8393 (0.8345)  loss_scale: 8192.0000 (4933.3039)  weight_decay: 0.0500 (0.0500)  time: 1.4734  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.98 GB / 503.51 GB
Epoch: [10]  [190/893]  eta: 0:18:05  lr: 0.002116  min_lr: 0.000003  loss: 0.3755 (0.3721)  class_acc: 0.8393 (0.8347)  loss_scale: 8192.0000 (5103.9162)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0004  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.68 GB / 503.51 GB
Epoch: [10]  [200/893]  eta: 0:17:47  lr: 0.002116  min_lr: 0.000003  loss: 0.3701 (0.3720)  class_acc: 0.8393 (0.8337)  loss_scale: 8192.0000 (5257.5522)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.19 GB / 503.51 GB
Epoch: [10]  [210/893]  eta: 0:17:29  lr: 0.002115  min_lr: 0.000003  loss: 0.3528 (0.3712)  class_acc: 0.8393 (0.8336)  loss_scale: 8192.0000 (5396.6256)  weight_decay: 0.0500 (0.0500)  time: 1.4649  data: 0.0002  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.49 GB / 503.51 GB
Epoch: [10]  [220/893]  eta: 0:17:12  lr: 0.002115  min_lr: 0.000003  loss: 0.3425 (0.3721)  class_acc: 0.8393 (0.8335)  loss_scale: 8192.0000 (5523.1131)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0002  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.96 GB / 503.51 GB
Epoch: [10]  [230/893]  eta: 0:16:54  lr: 0.002115  min_lr: 0.000003  loss: 0.3577 (0.3713)  class_acc: 0.8393 (0.8340)  loss_scale: 8192.0000 (5638.6494)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.04 GB / 503.51 GB
Epoch: [10]  [240/893]  eta: 0:16:37  lr: 0.002114  min_lr: 0.000003  loss: 0.3577 (0.3703)  class_acc: 0.8393 (0.8348)  loss_scale: 8192.0000 (5744.5975)  weight_decay: 0.0500 (0.0500)  time: 1.4586  data: 0.0004  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.24 GB / 503.51 GB
Epoch: [10]  [250/893]  eta: 0:16:20  lr: 0.002114  min_lr: 0.000003  loss: 0.3684 (0.3720)  class_acc: 0.8393 (0.8347)  loss_scale: 8192.0000 (5842.1036)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.47 GB / 503.51 GB
Epoch: [10]  [260/893]  eta: 0:16:03  lr: 0.002114  min_lr: 0.000003  loss: 0.3740 (0.3717)  class_acc: 0.8393 (0.8348)  loss_scale: 8192.0000 (5932.1379)  weight_decay: 0.0500 (0.0500)  time: 1.4628  data: 0.0002  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.48 GB / 503.51 GB
Epoch: [10]  [270/893]  eta: 0:15:47  lr: 0.002113  min_lr: 0.000003  loss: 0.3350 (0.3701)  class_acc: 0.8393 (0.8357)  loss_scale: 8192.0000 (6015.5277)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.99 GB / 503.51 GB
Epoch: [10]  [280/893]  eta: 0:15:30  lr: 0.002113  min_lr: 0.000003  loss: 0.3201 (0.3692)  class_acc: 0.8571 (0.8365)  loss_scale: 8192.0000 (6092.9822)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0003  max mem: 31081
[2025-03-11 01:14:58,688] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:14:58,688] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.27 GB / 503.51 GB
Epoch: [10]  [290/893]  eta: 0:15:14  lr: 0.002113  min_lr: 0.000003  loss: 0.3201 (0.3685)  class_acc: 0.8571 (0.8366)  loss_scale: 8192.0000 (6277.7182)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
[2025-03-11 01:15:04,497] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 9211
[2025-03-11 01:15:04,497] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 01:15:04,497] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.61 GB / 503.51 GB
Epoch: [10]  [300/893]  eta: 0:14:58  lr: 0.002112  min_lr: 0.000003  loss: 0.3811 (0.3689)  class_acc: 0.8393 (0.8367)  loss_scale: 8192.0000 (6341.3156)  weight_decay: 0.0500 (0.0500)  time: 1.4699  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.57 GB / 503.51 GB
Epoch: [10]  [310/893]  eta: 0:14:42  lr: 0.002112  min_lr: 0.000003  loss: 0.3774 (0.3686)  class_acc: 0.8393 (0.8368)  loss_scale: 8192.0000 (6400.8232)  weight_decay: 0.0500 (0.0500)  time: 1.4681  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.68 GB / 503.51 GB
Epoch: [10]  [320/893]  eta: 0:14:26  lr: 0.002112  min_lr: 0.000003  loss: 0.3611 (0.3690)  class_acc: 0.8214 (0.8361)  loss_scale: 8192.0000 (6456.6231)  weight_decay: 0.0500 (0.0500)  time: 1.4693  data: 0.0004  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.82 GB / 503.51 GB
Epoch: [10]  [330/893]  eta: 0:14:10  lr: 0.002112  min_lr: 0.000003  loss: 0.3206 (0.3676)  class_acc: 0.8393 (0.8369)  loss_scale: 8192.0000 (6509.0514)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0003  max mem: 31081
[2025-03-11 01:16:14,942] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 9259
[2025-03-11 01:16:14,942] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 01:16:14,942] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.08 GB / 503.51 GB
Epoch: [10]  [340/893]  eta: 0:13:54  lr: 0.002111  min_lr: 0.000003  loss: 0.3440 (0.3689)  class_acc: 0.8393 (0.8357)  loss_scale: 8192.0000 (6534.3812)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.93 GB / 503.51 GB
Epoch: [10]  [350/893]  eta: 0:13:39  lr: 0.002111  min_lr: 0.000003  loss: 0.3904 (0.3694)  class_acc: 0.8036 (0.8357)  loss_scale: 4096.0000 (6464.9117)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.45 GB / 503.51 GB
Epoch: [10]  [360/893]  eta: 0:13:23  lr: 0.002111  min_lr: 0.000003  loss: 0.3801 (0.3690)  class_acc: 0.8214 (0.8360)  loss_scale: 4096.0000 (6399.2909)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.21 GB / 503.51 GB
Epoch: [10]  [370/893]  eta: 0:13:07  lr: 0.002110  min_lr: 0.000003  loss: 0.3804 (0.3691)  class_acc: 0.8036 (0.8357)  loss_scale: 4096.0000 (6337.2075)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.23 GB / 503.51 GB
Epoch: [10]  [380/893]  eta: 0:12:51  lr: 0.002110  min_lr: 0.000003  loss: 0.3772 (0.3689)  class_acc: 0.8214 (0.8356)  loss_scale: 4096.0000 (6278.3832)  weight_decay: 0.0500 (0.0500)  time: 1.4573  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.76 GB / 503.51 GB
Epoch: [10]  [390/893]  eta: 0:12:36  lr: 0.002110  min_lr: 0.000003  loss: 0.3601 (0.3680)  class_acc: 0.8214 (0.8360)  loss_scale: 4096.0000 (6222.5678)  weight_decay: 0.0500 (0.0500)  time: 1.4616  data: 0.0002  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.35 GB / 503.51 GB
Epoch: [10]  [400/893]  eta: 0:12:20  lr: 0.002109  min_lr: 0.000003  loss: 0.3394 (0.3680)  class_acc: 0.8393 (0.8359)  loss_scale: 4096.0000 (6169.5362)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0002  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.88 GB / 503.51 GB
Epoch: [10]  [410/893]  eta: 0:12:05  lr: 0.002109  min_lr: 0.000003  loss: 0.3596 (0.3682)  class_acc: 0.8214 (0.8359)  loss_scale: 4096.0000 (6119.0852)  weight_decay: 0.0500 (0.0500)  time: 1.4737  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.10 GB / 503.51 GB
Epoch: [10]  [420/893]  eta: 0:11:50  lr: 0.002109  min_lr: 0.000003  loss: 0.3542 (0.3676)  class_acc: 0.8571 (0.8367)  loss_scale: 4096.0000 (6071.0309)  weight_decay: 0.0500 (0.0500)  time: 1.4796  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.92 GB / 503.51 GB
Epoch: [10]  [430/893]  eta: 0:11:35  lr: 0.002108  min_lr: 0.000003  loss: 0.3550 (0.3680)  class_acc: 0.8571 (0.8362)  loss_scale: 4096.0000 (6025.2065)  weight_decay: 0.0500 (0.0500)  time: 1.4834  data: 0.0003  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.54 GB / 503.51 GB
Epoch: [10]  [440/893]  eta: 0:11:19  lr: 0.002108  min_lr: 0.000003  loss: 0.3684 (0.3679)  class_acc: 0.8214 (0.8360)  loss_scale: 4096.0000 (5981.4603)  weight_decay: 0.0500 (0.0500)  time: 1.4731  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.93 GB / 503.51 GB
Epoch: [10]  [450/893]  eta: 0:11:04  lr: 0.002108  min_lr: 0.000003  loss: 0.3521 (0.3673)  class_acc: 0.8393 (0.8364)  loss_scale: 4096.0000 (5939.6541)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0004  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.58 GB / 503.51 GB
Epoch: [10]  [460/893]  eta: 0:10:49  lr: 0.002107  min_lr: 0.000003  loss: 0.3425 (0.3667)  class_acc: 0.8571 (0.8369)  loss_scale: 4096.0000 (5899.6616)  weight_decay: 0.0500 (0.0500)  time: 1.4734  data: 0.0003  max mem: 31081
[2025-03-11 01:19:24,590] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:19:24,590] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.56 GB / 503.51 GB
Epoch: [10]  [470/893]  eta: 0:10:33  lr: 0.002107  min_lr: 0.000003  loss: 0.3193 (0.3658)  class_acc: 0.8571 (0.8374)  loss_scale: 4096.0000 (5887.4565)  weight_decay: 0.0500 (0.0500)  time: 1.4708  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.29 GB / 503.51 GB
Epoch: [10]  [480/893]  eta: 0:10:18  lr: 0.002107  min_lr: 0.000003  loss: 0.3142 (0.3658)  class_acc: 0.8393 (0.8373)  loss_scale: 8192.0000 (5935.3680)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0005  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.04 GB / 503.51 GB
Epoch: [10]  [490/893]  eta: 0:10:03  lr: 0.002106  min_lr: 0.000003  loss: 0.3362 (0.3655)  class_acc: 0.8393 (0.8376)  loss_scale: 8192.0000 (5981.3279)  weight_decay: 0.0500 (0.0500)  time: 1.4711  data: 0.0004  max mem: 31081
[2025-03-11 01:20:10,034] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 9419
[2025-03-11 01:20:10,035] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 01:20:10,035] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.93 GB / 503.51 GB
Epoch: [10]  [500/893]  eta: 0:09:48  lr: 0.002106  min_lr: 0.000003  loss: 0.3347 (0.3652)  class_acc: 0.8393 (0.8378)  loss_scale: 8192.0000 (6009.1018)  weight_decay: 0.0500 (0.0500)  time: 1.4617  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.97 GB / 503.51 GB
Epoch: [10]  [510/893]  eta: 0:09:32  lr: 0.002106  min_lr: 0.000003  loss: 0.3511 (0.3651)  class_acc: 0.8393 (0.8381)  loss_scale: 4096.0000 (5971.6634)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.10 GB / 503.51 GB
Epoch: [10]  [520/893]  eta: 0:09:17  lr: 0.002105  min_lr: 0.000003  loss: 0.3547 (0.3656)  class_acc: 0.8393 (0.8378)  loss_scale: 4096.0000 (5935.6622)  weight_decay: 0.0500 (0.0500)  time: 1.4572  data: 0.0002  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.06 GB / 503.51 GB
Epoch: [10]  [530/893]  eta: 0:09:02  lr: 0.002105  min_lr: 0.000003  loss: 0.3677 (0.3661)  class_acc: 0.8214 (0.8376)  loss_scale: 4096.0000 (5901.0169)  weight_decay: 0.0500 (0.0500)  time: 1.4560  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.71 GB / 503.51 GB
Epoch: [10]  [540/893]  eta: 0:08:47  lr: 0.002105  min_lr: 0.000003  loss: 0.3643 (0.3661)  class_acc: 0.8393 (0.8375)  loss_scale: 4096.0000 (5867.6525)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0004  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.88 GB / 503.51 GB
Epoch: [10]  [550/893]  eta: 0:08:32  lr: 0.002104  min_lr: 0.000003  loss: 0.3416 (0.3657)  class_acc: 0.8393 (0.8377)  loss_scale: 4096.0000 (5835.4991)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.49 GB / 503.51 GB
Epoch: [10]  [560/893]  eta: 0:08:17  lr: 0.002104  min_lr: 0.000003  loss: 0.3384 (0.3651)  class_acc: 0.8393 (0.8377)  loss_scale: 4096.0000 (5804.4920)  weight_decay: 0.0500 (0.0500)  time: 1.4586  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.70 GB / 503.51 GB
Epoch: [10]  [570/893]  eta: 0:08:01  lr: 0.002104  min_lr: 0.000003  loss: 0.3069 (0.3641)  class_acc: 0.8750 (0.8383)  loss_scale: 4096.0000 (5774.5709)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.34 GB / 503.51 GB
Epoch: [10]  [580/893]  eta: 0:07:46  lr: 0.002104  min_lr: 0.000003  loss: 0.3264 (0.3643)  class_acc: 0.8750 (0.8383)  loss_scale: 4096.0000 (5745.6799)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.10 GB / 503.51 GB
Epoch: [10]  [590/893]  eta: 0:07:31  lr: 0.002103  min_lr: 0.000003  loss: 0.3923 (0.3653)  class_acc: 0.8214 (0.8378)  loss_scale: 4096.0000 (5717.7665)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.96 GB / 503.51 GB
Epoch: [10]  [600/893]  eta: 0:07:16  lr: 0.002103  min_lr: 0.000003  loss: 0.3801 (0.3653)  class_acc: 0.8393 (0.8380)  loss_scale: 4096.0000 (5690.7820)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.53 GB / 503.51 GB
Epoch: [10]  [610/893]  eta: 0:07:01  lr: 0.002103  min_lr: 0.000003  loss: 0.3564 (0.3649)  class_acc: 0.8393 (0.8378)  loss_scale: 4096.0000 (5664.6809)  weight_decay: 0.0500 (0.0500)  time: 1.4735  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.93 GB / 503.51 GB
Epoch: [10]  [620/893]  eta: 0:06:46  lr: 0.002102  min_lr: 0.000003  loss: 0.3713 (0.3657)  class_acc: 0.8214 (0.8373)  loss_scale: 4096.0000 (5639.4203)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0002  max mem: 31081
[2025-03-11 01:23:18,989] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:23:18,990] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.91 GB / 503.51 GB
Epoch: [10]  [630/893]  eta: 0:06:31  lr: 0.002102  min_lr: 0.000003  loss: 0.3787 (0.3658)  class_acc: 0.8214 (0.8374)  loss_scale: 4096.0000 (5634.4342)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.61 GB / 503.51 GB
Epoch: [10]  [640/893]  eta: 0:06:16  lr: 0.002102  min_lr: 0.000003  loss: 0.3503 (0.3654)  class_acc: 0.8571 (0.8376)  loss_scale: 8192.0000 (5674.3339)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0003  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.98 GB / 503.51 GB
Epoch: [10]  [650/893]  eta: 0:06:01  lr: 0.002101  min_lr: 0.000003  loss: 0.3503 (0.3653)  class_acc: 0.8393 (0.8377)  loss_scale: 8192.0000 (5713.0077)  weight_decay: 0.0500 (0.0500)  time: 1.4612  data: 0.0004  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.36 GB / 503.51 GB
Epoch: [10]  [660/893]  eta: 0:05:46  lr: 0.002101  min_lr: 0.000003  loss: 0.3613 (0.3650)  class_acc: 0.8393 (0.8378)  loss_scale: 8192.0000 (5750.5113)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.24 GB / 503.51 GB
Epoch: [10]  [670/893]  eta: 0:05:31  lr: 0.002101  min_lr: 0.000003  loss: 0.3528 (0.3649)  class_acc: 0.8393 (0.8376)  loss_scale: 8192.0000 (5786.8972)  weight_decay: 0.0500 (0.0500)  time: 1.4711  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.51 GB / 503.51 GB
Epoch: [10]  [680/893]  eta: 0:05:17  lr: 0.002100  min_lr: 0.000003  loss: 0.3503 (0.3648)  class_acc: 0.8393 (0.8376)  loss_scale: 8192.0000 (5822.2144)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
[2025-03-11 01:24:48,643] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 9609
[2025-03-11 01:24:48,643] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 01:24:48,643] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.97 GB / 503.51 GB
Epoch: [10]  [690/893]  eta: 0:05:02  lr: 0.002100  min_lr: 0.000003  loss: 0.3628 (0.3654)  class_acc: 0.8214 (0.8375)  loss_scale: 8192.0000 (5844.6541)  weight_decay: 0.0500 (0.0500)  time: 1.4776  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.20 GB / 503.51 GB
Epoch: [10]  [700/893]  eta: 0:04:47  lr: 0.002100  min_lr: 0.000003  loss: 0.3628 (0.3652)  class_acc: 0.8214 (0.8375)  loss_scale: 4096.0000 (5819.7090)  weight_decay: 0.0500 (0.0500)  time: 1.4737  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.03 GB / 503.51 GB
Epoch: [10]  [710/893]  eta: 0:04:32  lr: 0.002099  min_lr: 0.000003  loss: 0.3044 (0.3645)  class_acc: 0.8393 (0.8380)  loss_scale: 4096.0000 (5795.4655)  weight_decay: 0.0500 (0.0500)  time: 1.4594  data: 0.0004  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.97 GB / 503.51 GB
Epoch: [10]  [720/893]  eta: 0:04:17  lr: 0.002099  min_lr: 0.000003  loss: 0.2937 (0.3643)  class_acc: 0.8571 (0.8379)  loss_scale: 4096.0000 (5771.8946)  weight_decay: 0.0500 (0.0500)  time: 1.4614  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.49 GB / 503.51 GB
Epoch: [10]  [730/893]  eta: 0:04:02  lr: 0.002099  min_lr: 0.000003  loss: 0.3257 (0.3636)  class_acc: 0.8393 (0.8383)  loss_scale: 4096.0000 (5748.9685)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.84 GB / 503.51 GB
Epoch: [10]  [740/893]  eta: 0:03:47  lr: 0.002098  min_lr: 0.000003  loss: 0.3325 (0.3636)  class_acc: 0.8393 (0.8382)  loss_scale: 4096.0000 (5726.6613)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0004  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.96 GB / 503.51 GB
Epoch: [10]  [750/893]  eta: 0:03:32  lr: 0.002098  min_lr: 0.000003  loss: 0.3826 (0.3641)  class_acc: 0.8214 (0.8379)  loss_scale: 4096.0000 (5704.9481)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0005  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.71 GB / 503.51 GB
Epoch: [10]  [760/893]  eta: 0:03:17  lr: 0.002097  min_lr: 0.000003  loss: 0.3813 (0.3639)  class_acc: 0.8214 (0.8380)  loss_scale: 4096.0000 (5683.8055)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0004  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.98 GB / 503.51 GB
Epoch: [10]  [770/893]  eta: 0:03:02  lr: 0.002097  min_lr: 0.000003  loss: 0.3533 (0.3640)  class_acc: 0.8214 (0.8378)  loss_scale: 4096.0000 (5663.2114)  weight_decay: 0.0500 (0.0500)  time: 1.4579  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.90 GB / 503.51 GB
Epoch: [10]  [780/893]  eta: 0:02:47  lr: 0.002097  min_lr: 0.000003  loss: 0.3330 (0.3638)  class_acc: 0.8393 (0.8380)  loss_scale: 4096.0000 (5643.1447)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.11 GB / 503.51 GB
Epoch: [10]  [790/893]  eta: 0:02:32  lr: 0.002096  min_lr: 0.000003  loss: 0.3330 (0.3635)  class_acc: 0.8393 (0.8381)  loss_scale: 4096.0000 (5623.5853)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.33 GB / 503.51 GB
Epoch: [10]  [800/893]  eta: 0:02:18  lr: 0.002096  min_lr: 0.000003  loss: 0.3259 (0.3629)  class_acc: 0.8393 (0.8384)  loss_scale: 4096.0000 (5604.5144)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0004  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.40 GB / 503.51 GB
Epoch: [10]  [810/893]  eta: 0:02:03  lr: 0.002096  min_lr: 0.000003  loss: 0.3257 (0.3628)  class_acc: 0.8393 (0.8385)  loss_scale: 4096.0000 (5585.9137)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
[2025-03-11 01:27:57,463] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:27:57,463] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.02 GB / 503.51 GB
Epoch: [10]  [820/893]  eta: 0:01:48  lr: 0.002095  min_lr: 0.000003  loss: 0.3303 (0.3623)  class_acc: 0.8571 (0.8387)  loss_scale: 4096.0000 (5582.7333)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0002  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.28 GB / 503.51 GB
Epoch: [10]  [830/893]  eta: 0:01:33  lr: 0.002095  min_lr: 0.000003  loss: 0.3167 (0.3620)  class_acc: 0.8571 (0.8389)  loss_scale: 8192.0000 (5614.1324)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0002  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.99 GB / 503.51 GB
Epoch: [10]  [840/893]  eta: 0:01:18  lr: 0.002095  min_lr: 0.000003  loss: 0.3445 (0.3622)  class_acc: 0.8393 (0.8388)  loss_scale: 8192.0000 (5644.7848)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0002  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.85 GB / 503.51 GB
Epoch: [10]  [850/893]  eta: 0:01:03  lr: 0.002094  min_lr: 0.000003  loss: 0.3445 (0.3622)  class_acc: 0.8393 (0.8386)  loss_scale: 8192.0000 (5674.7168)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0002  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.82 GB / 503.51 GB
Epoch: [10]  [860/893]  eta: 0:00:48  lr: 0.002094  min_lr: 0.000003  loss: 0.3298 (0.3620)  class_acc: 0.8393 (0.8388)  loss_scale: 8192.0000 (5703.9535)  weight_decay: 0.0500 (0.0500)  time: 1.4483  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.81 GB / 503.51 GB
Epoch: [10]  [870/893]  eta: 0:00:34  lr: 0.002094  min_lr: 0.000003  loss: 0.3174 (0.3616)  class_acc: 0.8571 (0.8390)  loss_scale: 8192.0000 (5732.5189)  weight_decay: 0.0500 (0.0500)  time: 1.4425  data: 0.0002  max mem: 31081
[2025-03-11 01:29:17,470] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 9793
[2025-03-11 01:29:17,470] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 01:29:17,470] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.61 GB / 503.51 GB
Epoch: [10]  [880/893]  eta: 0:00:19  lr: 0.002093  min_lr: 0.000003  loss: 0.3313 (0.3619)  class_acc: 0.8393 (0.8389)  loss_scale: 8192.0000 (5723.2418)  weight_decay: 0.0500 (0.0500)  time: 1.4484  data: 0.0002  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.45 GB / 503.51 GB
Epoch: [10]  [890/893]  eta: 0:00:04  lr: 0.002093  min_lr: 0.000003  loss: 0.3562 (0.3620)  class_acc: 0.8214 (0.8388)  loss_scale: 4096.0000 (5704.9787)  weight_decay: 0.0500 (0.0500)  time: 1.4507  data: 0.0001  max mem: 31081
Epoch: [10]  [892/893]  eta: 0:00:01  lr: 0.002093  min_lr: 0.000003  loss: 0.3562 (0.3620)  class_acc: 0.8214 (0.8387)  loss_scale: 4096.0000 (5703.1749)  weight_decay: 0.0500 (0.0500)  time: 1.3987  data: 0.0001  max mem: 31081
Epoch: [10] Total time: 0:22:02 (1.4810 s / it)
Averaged stats: lr: 0.002093  min_lr: 0.000003  loss: 0.3562 (0.3620)  class_acc: 0.8214 (0.8387)  loss_scale: 4096.0000 (5703.1749)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:14:24  loss: 0.4118 (0.4118)  acc: 79.7619 (79.7619)  time: 11.0777  data: 10.5438  max mem: 31081
Val:  [ 10/728]  eta: 0:18:36  loss: 0.4138 (0.4505)  acc: 82.1429 (81.6017)  time: 1.5551  data: 1.0328  max mem: 31081
Val:  [ 20/728]  eta: 0:14:16  loss: 0.4138 (0.4541)  acc: 83.3333 (81.0091)  time: 0.7157  data: 0.1944  max mem: 31081
Val:  [ 30/728]  eta: 0:12:43  loss: 0.4368 (0.4907)  acc: 79.7619 (78.6866)  time: 0.8394  data: 0.3190  max mem: 31081
Val:  [ 40/728]  eta: 0:11:52  loss: 0.4773 (0.5075)  acc: 75.0000 (77.7294)  time: 0.8544  data: 0.3342  max mem: 31081
Val:  [ 50/728]  eta: 0:11:16  loss: 0.4323 (0.4841)  acc: 78.5714 (78.6181)  time: 0.8489  data: 0.3257  max mem: 31081
Val:  [ 60/728]  eta: 0:10:23  loss: 0.4357 (0.4832)  acc: 78.5714 (78.6690)  time: 0.7212  data: 0.1983  max mem: 31081
Val:  [ 70/728]  eta: 0:09:53  loss: 0.4344 (0.4857)  acc: 77.3810 (78.5044)  time: 0.6583  data: 0.1378  max mem: 31081
Val:  [ 80/728]  eta: 0:09:40  loss: 0.4459 (0.4920)  acc: 77.3810 (78.2628)  time: 0.7841  data: 0.2623  max mem: 31081
Val:  [ 90/728]  eta: 0:09:26  loss: 0.4808 (0.4972)  acc: 78.5714 (77.9697)  time: 0.8368  data: 0.3142  max mem: 31081
Val:  [100/728]  eta: 0:09:13  loss: 0.4476 (0.4909)  acc: 79.7619 (78.0764)  time: 0.8243  data: 0.3001  max mem: 31081
Val:  [110/728]  eta: 0:09:03  loss: 0.4355 (0.4913)  acc: 78.5714 (77.8207)  time: 0.8461  data: 0.3219  max mem: 31081
Val:  [120/728]  eta: 0:08:46  loss: 0.4364 (0.4893)  acc: 77.3810 (78.1976)  time: 0.7892  data: 0.2674  max mem: 31081
Val:  [130/728]  eta: 0:08:27  loss: 0.4364 (0.4892)  acc: 77.3810 (78.1807)  time: 0.6771  data: 0.1574  max mem: 31081
Val:  [140/728]  eta: 0:08:18  loss: 0.4957 (0.4947)  acc: 77.3810 (77.9551)  time: 0.7304  data: 0.2104  max mem: 31081
Val:  [150/728]  eta: 0:08:05  loss: 0.5160 (0.4969)  acc: 75.0000 (77.6805)  time: 0.7785  data: 0.2570  max mem: 31081
Val:  [160/728]  eta: 0:07:56  loss: 0.4991 (0.4990)  acc: 71.4286 (77.4919)  time: 0.7793  data: 0.2563  max mem: 31081
Val:  [170/728]  eta: 0:07:45  loss: 0.4943 (0.5001)  acc: 78.5714 (77.4227)  time: 0.7845  data: 0.2618  max mem: 31081
Val:  [180/728]  eta: 0:07:33  loss: 0.4776 (0.5001)  acc: 79.7619 (77.4993)  time: 0.7373  data: 0.2151  max mem: 31081
Val:  [190/728]  eta: 0:07:21  loss: 0.4670 (0.4997)  acc: 76.1905 (77.5991)  time: 0.7033  data: 0.1772  max mem: 31081
Val:  [200/728]  eta: 0:07:13  loss: 0.4087 (0.4951)  acc: 79.7619 (77.7778)  time: 0.7700  data: 0.2441  max mem: 31081
Val:  [210/728]  eta: 0:07:07  loss: 0.3869 (0.4974)  acc: 83.3333 (77.7703)  time: 0.8739  data: 0.3481  max mem: 31081
Val:  [220/728]  eta: 0:07:00  loss: 0.3869 (0.4934)  acc: 83.3333 (77.9681)  time: 0.8814  data: 0.3559  max mem: 31081
Val:  [230/728]  eta: 0:06:50  loss: 0.4259 (0.4967)  acc: 79.7619 (77.8139)  time: 0.8235  data: 0.3017  max mem: 31081
Val:  [240/728]  eta: 0:06:38  loss: 0.4211 (0.4940)  acc: 82.1429 (77.8947)  time: 0.6939  data: 0.1727  max mem: 31081
Val:  [250/728]  eta: 0:06:27  loss: 0.4179 (0.4946)  acc: 82.1429 (77.8932)  time: 0.6611  data: 0.1383  max mem: 31081
Val:  [260/728]  eta: 0:06:21  loss: 0.4383 (0.4944)  acc: 78.5714 (77.9739)  time: 0.8003  data: 0.2794  max mem: 31081
Val:  [270/728]  eta: 0:06:13  loss: 0.4244 (0.4954)  acc: 78.5714 (77.8774)  time: 0.8561  data: 0.3356  max mem: 31081
Val:  [280/728]  eta: 0:06:08  loss: 0.4540 (0.4966)  acc: 78.5714 (77.8851)  time: 0.9065  data: 0.3842  max mem: 31081
Val:  [290/728]  eta: 0:05:59  loss: 0.4889 (0.4945)  acc: 78.5714 (77.9496)  time: 0.9079  data: 0.3878  max mem: 31081
Val:  [300/728]  eta: 0:05:47  loss: 0.4441 (0.4933)  acc: 78.5714 (77.9861)  time: 0.6683  data: 0.1498  max mem: 31081
Val:  [310/728]  eta: 0:05:39  loss: 0.4777 (0.4965)  acc: 73.8095 (77.7523)  time: 0.6739  data: 0.1526  max mem: 31081
Val:  [320/728]  eta: 0:05:30  loss: 0.4777 (0.4952)  acc: 75.0000 (77.7667)  time: 0.8083  data: 0.2860  max mem: 31081
Val:  [330/728]  eta: 0:05:23  loss: 0.3928 (0.4931)  acc: 77.3810 (77.8449)  time: 0.8369  data: 0.3146  max mem: 31081
Val:  [340/728]  eta: 0:05:16  loss: 0.4086 (0.4908)  acc: 78.5714 (77.9256)  time: 0.8952  data: 0.3719  max mem: 31081
Val:  [350/728]  eta: 0:05:08  loss: 0.3967 (0.4920)  acc: 79.7619 (77.9643)  time: 0.8482  data: 0.3250  max mem: 31081
Val:  [360/728]  eta: 0:04:57  loss: 0.3967 (0.4887)  acc: 80.9524 (78.0834)  time: 0.6571  data: 0.1343  max mem: 31081
Val:  [370/728]  eta: 0:04:49  loss: 0.4047 (0.4921)  acc: 80.9524 (77.9168)  time: 0.6953  data: 0.1736  max mem: 31081
Val:  [380/728]  eta: 0:04:41  loss: 0.4969 (0.4929)  acc: 76.1905 (77.8496)  time: 0.8498  data: 0.3312  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.4020 (0.4896)  acc: 80.9524 (77.9990)  time: 0.8361  data: 0.3165  max mem: 31081
Val:  [400/728]  eta: 0:04:26  loss: 0.3287 (0.4889)  acc: 83.3333 (78.0875)  time: 0.8707  data: 0.3481  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.3272 (0.4877)  acc: 80.9524 (78.1688)  time: 0.8750  data: 0.3519  max mem: 31081
Val:  [420/728]  eta: 0:04:08  loss: 0.5850 (0.4897)  acc: 75.0000 (78.0766)  time: 0.6879  data: 0.1627  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.6026 (0.4906)  acc: 71.4286 (77.9941)  time: 0.6964  data: 0.1718  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.4189 (0.4902)  acc: 79.7619 (78.0207)  time: 0.8331  data: 0.3096  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4360 (0.4919)  acc: 79.7619 (77.9432)  time: 0.8306  data: 0.3063  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.4850 (0.4934)  acc: 76.1905 (77.8716)  time: 0.8363  data: 0.3104  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.3825 (0.4909)  acc: 79.7619 (77.9825)  time: 0.7907  data: 0.2682  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.3787 (0.4893)  acc: 82.1429 (78.0368)  time: 0.6464  data: 0.1262  max mem: 31081
Val:  [490/728]  eta: 0:03:11  loss: 0.4251 (0.4887)  acc: 78.5714 (78.0647)  time: 0.6895  data: 0.1686  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.3642 (0.4871)  acc: 78.5714 (78.1104)  time: 0.8504  data: 0.3291  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3589 (0.4854)  acc: 84.5238 (78.2057)  time: 0.8926  data: 0.3699  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4685 (0.4878)  acc: 75.0000 (78.0962)  time: 0.8652  data: 0.3450  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.5197 (0.4871)  acc: 75.0000 (78.1567)  time: 0.7426  data: 0.2231  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.4847 (0.4894)  acc: 75.0000 (78.0367)  time: 0.6087  data: 0.0875  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.5013 (0.4902)  acc: 75.0000 (77.9621)  time: 0.6688  data: 0.1502  max mem: 31081
Val:  [560/728]  eta: 0:02:14  loss: 0.4375 (0.4896)  acc: 78.5714 (77.9985)  time: 0.7674  data: 0.2441  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4407 (0.4933)  acc: 77.3810 (77.9376)  time: 0.8100  data: 0.2832  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4921 (0.4937)  acc: 76.1905 (77.9342)  time: 0.8450  data: 0.3219  max mem: 31081
Val:  [590/728]  eta: 0:01:50  loss: 0.4921 (0.4945)  acc: 76.1905 (77.8584)  time: 0.7377  data: 0.2141  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4184 (0.4943)  acc: 77.3810 (77.8425)  time: 0.6629  data: 0.1413  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4560 (0.4961)  acc: 75.0000 (77.7375)  time: 0.6550  data: 0.1334  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.5005 (0.4985)  acc: 72.6190 (77.7279)  time: 0.7806  data: 0.2561  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4788 (0.4986)  acc: 75.0000 (77.7017)  time: 0.8656  data: 0.3408  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.4788 (0.4976)  acc: 78.5714 (77.7450)  time: 0.8023  data: 0.2781  max mem: 31081
Val:  [650/728]  eta: 0:01:02  loss: 0.4655 (0.4974)  acc: 78.5714 (77.7503)  time: 0.7864  data: 0.2619  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4357 (0.4984)  acc: 78.5714 (77.7412)  time: 0.7498  data: 0.2237  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4476 (0.4994)  acc: 77.3810 (77.6701)  time: 0.6786  data: 0.1537  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4174 (0.4986)  acc: 77.3810 (77.6956)  time: 0.7082  data: 0.1786  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.4174 (0.4985)  acc: 79.7619 (77.7117)  time: 0.8483  data: 0.3180  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4762 (0.4981)  acc: 77.3810 (77.6849)  time: 0.8638  data: 0.3389  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4149 (0.4980)  acc: 73.8095 (77.7142)  time: 0.8078  data: 0.2831  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4626 (0.4987)  acc: 73.8095 (77.6782)  time: 0.7649  data: 0.2481  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4974 (0.4995)  acc: 75.0000 (77.6401)  time: 0.7415  data: 0.2481  max mem: 31081
Val: Total time: 0:09:34 (0.7893 s / it)
* Acc@1 77.640 AP 0.8008550405502319 loss 0.499
Accuracy of the network on the 61096 val videos: 77.6%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.35 GB / 503.51 GB
Epoch: [11]  [  0/893]  eta: 3:29:36  lr: 0.002093  min_lr: 0.000003  loss: 0.3713 (0.3713)  class_acc: 0.7500 (0.7500)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 14.0839  data: 12.8392  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.55 GB / 503.51 GB
Epoch: [11]  [ 10/893]  eta: 0:38:51  lr: 0.002093  min_lr: 0.000003  loss: 0.3713 (0.3855)  class_acc: 0.8036 (0.8198)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6406  data: 1.1678  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.98 GB / 503.51 GB
Epoch: [11]  [ 20/893]  eta: 0:30:33  lr: 0.002092  min_lr: 0.000003  loss: 0.3552 (0.3830)  class_acc: 0.8036 (0.8189)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5010  data: 0.0006  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.42 GB / 503.51 GB
Epoch: [11]  [ 30/893]  eta: 0:27:26  lr: 0.002092  min_lr: 0.000003  loss: 0.3630 (0.3775)  class_acc: 0.8036 (0.8203)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5057  data: 0.0005  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.13 GB / 503.51 GB
Epoch: [11]  [ 40/893]  eta: 0:25:43  lr: 0.002092  min_lr: 0.000003  loss: 0.3630 (0.3761)  class_acc: 0.8214 (0.8201)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5046  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.79 GB / 503.51 GB
Epoch: [11]  [ 50/893]  eta: 0:24:33  lr: 0.002091  min_lr: 0.000003  loss: 0.3694 (0.3759)  class_acc: 0.8214 (0.8221)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4979  data: 0.0006  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.27 GB / 503.51 GB
Epoch: [11]  [ 60/893]  eta: 0:23:36  lr: 0.002091  min_lr: 0.000003  loss: 0.4048 (0.3837)  class_acc: 0.8214 (0.8220)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4754  data: 0.0005  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.29 GB / 503.51 GB
Epoch: [11]  [ 70/893]  eta: 0:22:51  lr: 0.002090  min_lr: 0.000003  loss: 0.3518 (0.3766)  class_acc: 0.8393 (0.8282)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0002  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.42 GB / 503.51 GB
Epoch: [11]  [ 80/893]  eta: 0:22:14  lr: 0.002090  min_lr: 0.000003  loss: 0.3616 (0.3783)  class_acc: 0.8393 (0.8283)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4614  data: 0.0002  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.39 GB / 503.51 GB
Epoch: [11]  [ 90/893]  eta: 0:21:42  lr: 0.002090  min_lr: 0.000003  loss: 0.3616 (0.3762)  class_acc: 0.8393 (0.8295)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.94 GB / 503.51 GB
Epoch: [11]  [100/893]  eta: 0:21:13  lr: 0.002089  min_lr: 0.000003  loss: 0.3518 (0.3750)  class_acc: 0.8214 (0.8297)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
[2025-03-11 01:42:16,874] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:42:16,874] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.35 GB / 503.51 GB
Epoch: [11]  [110/893]  eta: 0:20:47  lr: 0.002089  min_lr: 0.000003  loss: 0.3564 (0.3722)  class_acc: 0.8214 (0.8319)  loss_scale: 4096.0000 (4132.9009)  weight_decay: 0.0500 (0.0500)  time: 1.4617  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.94 GB / 503.51 GB
Epoch: [11]  [120/893]  eta: 0:20:23  lr: 0.002089  min_lr: 0.000003  loss: 0.3467 (0.3710)  class_acc: 0.8393 (0.8325)  loss_scale: 8192.0000 (4468.3636)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.0004  max mem: 31081
[2025-03-11 01:42:35,978] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 9935
[2025-03-11 01:42:35,979] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 01:42:35,979] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.75 GB / 503.51 GB
Epoch: [11]  [130/893]  eta: 0:20:02  lr: 0.002088  min_lr: 0.000003  loss: 0.3628 (0.3715)  class_acc: 0.8214 (0.8326)  loss_scale: 8192.0000 (4502.4733)  weight_decay: 0.0500 (0.0500)  time: 1.4785  data: 0.0004  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.01 GB / 503.51 GB
Epoch: [11]  [140/893]  eta: 0:19:40  lr: 0.002088  min_lr: 0.000003  loss: 0.3628 (0.3697)  class_acc: 0.8393 (0.8336)  loss_scale: 4096.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  time: 1.4769  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.05 GB / 503.51 GB
Epoch: [11]  [150/893]  eta: 0:19:20  lr: 0.002088  min_lr: 0.000003  loss: 0.3484 (0.3684)  class_acc: 0.8393 (0.8342)  loss_scale: 4096.0000 (4448.6358)  weight_decay: 0.0500 (0.0500)  time: 1.4706  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.86 GB / 503.51 GB
Epoch: [11]  [160/893]  eta: 0:19:01  lr: 0.002087  min_lr: 0.000003  loss: 0.3220 (0.3663)  class_acc: 0.8393 (0.8354)  loss_scale: 4096.0000 (4426.7329)  weight_decay: 0.0500 (0.0500)  time: 1.4792  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.14 GB / 503.51 GB
Epoch: [11]  [170/893]  eta: 0:18:41  lr: 0.002087  min_lr: 0.000003  loss: 0.3323 (0.3681)  class_acc: 0.8214 (0.8344)  loss_scale: 4096.0000 (4407.3918)  weight_decay: 0.0500 (0.0500)  time: 1.4745  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.77 GB / 503.51 GB
Epoch: [11]  [180/893]  eta: 0:18:22  lr: 0.002087  min_lr: 0.000003  loss: 0.3525 (0.3659)  class_acc: 0.8214 (0.8350)  loss_scale: 4096.0000 (4390.1878)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
[2025-03-11 01:44:10,355] [INFO] [logging.py:129:log_dist] [Rank 0] step=10000, skipped=57, lr=[2.724895319579536e-06, 2.724895319579536e-06, 4.541492199299228e-06, 4.541492199299228e-06, 7.569153665498713e-06, 7.569153665498713e-06, 1.2615256109164521e-05, 1.2615256109164521e-05, 2.102542684860754e-05, 2.102542684860754e-05, 3.5042378081012564e-05, 3.5042378081012564e-05, 5.8403963468354275e-05, 5.8403963468354275e-05, 9.733993911392379e-05, 9.733993911392379e-05, 0.00016223323185653965, 0.00016223323185653965, 0.00027038871976089947, 0.00027038871976089947, 0.00045064786626816574, 0.00045064786626816574, 0.0007510797771136097, 0.0007510797771136097, 0.0012517996285226827, 0.0012517996285226827, 0.0020863327142044714, 0.0020863327142044714], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 01:44:10,356] [INFO] [timer.py:264:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=61.0457033345899, CurrSamplesPerSec=59.043709332543784, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.05 GB / 503.51 GB
Epoch: [11]  [190/893]  eta: 0:18:04  lr: 0.002086  min_lr: 0.000003  loss: 0.3525 (0.3675)  class_acc: 0.8393 (0.8341)  loss_scale: 4096.0000 (4374.7853)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0003  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.33 GB / 503.51 GB
Epoch: [11]  [200/893]  eta: 0:17:46  lr: 0.002086  min_lr: 0.000003  loss: 0.3589 (0.3666)  class_acc: 0.8393 (0.8339)  loss_scale: 4096.0000 (4360.9154)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0004  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.73 GB / 503.51 GB
Epoch: [11]  [210/893]  eta: 0:17:28  lr: 0.002086  min_lr: 0.000003  loss: 0.3513 (0.3670)  class_acc: 0.8393 (0.8341)  loss_scale: 4096.0000 (4348.3602)  weight_decay: 0.0500 (0.0500)  time: 1.4548  data: 0.0004  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.53 GB / 503.51 GB
Epoch: [11]  [220/893]  eta: 0:17:10  lr: 0.002085  min_lr: 0.000003  loss: 0.3840 (0.3683)  class_acc: 0.8214 (0.8332)  loss_scale: 4096.0000 (4336.9412)  weight_decay: 0.0500 (0.0500)  time: 1.4520  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.55 GB / 503.51 GB
Epoch: [11]  [230/893]  eta: 0:16:53  lr: 0.002085  min_lr: 0.000003  loss: 0.3354 (0.3666)  class_acc: 0.8214 (0.8343)  loss_scale: 4096.0000 (4326.5108)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.98 GB / 503.51 GB
Epoch: [11]  [240/893]  eta: 0:16:36  lr: 0.002084  min_lr: 0.000003  loss: 0.3315 (0.3655)  class_acc: 0.8393 (0.8347)  loss_scale: 4096.0000 (4316.9461)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.67 GB / 503.51 GB
Epoch: [11]  [250/893]  eta: 0:16:19  lr: 0.002084  min_lr: 0.000003  loss: 0.3298 (0.3642)  class_acc: 0.8393 (0.8357)  loss_scale: 4096.0000 (4308.1434)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0003  max mem: 31081
[2025-03-11 01:45:45,262] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:45:45,262] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.36 GB / 503.51 GB
Epoch: [11]  [260/893]  eta: 0:16:02  lr: 0.002084  min_lr: 0.000003  loss: 0.3145 (0.3644)  class_acc: 0.8571 (0.8362)  loss_scale: 4096.0000 (4441.2567)  weight_decay: 0.0500 (0.0500)  time: 1.4593  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.58 GB / 503.51 GB
Epoch: [11]  [270/893]  eta: 0:15:46  lr: 0.002083  min_lr: 0.000003  loss: 0.3552 (0.3649)  class_acc: 0.8393 (0.8362)  loss_scale: 8192.0000 (4579.6605)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0002  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.36 GB / 503.51 GB
Epoch: [11]  [280/893]  eta: 0:15:29  lr: 0.002083  min_lr: 0.000003  loss: 0.3494 (0.3643)  class_acc: 0.8214 (0.8362)  loss_scale: 8192.0000 (4708.2135)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.82 GB / 503.51 GB
Epoch: [11]  [290/893]  eta: 0:15:13  lr: 0.002083  min_lr: 0.000003  loss: 0.3486 (0.3640)  class_acc: 0.8393 (0.8366)  loss_scale: 8192.0000 (4827.9313)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.31 GB / 503.51 GB
Epoch: [11]  [300/893]  eta: 0:14:57  lr: 0.002082  min_lr: 0.000003  loss: 0.3713 (0.3649)  class_acc: 0.8393 (0.8358)  loss_scale: 8192.0000 (4939.6944)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0002  max mem: 31081
[2025-03-11 01:47:04,330] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 10118
[2025-03-11 01:47:04,330] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 01:47:04,330] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.60 GB / 503.51 GB
Epoch: [11]  [310/893]  eta: 0:14:41  lr: 0.002082  min_lr: 0.000003  loss: 0.3689 (0.3653)  class_acc: 0.8214 (0.8359)  loss_scale: 8192.0000 (4978.4180)  weight_decay: 0.0500 (0.0500)  time: 1.4590  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.05 GB / 503.51 GB
Epoch: [11]  [320/893]  eta: 0:14:25  lr: 0.002081  min_lr: 0.000003  loss: 0.3347 (0.3658)  class_acc: 0.8393 (0.8356)  loss_scale: 4096.0000 (4950.9283)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0003  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.90 GB / 503.51 GB
Epoch: [11]  [330/893]  eta: 0:14:09  lr: 0.002081  min_lr: 0.000003  loss: 0.3352 (0.3653)  class_acc: 0.8214 (0.8358)  loss_scale: 4096.0000 (4925.0997)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.03 GB / 503.51 GB
Epoch: [11]  [340/893]  eta: 0:13:53  lr: 0.002081  min_lr: 0.000003  loss: 0.3279 (0.3642)  class_acc: 0.8393 (0.8363)  loss_scale: 4096.0000 (4900.7859)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.79 GB / 503.51 GB
Epoch: [11]  [350/893]  eta: 0:13:38  lr: 0.002080  min_lr: 0.000003  loss: 0.3364 (0.3639)  class_acc: 0.8393 (0.8365)  loss_scale: 4096.0000 (4877.8575)  weight_decay: 0.0500 (0.0500)  time: 1.4712  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.34 GB / 503.51 GB
Epoch: [11]  [360/893]  eta: 0:13:22  lr: 0.002080  min_lr: 0.000003  loss: 0.3669 (0.3639)  class_acc: 0.8393 (0.8369)  loss_scale: 4096.0000 (4856.1994)  weight_decay: 0.0500 (0.0500)  time: 1.4752  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.08 GB / 503.51 GB
Epoch: [11]  [370/893]  eta: 0:13:06  lr: 0.002080  min_lr: 0.000003  loss: 0.3445 (0.3636)  class_acc: 0.8393 (0.8367)  loss_scale: 4096.0000 (4835.7089)  weight_decay: 0.0500 (0.0500)  time: 1.4699  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.18 GB / 503.51 GB
Epoch: [11]  [380/893]  eta: 0:12:51  lr: 0.002079  min_lr: 0.000003  loss: 0.3369 (0.3635)  class_acc: 0.8393 (0.8370)  loss_scale: 4096.0000 (4816.2940)  weight_decay: 0.0500 (0.0500)  time: 1.4706  data: 0.0002  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.89 GB / 503.51 GB
Epoch: [11]  [390/893]  eta: 0:12:36  lr: 0.002079  min_lr: 0.000003  loss: 0.3450 (0.3629)  class_acc: 0.8393 (0.8374)  loss_scale: 4096.0000 (4797.8721)  weight_decay: 0.0500 (0.0500)  time: 1.4741  data: 0.0003  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.66 GB / 503.51 GB
Epoch: [11]  [400/893]  eta: 0:12:20  lr: 0.002079  min_lr: 0.000003  loss: 0.3909 (0.3634)  class_acc: 0.8214 (0.8368)  loss_scale: 4096.0000 (4780.3691)  weight_decay: 0.0500 (0.0500)  time: 1.4730  data: 0.0003  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.02 GB / 503.51 GB
Epoch: [11]  [410/893]  eta: 0:12:05  lr: 0.002078  min_lr: 0.000003  loss: 0.3730 (0.3627)  class_acc: 0.8214 (0.8370)  loss_scale: 4096.0000 (4763.7178)  weight_decay: 0.0500 (0.0500)  time: 1.4729  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.18 GB / 503.51 GB
Epoch: [11]  [420/893]  eta: 0:11:49  lr: 0.002078  min_lr: 0.000003  loss: 0.3528 (0.3627)  class_acc: 0.8393 (0.8367)  loss_scale: 4096.0000 (4747.8575)  weight_decay: 0.0500 (0.0500)  time: 1.4713  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.65 GB / 503.51 GB
Epoch: [11]  [430/893]  eta: 0:11:34  lr: 0.002077  min_lr: 0.000003  loss: 0.3628 (0.3628)  class_acc: 0.8393 (0.8366)  loss_scale: 4096.0000 (4732.7332)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0003  max mem: 31081
[2025-03-11 01:50:13,946] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:50:13,946] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.39 GB / 503.51 GB
Epoch: [11]  [440/893]  eta: 0:11:19  lr: 0.002077  min_lr: 0.000003  loss: 0.3630 (0.3624)  class_acc: 0.8393 (0.8369)  loss_scale: 4096.0000 (4774.0227)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0002  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.22 GB / 503.51 GB
Epoch: [11]  [450/893]  eta: 0:11:03  lr: 0.002077  min_lr: 0.000003  loss: 0.3259 (0.3623)  class_acc: 0.8393 (0.8368)  loss_scale: 8192.0000 (4849.8093)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.93 GB / 503.51 GB
Epoch: [11]  [460/893]  eta: 0:10:48  lr: 0.002076  min_lr: 0.000003  loss: 0.3308 (0.3624)  class_acc: 0.8393 (0.8366)  loss_scale: 8192.0000 (4922.3080)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0003  max mem: 31081
[2025-03-11 01:50:56,527] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 10276
[2025-03-11 01:50:56,527] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 01:50:56,527] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.21 GB / 503.51 GB
Epoch: [11]  [470/893]  eta: 0:10:33  lr: 0.002076  min_lr: 0.000003  loss: 0.3308 (0.3623)  class_acc: 0.8571 (0.8368)  loss_scale: 8192.0000 (4930.8535)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.88 GB / 503.51 GB
Epoch: [11]  [480/893]  eta: 0:10:18  lr: 0.002076  min_lr: 0.000003  loss: 0.3696 (0.3636)  class_acc: 0.8571 (0.8366)  loss_scale: 4096.0000 (4913.4969)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.09 GB / 503.51 GB
Epoch: [11]  [490/893]  eta: 0:10:02  lr: 0.002075  min_lr: 0.000003  loss: 0.3850 (0.3641)  class_acc: 0.8393 (0.8363)  loss_scale: 4096.0000 (4896.8473)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0002  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.37 GB / 503.51 GB
Epoch: [11]  [500/893]  eta: 0:09:47  lr: 0.002075  min_lr: 0.000003  loss: 0.3560 (0.3638)  class_acc: 0.8214 (0.8365)  loss_scale: 4096.0000 (4880.8623)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0002  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.69 GB / 503.51 GB
Epoch: [11]  [510/893]  eta: 0:09:32  lr: 0.002074  min_lr: 0.000003  loss: 0.3386 (0.3633)  class_acc: 0.8393 (0.8368)  loss_scale: 4096.0000 (4865.5029)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.72 GB / 503.51 GB
Epoch: [11]  [520/893]  eta: 0:09:17  lr: 0.002074  min_lr: 0.000003  loss: 0.3481 (0.3630)  class_acc: 0.8393 (0.8369)  loss_scale: 4096.0000 (4850.7332)  weight_decay: 0.0500 (0.0500)  time: 1.4681  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.79 GB / 503.51 GB
Epoch: [11]  [530/893]  eta: 0:09:02  lr: 0.002074  min_lr: 0.000003  loss: 0.3481 (0.3628)  class_acc: 0.8571 (0.8370)  loss_scale: 4096.0000 (4836.5198)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0002  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.14 GB / 503.51 GB
Epoch: [11]  [540/893]  eta: 0:08:46  lr: 0.002073  min_lr: 0.000003  loss: 0.3577 (0.3628)  class_acc: 0.8393 (0.8369)  loss_scale: 4096.0000 (4822.8318)  weight_decay: 0.0500 (0.0500)  time: 1.4627  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.70 GB / 503.51 GB
Epoch: [11]  [550/893]  eta: 0:08:31  lr: 0.002073  min_lr: 0.000003  loss: 0.3477 (0.3630)  class_acc: 0.8393 (0.8367)  loss_scale: 4096.0000 (4809.6407)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.71 GB / 503.51 GB
Epoch: [11]  [560/893]  eta: 0:08:16  lr: 0.002073  min_lr: 0.000003  loss: 0.3469 (0.3631)  class_acc: 0.8393 (0.8368)  loss_scale: 4096.0000 (4796.9198)  weight_decay: 0.0500 (0.0500)  time: 1.4753  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.20 GB / 503.51 GB
Epoch: [11]  [570/893]  eta: 0:08:01  lr: 0.002072  min_lr: 0.000003  loss: 0.3469 (0.3630)  class_acc: 0.8214 (0.8367)  loss_scale: 4096.0000 (4784.6445)  weight_decay: 0.0500 (0.0500)  time: 1.4756  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.40 GB / 503.51 GB
Epoch: [11]  [580/893]  eta: 0:07:46  lr: 0.002072  min_lr: 0.000003  loss: 0.3032 (0.3617)  class_acc: 0.8571 (0.8371)  loss_scale: 4096.0000 (4772.7917)  weight_decay: 0.0500 (0.0500)  time: 1.4693  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.49 GB / 503.51 GB
Epoch: [11]  [590/893]  eta: 0:07:31  lr: 0.002071  min_lr: 0.000003  loss: 0.2966 (0.3616)  class_acc: 0.8571 (0.8374)  loss_scale: 4096.0000 (4761.3401)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0003  max mem: 31081
[2025-03-11 01:54:05,687] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:54:05,687] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.16 GB / 503.51 GB
Epoch: [11]  [600/893]  eta: 0:07:16  lr: 0.002071  min_lr: 0.000003  loss: 0.3311 (0.3614)  class_acc: 0.8393 (0.8374)  loss_scale: 4096.0000 (4804.7920)  weight_decay: 0.0500 (0.0500)  time: 1.4594  data: 0.0002  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.02 GB / 503.51 GB
Epoch: [11]  [610/893]  eta: 0:07:01  lr: 0.002071  min_lr: 0.000003  loss: 0.3198 (0.3607)  class_acc: 0.8571 (0.8378)  loss_scale: 8192.0000 (4860.2291)  weight_decay: 0.0500 (0.0500)  time: 1.4597  data: 0.0002  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.42 GB / 503.51 GB
Epoch: [11]  [620/893]  eta: 0:06:46  lr: 0.002070  min_lr: 0.000003  loss: 0.3286 (0.3604)  class_acc: 0.8750 (0.8382)  loss_scale: 8192.0000 (4913.8808)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0002  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.58 GB / 503.51 GB
Epoch: [11]  [630/893]  eta: 0:06:31  lr: 0.002070  min_lr: 0.000003  loss: 0.3496 (0.3606)  class_acc: 0.8571 (0.8381)  loss_scale: 8192.0000 (4965.8320)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0002  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.53 GB / 503.51 GB
Epoch: [11]  [640/893]  eta: 0:06:16  lr: 0.002069  min_lr: 0.000003  loss: 0.3308 (0.3600)  class_acc: 0.8571 (0.8385)  loss_scale: 8192.0000 (5016.1622)  weight_decay: 0.0500 (0.0500)  time: 1.4567  data: 0.0002  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.47 GB / 503.51 GB
Epoch: [11]  [650/893]  eta: 0:06:01  lr: 0.002069  min_lr: 0.000003  loss: 0.3096 (0.3599)  class_acc: 0.8571 (0.8386)  loss_scale: 8192.0000 (5064.9462)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0002  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.92 GB / 503.51 GB
Epoch: [11]  [660/893]  eta: 0:05:46  lr: 0.002069  min_lr: 0.000003  loss: 0.3213 (0.3599)  class_acc: 0.8393 (0.8385)  loss_scale: 8192.0000 (5112.2542)  weight_decay: 0.0500 (0.0500)  time: 1.4721  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.58 GB / 503.51 GB
Epoch: [11]  [670/893]  eta: 0:05:31  lr: 0.002068  min_lr: 0.000003  loss: 0.3423 (0.3596)  class_acc: 0.8393 (0.8389)  loss_scale: 8192.0000 (5158.1520)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.12 GB / 503.51 GB
Epoch: [11]  [680/893]  eta: 0:05:16  lr: 0.002068  min_lr: 0.000003  loss: 0.3423 (0.3590)  class_acc: 0.8571 (0.8392)  loss_scale: 8192.0000 (5202.7019)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
[2025-03-11 01:56:14,564] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 10493
[2025-03-11 01:56:14,564] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 01:56:14,564] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.92 GB / 503.51 GB
Epoch: [11]  [690/893]  eta: 0:05:01  lr: 0.002067  min_lr: 0.000003  loss: 0.3469 (0.3587)  class_acc: 0.8393 (0.8392)  loss_scale: 4096.0000 (5186.6860)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0004  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.12 GB / 503.51 GB
Epoch: [11]  [700/893]  eta: 0:04:47  lr: 0.002067  min_lr: 0.000003  loss: 0.3494 (0.3588)  class_acc: 0.8393 (0.8390)  loss_scale: 4096.0000 (5171.1270)  weight_decay: 0.0500 (0.0500)  time: 1.4751  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.53 GB / 503.51 GB
Epoch: [11]  [710/893]  eta: 0:04:32  lr: 0.002067  min_lr: 0.000003  loss: 0.3494 (0.3588)  class_acc: 0.8393 (0.8392)  loss_scale: 4096.0000 (5156.0056)  weight_decay: 0.0500 (0.0500)  time: 1.4783  data: 0.0002  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.00 GB / 503.51 GB
Epoch: [11]  [720/893]  eta: 0:04:17  lr: 0.002066  min_lr: 0.000003  loss: 0.3506 (0.3587)  class_acc: 0.8393 (0.8392)  loss_scale: 4096.0000 (5141.3037)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0002  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.42 GB / 503.51 GB
Epoch: [11]  [730/893]  eta: 0:04:02  lr: 0.002066  min_lr: 0.000003  loss: 0.3413 (0.3586)  class_acc: 0.8393 (0.8391)  loss_scale: 4096.0000 (5127.0041)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.77 GB / 503.51 GB
Epoch: [11]  [740/893]  eta: 0:03:47  lr: 0.002066  min_lr: 0.000003  loss: 0.3535 (0.3590)  class_acc: 0.8393 (0.8391)  loss_scale: 4096.0000 (5113.0904)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.14 GB / 503.51 GB
Epoch: [11]  [750/893]  eta: 0:03:32  lr: 0.002065  min_lr: 0.000003  loss: 0.3335 (0.3584)  class_acc: 0.8571 (0.8395)  loss_scale: 4096.0000 (5099.5473)  weight_decay: 0.0500 (0.0500)  time: 1.4744  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.75 GB / 503.51 GB
Epoch: [11]  [760/893]  eta: 0:03:17  lr: 0.002065  min_lr: 0.000003  loss: 0.2915 (0.3579)  class_acc: 0.8750 (0.8397)  loss_scale: 4096.0000 (5086.3601)  weight_decay: 0.0500 (0.0500)  time: 1.4747  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.59 GB / 503.51 GB
Epoch: [11]  [770/893]  eta: 0:03:02  lr: 0.002064  min_lr: 0.000003  loss: 0.3635 (0.3589)  class_acc: 0.8214 (0.8392)  loss_scale: 4096.0000 (5073.5149)  weight_decay: 0.0500 (0.0500)  time: 1.4716  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.75 GB / 503.51 GB
Epoch: [11]  [780/893]  eta: 0:02:47  lr: 0.002064  min_lr: 0.000003  loss: 0.4192 (0.3596)  class_acc: 0.8036 (0.8390)  loss_scale: 4096.0000 (5060.9987)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.31 GB / 503.51 GB
Epoch: [11]  [790/893]  eta: 0:02:32  lr: 0.002064  min_lr: 0.000003  loss: 0.3533 (0.3591)  class_acc: 0.8393 (0.8392)  loss_scale: 4096.0000 (5048.7990)  weight_decay: 0.0500 (0.0500)  time: 1.4644  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.81 GB / 503.51 GB
Epoch: [11]  [800/893]  eta: 0:02:18  lr: 0.002063  min_lr: 0.000003  loss: 0.3042 (0.3590)  class_acc: 0.8393 (0.8391)  loss_scale: 4096.0000 (5036.9039)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
[2025-03-11 01:59:24,235] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 01:59:24,235] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.92 GB / 503.51 GB
Epoch: [11]  [810/893]  eta: 0:02:03  lr: 0.002063  min_lr: 0.000003  loss: 0.3320 (0.3586)  class_acc: 0.8571 (0.8393)  loss_scale: 4096.0000 (5030.3527)  weight_decay: 0.0500 (0.0500)  time: 1.4725  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.61 GB / 503.51 GB
Epoch: [11]  [820/893]  eta: 0:01:48  lr: 0.002062  min_lr: 0.000003  loss: 0.3582 (0.3587)  class_acc: 0.8393 (0.8393)  loss_scale: 8192.0000 (5068.8624)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.63 GB / 503.51 GB
Epoch: [11]  [830/893]  eta: 0:01:33  lr: 0.002062  min_lr: 0.000003  loss: 0.3582 (0.3589)  class_acc: 0.8214 (0.8391)  loss_scale: 8192.0000 (5106.4452)  weight_decay: 0.0500 (0.0500)  time: 1.4743  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.92 GB / 503.51 GB
Epoch: [11]  [840/893]  eta: 0:01:18  lr: 0.002062  min_lr: 0.000003  loss: 0.3430 (0.3593)  class_acc: 0.8214 (0.8390)  loss_scale: 8192.0000 (5143.1344)  weight_decay: 0.0500 (0.0500)  time: 1.4728  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.00 GB / 503.51 GB
Epoch: [11]  [850/893]  eta: 0:01:03  lr: 0.002061  min_lr: 0.000003  loss: 0.3616 (0.3592)  class_acc: 0.8214 (0.8391)  loss_scale: 8192.0000 (5178.9612)  weight_decay: 0.0500 (0.0500)  time: 1.4608  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.98 GB / 503.51 GB
Epoch: [11]  [860/893]  eta: 0:00:48  lr: 0.002061  min_lr: 0.000003  loss: 0.3616 (0.3591)  class_acc: 0.8571 (0.8392)  loss_scale: 8192.0000 (5213.9559)  weight_decay: 0.0500 (0.0500)  time: 1.4443  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.02 GB / 503.51 GB
Epoch: [11]  [870/893]  eta: 0:00:34  lr: 0.002060  min_lr: 0.000003  loss: 0.3330 (0.3588)  class_acc: 0.8571 (0.8394)  loss_scale: 8192.0000 (5248.1470)  weight_decay: 0.0500 (0.0500)  time: 1.4372  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.00 GB / 503.51 GB
Epoch: [11]  [880/893]  eta: 0:00:19  lr: 0.002060  min_lr: 0.000003  loss: 0.3330 (0.3591)  class_acc: 0.8393 (0.8394)  loss_scale: 8192.0000 (5281.5619)  weight_decay: 0.0500 (0.0500)  time: 1.4410  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.02 GB / 503.51 GB
Epoch: [11]  [890/893]  eta: 0:00:04  lr: 0.002060  min_lr: 0.000003  loss: 0.3306 (0.3589)  class_acc: 0.8393 (0.8394)  loss_scale: 8192.0000 (5314.2267)  weight_decay: 0.0500 (0.0500)  time: 1.4428  data: 0.0002  max mem: 31081
Epoch: [11]  [892/893]  eta: 0:00:01  lr: 0.002060  min_lr: 0.000003  loss: 0.3215 (0.3588)  class_acc: 0.8393 (0.8395)  loss_scale: 8192.0000 (5317.4529)  weight_decay: 0.0500 (0.0500)  time: 1.3927  data: 0.0002  max mem: 31081
Epoch: [11] Total time: 0:22:02 (1.4810 s / it)
Averaged stats: lr: 0.002060  min_lr: 0.000003  loss: 0.3215 (0.3588)  class_acc: 0.8393 (0.8395)  loss_scale: 8192.0000 (5317.4529)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:15:14  loss: 0.5120 (0.5120)  acc: 77.3810 (77.3810)  time: 11.1464  data: 10.6011  max mem: 31081
Val:  [ 10/728]  eta: 0:18:24  loss: 0.4505 (0.4457)  acc: 77.3810 (79.2208)  time: 1.5388  data: 1.0126  max mem: 31081
Val:  [ 20/728]  eta: 0:14:09  loss: 0.3877 (0.4538)  acc: 80.9524 (79.3651)  time: 0.7029  data: 0.1818  max mem: 31081
Val:  [ 30/728]  eta: 0:12:45  loss: 0.3785 (0.4622)  acc: 79.7619 (78.4178)  time: 0.8529  data: 0.3330  max mem: 31081
Val:  [ 40/728]  eta: 0:11:52  loss: 0.4375 (0.4800)  acc: 77.3810 (77.7294)  time: 0.8621  data: 0.3380  max mem: 31081
Val:  [ 50/728]  eta: 0:11:14  loss: 0.4859 (0.4595)  acc: 78.5714 (78.8749)  time: 0.8364  data: 0.3137  max mem: 31081
Val:  [ 60/728]  eta: 0:10:21  loss: 0.4196 (0.4656)  acc: 79.7619 (78.7861)  time: 0.7155  data: 0.1928  max mem: 31081
Val:  [ 70/728]  eta: 0:09:52  loss: 0.4196 (0.4816)  acc: 79.7619 (77.9175)  time: 0.6625  data: 0.1386  max mem: 31081
Val:  [ 80/728]  eta: 0:09:36  loss: 0.4556 (0.4869)  acc: 78.5714 (77.8366)  time: 0.7631  data: 0.2424  max mem: 31081
Val:  [ 90/728]  eta: 0:09:21  loss: 0.4757 (0.5066)  acc: 78.5714 (77.3679)  time: 0.8096  data: 0.2885  max mem: 31081
Val:  [100/728]  eta: 0:09:11  loss: 0.4547 (0.4935)  acc: 78.5714 (77.8760)  time: 0.8313  data: 0.3102  max mem: 31081
Val:  [110/728]  eta: 0:09:02  loss: 0.3777 (0.4948)  acc: 80.9524 (77.7778)  time: 0.8652  data: 0.3440  max mem: 31081
Val:  [120/728]  eta: 0:08:45  loss: 0.3818 (0.4865)  acc: 82.1429 (78.2468)  time: 0.7997  data: 0.2758  max mem: 31081
Val:  [130/728]  eta: 0:08:27  loss: 0.3818 (0.4903)  acc: 82.1429 (78.2170)  time: 0.6819  data: 0.1571  max mem: 31081
Val:  [140/728]  eta: 0:08:17  loss: 0.4152 (0.4914)  acc: 80.9524 (78.1155)  time: 0.7275  data: 0.2049  max mem: 31081
Val:  [150/728]  eta: 0:08:04  loss: 0.4152 (0.4940)  acc: 80.9524 (78.1615)  time: 0.7769  data: 0.2568  max mem: 31081
Val:  [160/728]  eta: 0:07:54  loss: 0.5266 (0.4999)  acc: 78.5714 (77.9947)  time: 0.7671  data: 0.2441  max mem: 31081
Val:  [170/728]  eta: 0:07:45  loss: 0.5217 (0.5033)  acc: 77.3810 (77.9170)  time: 0.7939  data: 0.2712  max mem: 31081
Val:  [180/728]  eta: 0:07:33  loss: 0.4884 (0.5026)  acc: 78.5714 (77.9926)  time: 0.7610  data: 0.2397  max mem: 31081
Val:  [190/728]  eta: 0:07:21  loss: 0.4805 (0.5009)  acc: 77.3810 (78.0229)  time: 0.7160  data: 0.1929  max mem: 31081
Val:  [200/728]  eta: 0:07:13  loss: 0.3654 (0.4964)  acc: 82.1429 (78.1450)  time: 0.7626  data: 0.2409  max mem: 31081
Val:  [210/728]  eta: 0:07:06  loss: 0.3654 (0.4985)  acc: 80.9524 (78.1539)  time: 0.8424  data: 0.3208  max mem: 31081
Val:  [220/728]  eta: 0:06:59  loss: 0.3653 (0.4936)  acc: 80.9524 (78.2967)  time: 0.8727  data: 0.3517  max mem: 31081
Val:  [230/728]  eta: 0:06:49  loss: 0.3812 (0.4982)  acc: 80.9524 (78.1128)  time: 0.8182  data: 0.2951  max mem: 31081
Val:  [240/728]  eta: 0:06:38  loss: 0.4506 (0.4965)  acc: 79.7619 (78.2059)  time: 0.7283  data: 0.2035  max mem: 31081
Val:  [250/728]  eta: 0:06:27  loss: 0.5006 (0.4979)  acc: 78.5714 (78.1825)  time: 0.6629  data: 0.1398  max mem: 31081
Val:  [260/728]  eta: 0:06:20  loss: 0.3725 (0.4962)  acc: 82.1429 (78.3114)  time: 0.7553  data: 0.2331  max mem: 31081
Val:  [270/728]  eta: 0:06:12  loss: 0.4005 (0.4976)  acc: 82.1429 (78.1673)  time: 0.8558  data: 0.3328  max mem: 31081
Val:  [280/728]  eta: 0:06:07  loss: 0.4659 (0.4975)  acc: 77.3810 (78.1647)  time: 0.9148  data: 0.3903  max mem: 31081
Val:  [290/728]  eta: 0:05:59  loss: 0.4260 (0.4932)  acc: 79.7619 (78.3096)  time: 0.9093  data: 0.3845  max mem: 31081
Val:  [300/728]  eta: 0:05:46  loss: 0.4181 (0.4934)  acc: 79.7619 (78.3855)  time: 0.6689  data: 0.1468  max mem: 31081
Val:  [310/728]  eta: 0:05:39  loss: 0.5463 (0.4967)  acc: 77.3810 (78.2346)  time: 0.6816  data: 0.1585  max mem: 31081
Val:  [320/728]  eta: 0:05:30  loss: 0.4893 (0.4945)  acc: 76.1905 (78.2933)  time: 0.8104  data: 0.2861  max mem: 31081
Val:  [330/728]  eta: 0:05:23  loss: 0.3524 (0.4913)  acc: 82.1429 (78.4312)  time: 0.8327  data: 0.3115  max mem: 31081
Val:  [340/728]  eta: 0:05:16  loss: 0.3517 (0.4877)  acc: 83.3333 (78.5435)  time: 0.8972  data: 0.3761  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3545 (0.4876)  acc: 82.1429 (78.6155)  time: 0.8506  data: 0.3240  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.3778 (0.4851)  acc: 80.9524 (78.7264)  time: 0.6557  data: 0.1319  max mem: 31081
Val:  [370/728]  eta: 0:04:49  loss: 0.4271 (0.4868)  acc: 80.9524 (78.6228)  time: 0.7053  data: 0.1857  max mem: 31081
Val:  [380/728]  eta: 0:04:41  loss: 0.4539 (0.4889)  acc: 73.8095 (78.4933)  time: 0.8688  data: 0.3485  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.3740 (0.4853)  acc: 78.5714 (78.6201)  time: 0.8371  data: 0.3176  max mem: 31081
Val:  [400/728]  eta: 0:04:26  loss: 0.3258 (0.4844)  acc: 82.1429 (78.7021)  time: 0.8630  data: 0.3400  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.3803 (0.4845)  acc: 82.1429 (78.7307)  time: 0.8725  data: 0.3495  max mem: 31081
Val:  [420/728]  eta: 0:04:08  loss: 0.5079 (0.4867)  acc: 78.5714 (78.6647)  time: 0.6858  data: 0.1643  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.5079 (0.4856)  acc: 78.5714 (78.6709)  time: 0.6954  data: 0.1718  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.3900 (0.4845)  acc: 82.1429 (78.7010)  time: 0.8321  data: 0.3076  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4173 (0.4840)  acc: 80.9524 (78.7351)  time: 0.8274  data: 0.3038  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.4355 (0.4866)  acc: 78.5714 (78.6463)  time: 0.8309  data: 0.3075  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.3821 (0.4842)  acc: 78.5714 (78.7433)  time: 0.7906  data: 0.2681  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.3821 (0.4827)  acc: 80.9524 (78.8140)  time: 0.6522  data: 0.1286  max mem: 31081
Val:  [490/728]  eta: 0:03:11  loss: 0.4557 (0.4820)  acc: 77.3810 (78.8575)  time: 0.6876  data: 0.1627  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.4763 (0.4820)  acc: 76.1905 (78.8637)  time: 0.8437  data: 0.3202  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.4074 (0.4810)  acc: 80.9524 (78.8603)  time: 0.8873  data: 0.3656  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4489 (0.4834)  acc: 76.1905 (78.6811)  time: 0.8629  data: 0.3427  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.4742 (0.4827)  acc: 76.1905 (78.7216)  time: 0.7440  data: 0.2234  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.4581 (0.4838)  acc: 78.5714 (78.6947)  time: 0.6127  data: 0.0888  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.5202 (0.4856)  acc: 76.1905 (78.5758)  time: 0.6612  data: 0.1376  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.4545 (0.4856)  acc: 78.5714 (78.5778)  time: 0.7645  data: 0.2434  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4571 (0.4901)  acc: 78.5714 (78.5047)  time: 0.8135  data: 0.2931  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4836 (0.4907)  acc: 79.7619 (78.4936)  time: 0.8399  data: 0.3187  max mem: 31081
Val:  [590/728]  eta: 0:01:50  loss: 0.5045 (0.4918)  acc: 78.5714 (78.4204)  time: 0.7393  data: 0.2165  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4567 (0.4917)  acc: 78.5714 (78.4248)  time: 0.6723  data: 0.1491  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4567 (0.4936)  acc: 78.5714 (78.2987)  time: 0.6610  data: 0.1379  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.5337 (0.4965)  acc: 72.6190 (78.2417)  time: 0.8040  data: 0.2802  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.5100 (0.4956)  acc: 72.6190 (78.2243)  time: 0.8660  data: 0.3438  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.4286 (0.4947)  acc: 77.3810 (78.2427)  time: 0.7834  data: 0.2636  max mem: 31081
Val:  [650/728]  eta: 0:01:02  loss: 0.4240 (0.4939)  acc: 82.1429 (78.2971)  time: 0.7964  data: 0.2760  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4234 (0.4939)  acc: 80.9524 (78.3103)  time: 0.7627  data: 0.2389  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.5151 (0.4957)  acc: 79.7619 (78.2503)  time: 0.6798  data: 0.1554  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4790 (0.4945)  acc: 78.5714 (78.2708)  time: 0.6992  data: 0.1764  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3690 (0.4946)  acc: 79.7619 (78.3009)  time: 0.8330  data: 0.3112  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4262 (0.4941)  acc: 79.7619 (78.3082)  time: 0.8605  data: 0.3375  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4356 (0.4946)  acc: 76.1905 (78.2801)  time: 0.8222  data: 0.2992  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4374 (0.4948)  acc: 76.1905 (78.2759)  time: 0.7936  data: 0.2787  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4375 (0.4961)  acc: 76.1905 (78.2457)  time: 0.7695  data: 0.2786  max mem: 31081
Val: Total time: 0:09:35 (0.7899 s / it)
* Acc@1 78.246 AP 0.787962019443512 loss 0.496
Accuracy of the network on the 61096 val videos: 78.2%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.37 GB / 503.51 GB
Epoch: [12]  [  0/893]  eta: 3:27:10  lr: 0.002059  min_lr: 0.000003  loss: 0.4705 (0.4705)  class_acc: 0.7857 (0.7857)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 13.9201  data: 12.6126  max mem: 31081
[2025-03-11 02:11:17,247] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 10707
[2025-03-11 02:11:17,248] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 02:11:17,249] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.67 GB / 503.51 GB
Epoch: [12]  [ 10/893]  eta: 0:38:43  lr: 0.002059  min_lr: 0.000003  loss: 0.3662 (0.3743)  class_acc: 0.8393 (0.8263)  loss_scale: 4096.0000 (5213.0909)  weight_decay: 0.0500 (0.0500)  time: 2.6309  data: 1.1470  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.56 GB / 503.51 GB
Epoch: [12]  [ 20/893]  eta: 0:30:30  lr: 0.002059  min_lr: 0.000003  loss: 0.3645 (0.3748)  class_acc: 0.8393 (0.8299)  loss_scale: 4096.0000 (4681.1429)  weight_decay: 0.0500 (0.0500)  time: 1.5053  data: 0.0005  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.20 GB / 503.51 GB
Epoch: [12]  [ 30/893]  eta: 0:27:27  lr: 0.002058  min_lr: 0.000003  loss: 0.3630 (0.3646)  class_acc: 0.8393 (0.8370)  loss_scale: 4096.0000 (4492.3871)  weight_decay: 0.0500 (0.0500)  time: 1.5113  data: 0.0008  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.98 GB / 503.51 GB
Epoch: [12]  [ 40/893]  eta: 0:25:44  lr: 0.002058  min_lr: 0.000003  loss: 0.3525 (0.3587)  class_acc: 0.8571 (0.8445)  loss_scale: 4096.0000 (4395.7073)  weight_decay: 0.0500 (0.0500)  time: 1.5095  data: 0.0009  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.68 GB / 503.51 GB
Epoch: [12]  [ 50/893]  eta: 0:24:32  lr: 0.002057  min_lr: 0.000003  loss: 0.3408 (0.3566)  class_acc: 0.8571 (0.8466)  loss_scale: 4096.0000 (4336.9412)  weight_decay: 0.0500 (0.0500)  time: 1.4970  data: 0.0007  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.85 GB / 503.51 GB
Epoch: [12]  [ 60/893]  eta: 0:23:38  lr: 0.002057  min_lr: 0.000003  loss: 0.3337 (0.3533)  class_acc: 0.8571 (0.8489)  loss_scale: 4096.0000 (4297.4426)  weight_decay: 0.0500 (0.0500)  time: 1.4827  data: 0.0005  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.74 GB / 503.51 GB
Epoch: [12]  [ 70/893]  eta: 0:22:53  lr: 0.002057  min_lr: 0.000003  loss: 0.3491 (0.3540)  class_acc: 0.8393 (0.8471)  loss_scale: 4096.0000 (4269.0704)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.63 GB / 503.51 GB
Epoch: [12]  [ 80/893]  eta: 0:22:14  lr: 0.002056  min_lr: 0.000003  loss: 0.3486 (0.3542)  class_acc: 0.8393 (0.8479)  loss_scale: 4096.0000 (4247.7037)  weight_decay: 0.0500 (0.0500)  time: 1.4549  data: 0.0004  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.64 GB / 503.51 GB
Epoch: [12]  [ 90/893]  eta: 0:21:43  lr: 0.002056  min_lr: 0.000003  loss: 0.3389 (0.3519)  class_acc: 0.8571 (0.8487)  loss_scale: 4096.0000 (4231.0330)  weight_decay: 0.0500 (0.0500)  time: 1.4609  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.73 GB / 503.51 GB
Epoch: [12]  [100/893]  eta: 0:21:13  lr: 0.002055  min_lr: 0.000003  loss: 0.3164 (0.3517)  class_acc: 0.8393 (0.8485)  loss_scale: 4096.0000 (4217.6634)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.09 GB / 503.51 GB
Epoch: [12]  [110/893]  eta: 0:20:47  lr: 0.002055  min_lr: 0.000003  loss: 0.3848 (0.3535)  class_acc: 0.8393 (0.8477)  loss_scale: 4096.0000 (4206.7027)  weight_decay: 0.0500 (0.0500)  time: 1.4614  data: 0.0004  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.50 GB / 503.51 GB
Epoch: [12]  [120/893]  eta: 0:20:23  lr: 0.002055  min_lr: 0.000003  loss: 0.3176 (0.3532)  class_acc: 0.8571 (0.8478)  loss_scale: 4096.0000 (4197.5537)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0004  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.27 GB / 503.51 GB
Epoch: [12]  [130/893]  eta: 0:20:00  lr: 0.002054  min_lr: 0.000003  loss: 0.3579 (0.3552)  class_acc: 0.8214 (0.8461)  loss_scale: 4096.0000 (4189.8015)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
[2025-03-11 02:14:28,012] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 02:14:28,012] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.99 GB / 503.51 GB
Epoch: [12]  [140/893]  eta: 0:19:38  lr: 0.002054  min_lr: 0.000003  loss: 0.3933 (0.3565)  class_acc: 0.8214 (0.8457)  loss_scale: 4096.0000 (4444.5957)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0002  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.52 GB / 503.51 GB
Epoch: [12]  [150/893]  eta: 0:19:18  lr: 0.002053  min_lr: 0.000003  loss: 0.3228 (0.3548)  class_acc: 0.8571 (0.8461)  loss_scale: 8192.0000 (4692.7682)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.13 GB / 503.51 GB
Epoch: [12]  [160/893]  eta: 0:18:58  lr: 0.002053  min_lr: 0.000003  loss: 0.3108 (0.3530)  class_acc: 0.8571 (0.8467)  loss_scale: 8192.0000 (4910.1118)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.63 GB / 503.51 GB
Epoch: [12]  [170/893]  eta: 0:18:38  lr: 0.002053  min_lr: 0.000003  loss: 0.3232 (0.3526)  class_acc: 0.8571 (0.8469)  loss_scale: 8192.0000 (5102.0351)  weight_decay: 0.0500 (0.0500)  time: 1.4572  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.34 GB / 503.51 GB
Epoch: [12]  [180/893]  eta: 0:18:19  lr: 0.002052  min_lr: 0.000003  loss: 0.3689 (0.3534)  class_acc: 0.8393 (0.8471)  loss_scale: 8192.0000 (5272.7514)  weight_decay: 0.0500 (0.0500)  time: 1.4570  data: 0.0004  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.48 GB / 503.51 GB
Epoch: [12]  [190/893]  eta: 0:18:01  lr: 0.002052  min_lr: 0.000003  loss: 0.3425 (0.3540)  class_acc: 0.8393 (0.8465)  loss_scale: 8192.0000 (5425.5916)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0003  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.05 GB / 503.51 GB
Epoch: [12]  [200/893]  eta: 0:17:43  lr: 0.002051  min_lr: 0.000003  loss: 0.3398 (0.3522)  class_acc: 0.8393 (0.8472)  loss_scale: 8192.0000 (5563.2239)  weight_decay: 0.0500 (0.0500)  time: 1.4688  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.83 GB / 503.51 GB
Epoch: [12]  [210/893]  eta: 0:17:25  lr: 0.002051  min_lr: 0.000003  loss: 0.2988 (0.3498)  class_acc: 0.8571 (0.8482)  loss_scale: 8192.0000 (5687.8104)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.54 GB / 503.51 GB
Epoch: [12]  [220/893]  eta: 0:17:08  lr: 0.002051  min_lr: 0.000003  loss: 0.3320 (0.3489)  class_acc: 0.8571 (0.8486)  loss_scale: 8192.0000 (5801.1222)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.27 GB / 503.51 GB
Epoch: [12]  [230/893]  eta: 0:16:51  lr: 0.002050  min_lr: 0.000003  loss: 0.3611 (0.3507)  class_acc: 0.8393 (0.8472)  loss_scale: 8192.0000 (5904.6234)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.99 GB / 503.51 GB
Epoch: [12]  [240/893]  eta: 0:16:34  lr: 0.002050  min_lr: 0.000003  loss: 0.3792 (0.3515)  class_acc: 0.8214 (0.8466)  loss_scale: 8192.0000 (5999.5353)  weight_decay: 0.0500 (0.0500)  time: 1.4644  data: 0.0004  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.61 GB / 503.51 GB
Epoch: [12]  [250/893]  eta: 0:16:17  lr: 0.002049  min_lr: 0.000003  loss: 0.3401 (0.3502)  class_acc: 0.8214 (0.8470)  loss_scale: 8192.0000 (6086.8845)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
[2025-03-11 02:17:35,247] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 02:17:35,247] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.59 GB / 503.51 GB
Epoch: [12]  [260/893]  eta: 0:16:01  lr: 0.002049  min_lr: 0.000003  loss: 0.3491 (0.3526)  class_acc: 0.8214 (0.8459)  loss_scale: 8192.0000 (6198.9272)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0002  max mem: 31081
[2025-03-11 02:17:38,163] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 10966
[2025-03-11 02:17:38,163] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 02:17:38,163] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.29 GB / 503.51 GB
Epoch: [12]  [270/893]  eta: 0:15:44  lr: 0.002048  min_lr: 0.000003  loss: 0.3884 (0.3537)  class_acc: 0.8214 (0.8457)  loss_scale: 8192.0000 (6302.7011)  weight_decay: 0.0500 (0.0500)  time: 1.4591  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.02 GB / 503.51 GB
Epoch: [12]  [280/893]  eta: 0:15:28  lr: 0.002048  min_lr: 0.000003  loss: 0.3359 (0.3527)  class_acc: 0.8571 (0.8463)  loss_scale: 8192.0000 (6369.9359)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.16 GB / 503.51 GB
Epoch: [12]  [290/893]  eta: 0:15:12  lr: 0.002048  min_lr: 0.000003  loss: 0.3386 (0.3529)  class_acc: 0.8571 (0.8460)  loss_scale: 8192.0000 (6432.5498)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
[2025-03-11 02:18:26,390] [INFO] [logging.py:129:log_dist] [Rank 0] step=11000, skipped=62, lr=[2.6740152143928523e-06, 2.6740152143928523e-06, 4.456692023988087e-06, 4.456692023988087e-06, 7.427820039980146e-06, 7.427820039980146e-06, 1.2379700066633576e-05, 1.2379700066633576e-05, 2.0632833444389297e-05, 2.0632833444389297e-05, 3.438805574064883e-05, 3.438805574064883e-05, 5.731342623441472e-05, 5.731342623441472e-05, 9.552237705735786e-05, 9.552237705735786e-05, 0.0001592039617622631, 0.0001592039617622631, 0.0002653399362704385, 0.0002653399362704385, 0.0004422332271173975, 0.0004422332271173975, 0.000737055378528996, 0.000737055378528996, 0.00122842563088166, 0.00122842563088166, 0.0020473760514694334, 0.0020473760514694334], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 02:18:26,391] [INFO] [timer.py:264:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=61.03616461754441, CurrSamplesPerSec=61.70788661379977, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.30 GB / 503.51 GB
Epoch: [12]  [300/893]  eta: 0:14:55  lr: 0.002047  min_lr: 0.000003  loss: 0.3589 (0.3532)  class_acc: 0.8393 (0.8458)  loss_scale: 8192.0000 (6491.0033)  weight_decay: 0.0500 (0.0500)  time: 1.4593  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.19 GB / 503.51 GB
Epoch: [12]  [310/893]  eta: 0:14:39  lr: 0.002047  min_lr: 0.000003  loss: 0.3608 (0.3542)  class_acc: 0.8393 (0.8457)  loss_scale: 8192.0000 (6545.6977)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.17 GB / 503.51 GB
Epoch: [12]  [320/893]  eta: 0:14:24  lr: 0.002046  min_lr: 0.000003  loss: 0.3533 (0.3543)  class_acc: 0.8571 (0.8456)  loss_scale: 8192.0000 (6596.9844)  weight_decay: 0.0500 (0.0500)  time: 1.4710  data: 0.0003  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.79 GB / 503.51 GB
Epoch: [12]  [330/893]  eta: 0:14:08  lr: 0.002046  min_lr: 0.000003  loss: 0.3420 (0.3546)  class_acc: 0.8571 (0.8453)  loss_scale: 8192.0000 (6645.1722)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.54 GB / 503.51 GB
Epoch: [12]  [340/893]  eta: 0:13:52  lr: 0.002045  min_lr: 0.000003  loss: 0.3420 (0.3537)  class_acc: 0.8393 (0.8451)  loss_scale: 8192.0000 (6690.5337)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0003  max mem: 31081
[2025-03-11 02:19:33,884] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 11045
[2025-03-11 02:19:33,884] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 02:19:33,885] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.76 GB / 503.51 GB
Epoch: [12]  [350/893]  eta: 0:13:37  lr: 0.002045  min_lr: 0.000003  loss: 0.3450 (0.3542)  class_acc: 0.8214 (0.8439)  loss_scale: 4096.0000 (6616.6154)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.11 GB / 503.51 GB
Epoch: [12]  [360/893]  eta: 0:13:21  lr: 0.002045  min_lr: 0.000003  loss: 0.3315 (0.3531)  class_acc: 0.8214 (0.8438)  loss_scale: 4096.0000 (6546.7922)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.36 GB / 503.51 GB
Epoch: [12]  [370/893]  eta: 0:13:05  lr: 0.002044  min_lr: 0.000003  loss: 0.3171 (0.3531)  class_acc: 0.8393 (0.8441)  loss_scale: 4096.0000 (6480.7332)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0002  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.01 GB / 503.51 GB
Epoch: [12]  [380/893]  eta: 0:12:50  lr: 0.002044  min_lr: 0.000003  loss: 0.3367 (0.3529)  class_acc: 0.8571 (0.8443)  loss_scale: 4096.0000 (6418.1417)  weight_decay: 0.0500 (0.0500)  time: 1.4628  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.64 GB / 503.51 GB
Epoch: [12]  [390/893]  eta: 0:12:34  lr: 0.002043  min_lr: 0.000003  loss: 0.3511 (0.3534)  class_acc: 0.8393 (0.8440)  loss_scale: 4096.0000 (6358.7519)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.59 GB / 503.51 GB
Epoch: [12]  [400/893]  eta: 0:12:19  lr: 0.002043  min_lr: 0.000003  loss: 0.3579 (0.3534)  class_acc: 0.8571 (0.8438)  loss_scale: 4096.0000 (6302.3242)  weight_decay: 0.0500 (0.0500)  time: 1.4679  data: 0.0002  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.08 GB / 503.51 GB
Epoch: [12]  [410/893]  eta: 0:12:04  lr: 0.002043  min_lr: 0.000003  loss: 0.3545 (0.3536)  class_acc: 0.8571 (0.8439)  loss_scale: 4096.0000 (6248.6423)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.12 GB / 503.51 GB
Epoch: [12]  [420/893]  eta: 0:11:48  lr: 0.002042  min_lr: 0.000003  loss: 0.3687 (0.3545)  class_acc: 0.8214 (0.8434)  loss_scale: 4096.0000 (6197.5107)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.78 GB / 503.51 GB
Epoch: [12]  [430/893]  eta: 0:11:33  lr: 0.002042  min_lr: 0.000003  loss: 0.3801 (0.3551)  class_acc: 0.8214 (0.8429)  loss_scale: 4096.0000 (6148.7517)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0003  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.33 GB / 503.51 GB
Epoch: [12]  [440/893]  eta: 0:11:18  lr: 0.002041  min_lr: 0.000003  loss: 0.3586 (0.3542)  class_acc: 0.8393 (0.8433)  loss_scale: 4096.0000 (6102.2041)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.18 GB / 503.51 GB
Epoch: [12]  [450/893]  eta: 0:11:02  lr: 0.002041  min_lr: 0.000003  loss: 0.3296 (0.3540)  class_acc: 0.8393 (0.8434)  loss_scale: 4096.0000 (6057.7206)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.0002  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.61 GB / 503.51 GB
Epoch: [12]  [460/893]  eta: 0:10:47  lr: 0.002040  min_lr: 0.000003  loss: 0.3335 (0.3538)  class_acc: 0.8393 (0.8434)  loss_scale: 4096.0000 (6015.1670)  weight_decay: 0.0500 (0.0500)  time: 1.4673  data: 0.0002  max mem: 31081
[2025-03-11 02:22:42,897] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 02:22:42,897] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.18 GB / 503.51 GB
Epoch: [12]  [470/893]  eta: 0:10:32  lr: 0.002040  min_lr: 0.000003  loss: 0.3384 (0.3549)  class_acc: 0.8214 (0.8428)  loss_scale: 4096.0000 (5983.1168)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0003  max mem: 31081
[2025-03-11 02:22:44,408] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 11175
[2025-03-11 02:22:44,408] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 02:22:44,408] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.55 GB / 503.51 GB
Epoch: [12]  [480/893]  eta: 0:10:17  lr: 0.002040  min_lr: 0.000003  loss: 0.3752 (0.3551)  class_acc: 0.8214 (0.8430)  loss_scale: 4096.0000 (5943.8836)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.22 GB / 503.51 GB
Epoch: [12]  [490/893]  eta: 0:10:01  lr: 0.002039  min_lr: 0.000003  loss: 0.3669 (0.3555)  class_acc: 0.8393 (0.8428)  loss_scale: 4096.0000 (5906.2485)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.46 GB / 503.51 GB
Epoch: [12]  [500/893]  eta: 0:09:46  lr: 0.002039  min_lr: 0.000003  loss: 0.3340 (0.3546)  class_acc: 0.8393 (0.8432)  loss_scale: 4096.0000 (5870.1158)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.93 GB / 503.51 GB
Epoch: [12]  [510/893]  eta: 0:09:31  lr: 0.002038  min_lr: 0.000003  loss: 0.3340 (0.3547)  class_acc: 0.8393 (0.8429)  loss_scale: 4096.0000 (5835.3973)  weight_decay: 0.0500 (0.0500)  time: 1.4746  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.91 GB / 503.51 GB
Epoch: [12]  [520/893]  eta: 0:09:16  lr: 0.002038  min_lr: 0.000003  loss: 0.3494 (0.3549)  class_acc: 0.8393 (0.8429)  loss_scale: 4096.0000 (5802.0115)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0004  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.53 GB / 503.51 GB
Epoch: [12]  [530/893]  eta: 0:09:01  lr: 0.002037  min_lr: 0.000003  loss: 0.3447 (0.3547)  class_acc: 0.8393 (0.8428)  loss_scale: 4096.0000 (5769.8832)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.70 GB / 503.51 GB
Epoch: [12]  [540/893]  eta: 0:08:46  lr: 0.002037  min_lr: 0.000003  loss: 0.3333 (0.3545)  class_acc: 0.8571 (0.8427)  loss_scale: 4096.0000 (5738.9427)  weight_decay: 0.0500 (0.0500)  time: 1.4719  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.01 GB / 503.51 GB
Epoch: [12]  [550/893]  eta: 0:08:31  lr: 0.002036  min_lr: 0.000003  loss: 0.3782 (0.3556)  class_acc: 0.8393 (0.8423)  loss_scale: 4096.0000 (5709.1252)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.05 GB / 503.51 GB
Epoch: [12]  [560/893]  eta: 0:08:16  lr: 0.002036  min_lr: 0.000003  loss: 0.3782 (0.3551)  class_acc: 0.8393 (0.8426)  loss_scale: 4096.0000 (5680.3708)  weight_decay: 0.0500 (0.0500)  time: 1.4644  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.47 GB / 503.51 GB
Epoch: [12]  [570/893]  eta: 0:08:01  lr: 0.002036  min_lr: 0.000003  loss: 0.3105 (0.3546)  class_acc: 0.8571 (0.8428)  loss_scale: 4096.0000 (5652.6235)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0005  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.73 GB / 503.51 GB
Epoch: [12]  [580/893]  eta: 0:07:46  lr: 0.002035  min_lr: 0.000003  loss: 0.3098 (0.3544)  class_acc: 0.8571 (0.8433)  loss_scale: 4096.0000 (5625.8313)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.28 GB / 503.51 GB
Epoch: [12]  [590/893]  eta: 0:07:31  lr: 0.002035  min_lr: 0.000003  loss: 0.3420 (0.3544)  class_acc: 0.8571 (0.8431)  loss_scale: 4096.0000 (5599.9459)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0002  max mem: 31081
[2025-03-11 02:25:53,900] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 02:25:53,900] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.11 GB / 503.51 GB
Epoch: [12]  [600/893]  eta: 0:07:16  lr: 0.002034  min_lr: 0.000003  loss: 0.3538 (0.3546)  class_acc: 0.8393 (0.8432)  loss_scale: 4096.0000 (5581.7371)  weight_decay: 0.0500 (0.0500)  time: 1.4725  data: 0.0002  max mem: 31081
[2025-03-11 02:26:05,717] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 11312
[2025-03-11 02:26:05,717] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 02:26:05,717] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.93 GB / 503.51 GB
Epoch: [12]  [610/893]  eta: 0:07:01  lr: 0.002034  min_lr: 0.000003  loss: 0.3606 (0.3546)  class_acc: 0.8393 (0.8431)  loss_scale: 4096.0000 (5604.3470)  weight_decay: 0.0500 (0.0500)  time: 1.4745  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.52 GB / 503.51 GB
Epoch: [12]  [620/893]  eta: 0:06:46  lr: 0.002033  min_lr: 0.000003  loss: 0.3579 (0.3548)  class_acc: 0.8393 (0.8431)  loss_scale: 4096.0000 (5580.0580)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.71 GB / 503.51 GB
Epoch: [12]  [630/893]  eta: 0:06:31  lr: 0.002033  min_lr: 0.000003  loss: 0.3501 (0.3546)  class_acc: 0.8214 (0.8430)  loss_scale: 4096.0000 (5556.5388)  weight_decay: 0.0500 (0.0500)  time: 1.4585  data: 0.0002  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.57 GB / 503.51 GB
Epoch: [12]  [640/893]  eta: 0:06:16  lr: 0.002033  min_lr: 0.000003  loss: 0.3416 (0.3545)  class_acc: 0.8214 (0.8432)  loss_scale: 4096.0000 (5533.7535)  weight_decay: 0.0500 (0.0500)  time: 1.4591  data: 0.0002  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.81 GB / 503.51 GB
Epoch: [12]  [650/893]  eta: 0:06:01  lr: 0.002032  min_lr: 0.000003  loss: 0.3494 (0.3546)  class_acc: 0.8393 (0.8433)  loss_scale: 4096.0000 (5511.6682)  weight_decay: 0.0500 (0.0500)  time: 1.4733  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.31 GB / 503.51 GB
Epoch: [12]  [660/893]  eta: 0:05:46  lr: 0.002032  min_lr: 0.000003  loss: 0.3445 (0.3542)  class_acc: 0.8393 (0.8435)  loss_scale: 4096.0000 (5490.2511)  weight_decay: 0.0500 (0.0500)  time: 1.4756  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.44 GB / 503.51 GB
Epoch: [12]  [670/893]  eta: 0:05:31  lr: 0.002031  min_lr: 0.000003  loss: 0.3555 (0.3544)  class_acc: 0.8393 (0.8433)  loss_scale: 4096.0000 (5469.4724)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0002  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.85 GB / 503.51 GB
Epoch: [12]  [680/893]  eta: 0:05:16  lr: 0.002031  min_lr: 0.000003  loss: 0.3618 (0.3543)  class_acc: 0.8393 (0.8432)  loss_scale: 4096.0000 (5449.3040)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.63 GB / 503.51 GB
Epoch: [12]  [690/893]  eta: 0:05:01  lr: 0.002030  min_lr: 0.000003  loss: 0.3198 (0.3537)  class_acc: 0.8393 (0.8435)  loss_scale: 4096.0000 (5429.7192)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.20 GB / 503.51 GB
Epoch: [12]  [700/893]  eta: 0:04:46  lr: 0.002030  min_lr: 0.000003  loss: 0.3171 (0.3535)  class_acc: 0.8571 (0.8436)  loss_scale: 4096.0000 (5410.6933)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.73 GB / 503.51 GB
Epoch: [12]  [710/893]  eta: 0:04:31  lr: 0.002029  min_lr: 0.000003  loss: 0.3203 (0.3529)  class_acc: 0.8571 (0.8439)  loss_scale: 4096.0000 (5392.2025)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0004  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.64 GB / 503.51 GB
Epoch: [12]  [720/893]  eta: 0:04:16  lr: 0.002029  min_lr: 0.000003  loss: 0.3206 (0.3526)  class_acc: 0.8393 (0.8440)  loss_scale: 4096.0000 (5374.2247)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.99 GB / 503.51 GB
Epoch: [12]  [730/893]  eta: 0:04:02  lr: 0.002029  min_lr: 0.000003  loss: 0.3372 (0.3525)  class_acc: 0.8393 (0.8441)  loss_scale: 4096.0000 (5356.7387)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0003  max mem: 31081
[2025-03-11 02:29:14,735] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 02:29:14,735] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.90 GB / 503.51 GB
Epoch: [12]  [740/893]  eta: 0:03:47  lr: 0.002028  min_lr: 0.000003  loss: 0.3386 (0.3526)  class_acc: 0.8393 (0.8440)  loss_scale: 4096.0000 (5361.8354)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.66 GB / 503.51 GB
Epoch: [12]  [750/893]  eta: 0:03:32  lr: 0.002028  min_lr: 0.000003  loss: 0.3350 (0.3525)  class_acc: 0.8393 (0.8441)  loss_scale: 8192.0000 (5399.5206)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0003  max mem: 31081
[2025-03-11 02:29:35,292] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 11455
[2025-03-11 02:29:35,292] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 02:29:35,292] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.85 GB / 503.51 GB
Epoch: [12]  [760/893]  eta: 0:03:17  lr: 0.002027  min_lr: 0.000003  loss: 0.3313 (0.3523)  class_acc: 0.8393 (0.8441)  loss_scale: 4096.0000 (5382.3916)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.68 GB / 503.51 GB
Epoch: [12]  [770/893]  eta: 0:03:02  lr: 0.002027  min_lr: 0.000003  loss: 0.3333 (0.3522)  class_acc: 0.8393 (0.8443)  loss_scale: 4096.0000 (5365.7069)  weight_decay: 0.0500 (0.0500)  time: 1.4569  data: 0.0002  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.92 GB / 503.51 GB
Epoch: [12]  [780/893]  eta: 0:02:47  lr: 0.002026  min_lr: 0.000003  loss: 0.3364 (0.3525)  class_acc: 0.8393 (0.8442)  loss_scale: 4096.0000 (5349.4494)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.36 GB / 503.51 GB
Epoch: [12]  [790/893]  eta: 0:02:32  lr: 0.002026  min_lr: 0.000003  loss: 0.3267 (0.3521)  class_acc: 0.8571 (0.8444)  loss_scale: 4096.0000 (5333.6030)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0004  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.59 GB / 503.51 GB
Epoch: [12]  [800/893]  eta: 0:02:17  lr: 0.002025  min_lr: 0.000003  loss: 0.3425 (0.3525)  class_acc: 0.8393 (0.8443)  loss_scale: 4096.0000 (5318.1523)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.93 GB / 503.51 GB
Epoch: [12]  [810/893]  eta: 0:02:03  lr: 0.002025  min_lr: 0.000003  loss: 0.3469 (0.3525)  class_acc: 0.8393 (0.8440)  loss_scale: 4096.0000 (5303.0826)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.67 GB / 503.51 GB
Epoch: [12]  [820/893]  eta: 0:01:48  lr: 0.002025  min_lr: 0.000003  loss: 0.3228 (0.3520)  class_acc: 0.8571 (0.8444)  loss_scale: 4096.0000 (5288.3800)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.0004  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.44 GB / 503.51 GB
Epoch: [12]  [830/893]  eta: 0:01:33  lr: 0.002024  min_lr: 0.000003  loss: 0.3384 (0.3519)  class_acc: 0.8571 (0.8446)  loss_scale: 4096.0000 (5274.0313)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.99 GB / 503.51 GB
Epoch: [12]  [840/893]  eta: 0:01:18  lr: 0.002024  min_lr: 0.000003  loss: 0.3374 (0.3515)  class_acc: 0.8571 (0.8447)  loss_scale: 4096.0000 (5260.0238)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.26 GB / 503.51 GB
Epoch: [12]  [850/893]  eta: 0:01:03  lr: 0.002023  min_lr: 0.000003  loss: 0.3245 (0.3511)  class_acc: 0.8571 (0.8447)  loss_scale: 4096.0000 (5246.3455)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.81 GB / 503.51 GB
Epoch: [12]  [860/893]  eta: 0:00:48  lr: 0.002023  min_lr: 0.000003  loss: 0.3501 (0.3515)  class_acc: 0.8393 (0.8446)  loss_scale: 4096.0000 (5232.9849)  weight_decay: 0.0500 (0.0500)  time: 1.4577  data: 0.0003  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.72 GB / 503.51 GB
Epoch: [12]  [870/893]  eta: 0:00:34  lr: 0.002022  min_lr: 0.000003  loss: 0.3611 (0.3518)  class_acc: 0.8214 (0.8444)  loss_scale: 4096.0000 (5219.9311)  weight_decay: 0.0500 (0.0500)  time: 1.4506  data: 0.0002  max mem: 31081
[2025-03-11 02:32:43,933] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 02:32:43,934] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.55 GB / 503.51 GB
Epoch: [12]  [880/893]  eta: 0:00:19  lr: 0.002022  min_lr: 0.000003  loss: 0.3435 (0.3514)  class_acc: 0.8571 (0.8447)  loss_scale: 4096.0000 (5211.8229)  weight_decay: 0.0500 (0.0500)  time: 1.4539  data: 0.0002  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.62 GB / 503.51 GB
Epoch: [12]  [890/893]  eta: 0:00:04  lr: 0.002021  min_lr: 0.000003  loss: 0.3120 (0.3512)  class_acc: 0.8571 (0.8448)  loss_scale: 8192.0000 (5245.2705)  weight_decay: 0.0500 (0.0500)  time: 1.4528  data: 0.0002  max mem: 31081
Epoch: [12]  [892/893]  eta: 0:00:01  lr: 0.002021  min_lr: 0.000003  loss: 0.3352 (0.3513)  class_acc: 0.8571 (0.8447)  loss_scale: 8192.0000 (5248.5740)  weight_decay: 0.0500 (0.0500)  time: 1.3956  data: 0.0001  max mem: 31081
Epoch: [12] Total time: 0:22:01 (1.4798 s / it)
Averaged stats: lr: 0.002021  min_lr: 0.000003  loss: 0.3352 (0.3513)  class_acc: 0.8571 (0.8447)  loss_scale: 8192.0000 (5248.5740)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:14:42  loss: 0.8590 (0.8590)  acc: 70.2381 (70.2381)  time: 11.1017  data: 10.5551  max mem: 31081
Val:  [ 10/728]  eta: 0:18:29  loss: 0.4330 (0.4547)  acc: 83.3333 (80.7359)  time: 1.5456  data: 1.0214  max mem: 31081
Val:  [ 20/728]  eta: 0:14:07  loss: 0.4465 (0.4660)  acc: 82.1429 (81.0091)  time: 0.7022  data: 0.1792  max mem: 31081
Val:  [ 30/728]  eta: 0:12:35  loss: 0.4750 (0.4653)  acc: 80.9524 (80.0691)  time: 0.8286  data: 0.3056  max mem: 31081
Val:  [ 40/728]  eta: 0:11:47  loss: 0.4346 (0.4866)  acc: 80.9524 (79.4425)  time: 0.8510  data: 0.3282  max mem: 31081
Val:  [ 50/728]  eta: 0:11:10  loss: 0.3927 (0.4725)  acc: 82.1429 (80.2288)  time: 0.8421  data: 0.3187  max mem: 31081
Val:  [ 60/728]  eta: 0:10:15  loss: 0.3927 (0.4767)  acc: 79.7619 (79.9376)  time: 0.7028  data: 0.1804  max mem: 31081
Val:  [ 70/728]  eta: 0:09:46  loss: 0.4904 (0.4803)  acc: 76.1905 (79.2421)  time: 0.6423  data: 0.1232  max mem: 31081
Val:  [ 80/728]  eta: 0:09:35  loss: 0.5383 (0.4973)  acc: 75.0000 (78.7331)  time: 0.7851  data: 0.2656  max mem: 31081
Val:  [ 90/728]  eta: 0:09:19  loss: 0.5611 (0.5131)  acc: 75.0000 (78.1397)  time: 0.8271  data: 0.3037  max mem: 31081
Val:  [100/728]  eta: 0:09:08  loss: 0.4356 (0.5067)  acc: 80.9524 (78.3711)  time: 0.8123  data: 0.2884  max mem: 31081
Val:  [110/728]  eta: 0:08:59  loss: 0.4132 (0.5058)  acc: 80.9524 (78.3033)  time: 0.8517  data: 0.3298  max mem: 31081
Val:  [120/728]  eta: 0:08:40  loss: 0.3548 (0.4966)  acc: 82.1429 (78.6501)  time: 0.7715  data: 0.2503  max mem: 31081
Val:  [130/728]  eta: 0:08:23  loss: 0.3641 (0.5042)  acc: 82.1429 (78.4533)  time: 0.6698  data: 0.1499  max mem: 31081
Val:  [140/728]  eta: 0:08:12  loss: 0.4504 (0.5049)  acc: 79.7619 (78.4195)  time: 0.7231  data: 0.2037  max mem: 31081
Val:  [150/728]  eta: 0:08:00  loss: 0.4504 (0.5118)  acc: 79.7619 (78.2718)  time: 0.7608  data: 0.2384  max mem: 31081
Val:  [160/728]  eta: 0:07:51  loss: 0.4271 (0.5152)  acc: 78.5714 (78.0982)  time: 0.7804  data: 0.2579  max mem: 31081
Val:  [170/728]  eta: 0:07:41  loss: 0.4413 (0.5151)  acc: 77.3810 (78.0911)  time: 0.7998  data: 0.2760  max mem: 31081
Val:  [180/728]  eta: 0:07:29  loss: 0.4962 (0.5173)  acc: 77.3810 (78.0124)  time: 0.7346  data: 0.2083  max mem: 31081
Val:  [190/728]  eta: 0:07:18  loss: 0.5216 (0.5169)  acc: 75.0000 (77.9481)  time: 0.7032  data: 0.1786  max mem: 31081
Val:  [200/728]  eta: 0:07:09  loss: 0.4243 (0.5136)  acc: 76.1905 (77.9614)  time: 0.7657  data: 0.2421  max mem: 31081
Val:  [210/728]  eta: 0:07:03  loss: 0.4662 (0.5172)  acc: 78.5714 (77.9000)  time: 0.8593  data: 0.3363  max mem: 31081
Val:  [220/728]  eta: 0:06:56  loss: 0.4662 (0.5120)  acc: 79.7619 (78.1243)  time: 0.8758  data: 0.3560  max mem: 31081
Val:  [230/728]  eta: 0:06:47  loss: 0.4241 (0.5193)  acc: 80.9524 (77.9891)  time: 0.8278  data: 0.3092  max mem: 31081
Val:  [240/728]  eta: 0:06:34  loss: 0.5084 (0.5204)  acc: 80.9524 (78.1565)  time: 0.6777  data: 0.1557  max mem: 31081
Val:  [250/728]  eta: 0:06:25  loss: 0.5007 (0.5200)  acc: 77.3810 (78.1635)  time: 0.6678  data: 0.1444  max mem: 31081
Val:  [260/728]  eta: 0:06:18  loss: 0.3189 (0.5189)  acc: 80.9524 (78.2704)  time: 0.8299  data: 0.3074  max mem: 31081
Val:  [270/728]  eta: 0:06:10  loss: 0.4368 (0.5199)  acc: 80.9524 (78.2024)  time: 0.8398  data: 0.3186  max mem: 31081
Val:  [280/728]  eta: 0:06:05  loss: 0.4511 (0.5198)  acc: 78.5714 (78.2749)  time: 0.9022  data: 0.3801  max mem: 31081
Val:  [290/728]  eta: 0:05:57  loss: 0.4045 (0.5165)  acc: 82.1429 (78.3546)  time: 0.9048  data: 0.3816  max mem: 31081
Val:  [300/728]  eta: 0:05:45  loss: 0.4102 (0.5170)  acc: 80.9524 (78.3895)  time: 0.6660  data: 0.1448  max mem: 31081
Val:  [310/728]  eta: 0:05:37  loss: 0.4757 (0.5181)  acc: 77.3810 (78.3341)  time: 0.6733  data: 0.1514  max mem: 31081
Val:  [320/728]  eta: 0:05:28  loss: 0.4670 (0.5159)  acc: 77.3810 (78.3563)  time: 0.8049  data: 0.2787  max mem: 31081
Val:  [330/728]  eta: 0:05:21  loss: 0.4369 (0.5131)  acc: 80.9524 (78.4312)  time: 0.8305  data: 0.3062  max mem: 31081
Val:  [340/728]  eta: 0:05:14  loss: 0.3471 (0.5074)  acc: 83.3333 (78.6098)  time: 0.8939  data: 0.3726  max mem: 31081
Val:  [350/728]  eta: 0:05:06  loss: 0.3372 (0.5076)  acc: 84.5238 (78.7003)  time: 0.8470  data: 0.3231  max mem: 31081
Val:  [360/728]  eta: 0:04:55  loss: 0.3784 (0.5033)  acc: 80.9524 (78.8583)  time: 0.6550  data: 0.1293  max mem: 31081
Val:  [370/728]  eta: 0:04:48  loss: 0.4021 (0.5039)  acc: 80.9524 (78.7319)  time: 0.7053  data: 0.1794  max mem: 31081
Val:  [380/728]  eta: 0:04:40  loss: 0.4491 (0.5070)  acc: 75.0000 (78.6120)  time: 0.8557  data: 0.3308  max mem: 31081
Val:  [390/728]  eta: 0:04:32  loss: 0.3490 (0.5021)  acc: 80.9524 (78.7937)  time: 0.8285  data: 0.3043  max mem: 31081
Val:  [400/728]  eta: 0:04:24  loss: 0.3098 (0.5026)  acc: 84.5238 (78.8446)  time: 0.8475  data: 0.3227  max mem: 31081
Val:  [410/728]  eta: 0:04:17  loss: 0.3941 (0.5016)  acc: 79.7619 (78.8524)  time: 0.8569  data: 0.3317  max mem: 31081
Val:  [420/728]  eta: 0:04:06  loss: 0.3658 (0.5005)  acc: 84.5238 (78.9136)  time: 0.6867  data: 0.1620  max mem: 31081
Val:  [430/728]  eta: 0:03:59  loss: 0.4456 (0.4998)  acc: 82.1429 (78.9195)  time: 0.6932  data: 0.1687  max mem: 31081
Val:  [440/728]  eta: 0:03:51  loss: 0.4399 (0.4980)  acc: 80.9524 (78.9413)  time: 0.8273  data: 0.3025  max mem: 31081
Val:  [450/728]  eta: 0:03:43  loss: 0.4238 (0.4979)  acc: 80.9524 (78.9410)  time: 0.8245  data: 0.3020  max mem: 31081
Val:  [460/728]  eta: 0:03:35  loss: 0.4269 (0.5009)  acc: 77.3810 (78.8891)  time: 0.8284  data: 0.3079  max mem: 31081
Val:  [470/728]  eta: 0:03:27  loss: 0.3687 (0.4994)  acc: 80.9524 (78.9657)  time: 0.7911  data: 0.2668  max mem: 31081
Val:  [480/728]  eta: 0:03:17  loss: 0.3599 (0.4982)  acc: 83.3333 (79.0268)  time: 0.6534  data: 0.1271  max mem: 31081
Val:  [490/728]  eta: 0:03:10  loss: 0.4167 (0.4978)  acc: 78.5714 (79.0127)  time: 0.6838  data: 0.1594  max mem: 31081
Val:  [500/728]  eta: 0:03:02  loss: 0.4958 (0.4986)  acc: 78.5714 (78.9944)  time: 0.8297  data: 0.3082  max mem: 31081
Val:  [510/728]  eta: 0:02:54  loss: 0.3536 (0.4963)  acc: 82.1429 (79.0677)  time: 0.8758  data: 0.3546  max mem: 31081
Val:  [520/728]  eta: 0:02:46  loss: 0.3617 (0.4994)  acc: 82.1429 (78.9416)  time: 0.8672  data: 0.3447  max mem: 31081
Val:  [530/728]  eta: 0:02:38  loss: 0.4602 (0.4986)  acc: 80.9524 (79.0265)  time: 0.7469  data: 0.2220  max mem: 31081
Val:  [540/728]  eta: 0:02:29  loss: 0.5173 (0.5005)  acc: 78.5714 (78.8949)  time: 0.6096  data: 0.0832  max mem: 31081
Val:  [550/728]  eta: 0:02:21  loss: 0.5359 (0.5021)  acc: 73.8095 (78.8242)  time: 0.6560  data: 0.1317  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.4488 (0.5019)  acc: 80.9524 (78.8706)  time: 0.7439  data: 0.2196  max mem: 31081
Val:  [570/728]  eta: 0:02:05  loss: 0.4664 (0.5098)  acc: 80.9524 (78.7778)  time: 0.7887  data: 0.2635  max mem: 31081
Val:  [580/728]  eta: 0:01:57  loss: 0.5119 (0.5103)  acc: 79.7619 (78.7968)  time: 0.8220  data: 0.2980  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.5119 (0.5120)  acc: 79.7619 (78.7104)  time: 0.7302  data: 0.2068  max mem: 31081
Val:  [600/728]  eta: 0:01:40  loss: 0.3866 (0.5119)  acc: 80.9524 (78.7378)  time: 0.6058  data: 0.0844  max mem: 31081
Val:  [610/728]  eta: 0:01:32  loss: 0.4310 (0.5142)  acc: 80.9524 (78.6279)  time: 0.6553  data: 0.1323  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4635 (0.5179)  acc: 76.1905 (78.5733)  time: 0.8297  data: 0.3052  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4635 (0.5163)  acc: 76.1905 (78.5827)  time: 0.8588  data: 0.3342  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3963 (0.5148)  acc: 78.5714 (78.6253)  time: 0.8052  data: 0.2788  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4472 (0.5150)  acc: 80.9524 (78.6336)  time: 0.7794  data: 0.2554  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4599 (0.5145)  acc: 82.1429 (78.6615)  time: 0.6728  data: 0.1498  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4599 (0.5161)  acc: 80.9524 (78.6211)  time: 0.6871  data: 0.1624  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4011 (0.5143)  acc: 80.9524 (78.6798)  time: 0.7806  data: 0.2563  max mem: 31081
Val:  [690/728]  eta: 0:00:29  loss: 0.4093 (0.5137)  acc: 80.9524 (78.6903)  time: 0.8220  data: 0.2979  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4093 (0.5131)  acc: 79.7619 (78.6784)  time: 0.8725  data: 0.3480  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4582 (0.5136)  acc: 77.3810 (78.6485)  time: 0.8015  data: 0.2776  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4715 (0.5133)  acc: 78.5714 (78.6688)  time: 0.6819  data: 0.1657  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4582 (0.5148)  acc: 78.5714 (78.6500)  time: 0.6858  data: 0.1936  max mem: 31081
Val: Total time: 0:09:30 (0.7831 s / it)
* Acc@1 78.650 AP 0.7959991097450256 loss 0.515
Accuracy of the network on the 61096 val videos: 78.6%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.49 GB / 503.51 GB
Epoch: [13]  [  0/893]  eta: 3:14:49  lr: 0.002021  min_lr: 0.000003  loss: 0.4692 (0.4692)  class_acc: 0.7679 (0.7679)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 13.0898  data: 11.8067  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.38 GB / 503.51 GB
Epoch: [13]  [ 10/893]  eta: 0:38:15  lr: 0.002021  min_lr: 0.000003  loss: 0.3579 (0.3783)  class_acc: 0.8214 (0.8393)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5996  data: 1.1141  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.55 GB / 503.51 GB
Epoch: [13]  [ 20/893]  eta: 0:30:11  lr: 0.002020  min_lr: 0.000003  loss: 0.3447 (0.3652)  class_acc: 0.8393 (0.8444)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5239  data: 0.0226  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.73 GB / 503.51 GB
Epoch: [13]  [ 30/893]  eta: 0:27:10  lr: 0.002020  min_lr: 0.000003  loss: 0.3259 (0.3550)  class_acc: 0.8393 (0.8445)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4996  data: 0.0004  max mem: 31081
[2025-03-11 02:43:33,341] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 11628
[2025-03-11 02:43:33,341] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 02:43:33,341] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.18 GB / 503.51 GB
Epoch: [13]  [ 40/893]  eta: 0:25:37  lr: 0.002020  min_lr: 0.000003  loss: 0.2795 (0.3336)  class_acc: 0.8750 (0.8554)  loss_scale: 8192.0000 (7292.8780)  weight_decay: 0.0500 (0.0500)  time: 1.5165  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.48 GB / 503.51 GB
Epoch: [13]  [ 50/893]  eta: 0:24:28  lr: 0.002019  min_lr: 0.000003  loss: 0.2717 (0.3336)  class_acc: 0.8929 (0.8543)  loss_scale: 4096.0000 (6666.0392)  weight_decay: 0.0500 (0.0500)  time: 1.5121  data: 0.0004  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.67 GB / 503.51 GB
Epoch: [13]  [ 60/893]  eta: 0:23:32  lr: 0.002019  min_lr: 0.000003  loss: 0.3350 (0.3320)  class_acc: 0.8571 (0.8536)  loss_scale: 4096.0000 (6244.7213)  weight_decay: 0.0500 (0.0500)  time: 1.4774  data: 0.0003  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.81 GB / 503.51 GB
Epoch: [13]  [ 70/893]  eta: 0:22:47  lr: 0.002018  min_lr: 0.000003  loss: 0.3379 (0.3319)  class_acc: 0.8571 (0.8534)  loss_scale: 4096.0000 (5942.0845)  weight_decay: 0.0500 (0.0500)  time: 1.4573  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.41 GB / 503.51 GB
Epoch: [13]  [ 80/893]  eta: 0:22:12  lr: 0.002018  min_lr: 0.000003  loss: 0.3103 (0.3326)  class_acc: 0.8571 (0.8523)  loss_scale: 4096.0000 (5714.1728)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0002  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.36 GB / 503.51 GB
Epoch: [13]  [ 90/893]  eta: 0:21:40  lr: 0.002017  min_lr: 0.000003  loss: 0.3052 (0.3311)  class_acc: 0.8571 (0.8552)  loss_scale: 4096.0000 (5536.3516)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0002  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.10 GB / 503.51 GB
Epoch: [13]  [100/893]  eta: 0:21:11  lr: 0.002017  min_lr: 0.000003  loss: 0.3071 (0.3331)  class_acc: 0.8571 (0.8541)  loss_scale: 4096.0000 (5393.7426)  weight_decay: 0.0500 (0.0500)  time: 1.4571  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.41 GB / 503.51 GB
Epoch: [13]  [110/893]  eta: 0:20:45  lr: 0.002016  min_lr: 0.000003  loss: 0.3491 (0.3325)  class_acc: 0.8393 (0.8551)  loss_scale: 4096.0000 (5276.8288)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0002  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.73 GB / 503.51 GB
Epoch: [13]  [120/893]  eta: 0:20:22  lr: 0.002016  min_lr: 0.000003  loss: 0.3333 (0.3313)  class_acc: 0.8571 (0.8554)  loss_scale: 4096.0000 (5179.2397)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0002  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.65 GB / 503.51 GB
Epoch: [13]  [130/893]  eta: 0:20:00  lr: 0.002015  min_lr: 0.000003  loss: 0.2808 (0.3295)  class_acc: 0.8571 (0.8574)  loss_scale: 4096.0000 (5096.5496)  weight_decay: 0.0500 (0.0500)  time: 1.4740  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.38 GB / 503.51 GB
Epoch: [13]  [140/893]  eta: 0:19:38  lr: 0.002015  min_lr: 0.000003  loss: 0.3120 (0.3310)  class_acc: 0.8750 (0.8578)  loss_scale: 4096.0000 (5025.5887)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.01 GB / 503.51 GB
Epoch: [13]  [150/893]  eta: 0:19:18  lr: 0.002014  min_lr: 0.000003  loss: 0.3394 (0.3313)  class_acc: 0.8571 (0.8570)  loss_scale: 4096.0000 (4964.0265)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.20 GB / 503.51 GB
Epoch: [13]  [160/893]  eta: 0:18:58  lr: 0.002014  min_lr: 0.000003  loss: 0.3533 (0.3346)  class_acc: 0.8214 (0.8550)  loss_scale: 4096.0000 (4910.1118)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0003  max mem: 31081
[2025-03-11 02:46:43,275] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 02:46:43,275] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.32 GB / 503.51 GB
Epoch: [13]  [170/893]  eta: 0:18:38  lr: 0.002014  min_lr: 0.000003  loss: 0.3401 (0.3354)  class_acc: 0.8214 (0.8539)  loss_scale: 4096.0000 (5102.0351)  weight_decay: 0.0500 (0.0500)  time: 1.4584  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.09 GB / 503.51 GB
Epoch: [13]  [180/893]  eta: 0:18:20  lr: 0.002013  min_lr: 0.000003  loss: 0.3384 (0.3360)  class_acc: 0.8393 (0.8527)  loss_scale: 8192.0000 (5272.7514)  weight_decay: 0.0500 (0.0500)  time: 1.4584  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.64 GB / 503.51 GB
Epoch: [13]  [190/893]  eta: 0:18:01  lr: 0.002013  min_lr: 0.000003  loss: 0.3455 (0.3376)  class_acc: 0.8214 (0.8507)  loss_scale: 8192.0000 (5425.5916)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0004  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.19 GB / 503.51 GB
Epoch: [13]  [200/893]  eta: 0:17:43  lr: 0.002012  min_lr: 0.000003  loss: 0.3455 (0.3383)  class_acc: 0.8393 (0.8507)  loss_scale: 8192.0000 (5563.2239)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.94 GB / 503.51 GB
Epoch: [13]  [210/893]  eta: 0:17:25  lr: 0.002012  min_lr: 0.000003  loss: 0.2993 (0.3372)  class_acc: 0.8571 (0.8515)  loss_scale: 8192.0000 (5687.8104)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.89 GB / 503.51 GB
Epoch: [13]  [220/893]  eta: 0:17:08  lr: 0.002011  min_lr: 0.000003  loss: 0.3230 (0.3390)  class_acc: 0.8393 (0.8497)  loss_scale: 8192.0000 (5801.1222)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0003  max mem: 31081
[2025-03-11 02:48:21,288] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 11824
[2025-03-11 02:48:21,289] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 02:48:21,289] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.88 GB / 503.51 GB
Epoch: [13]  [230/893]  eta: 0:16:51  lr: 0.002011  min_lr: 0.000003  loss: 0.3989 (0.3420)  class_acc: 0.8036 (0.8480)  loss_scale: 8192.0000 (5851.4286)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0004  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.32 GB / 503.51 GB
Epoch: [13]  [240/893]  eta: 0:16:34  lr: 0.002010  min_lr: 0.000003  loss: 0.3718 (0.3415)  class_acc: 0.8214 (0.8482)  loss_scale: 4096.0000 (5778.5892)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0004  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.01 GB / 503.51 GB
Epoch: [13]  [250/893]  eta: 0:16:18  lr: 0.002010  min_lr: 0.000003  loss: 0.3406 (0.3422)  class_acc: 0.8393 (0.8476)  loss_scale: 4096.0000 (5711.5538)  weight_decay: 0.0500 (0.0500)  time: 1.4673  data: 0.0004  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.07 GB / 503.51 GB
Epoch: [13]  [260/893]  eta: 0:16:01  lr: 0.002009  min_lr: 0.000003  loss: 0.3142 (0.3404)  class_acc: 0.8571 (0.8482)  loss_scale: 4096.0000 (5649.6552)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.69 GB / 503.51 GB
Epoch: [13]  [270/893]  eta: 0:15:44  lr: 0.002009  min_lr: 0.000003  loss: 0.3142 (0.3409)  class_acc: 0.8571 (0.8482)  loss_scale: 4096.0000 (5592.3247)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0002  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.97 GB / 503.51 GB
Epoch: [13]  [280/893]  eta: 0:15:28  lr: 0.002008  min_lr: 0.000003  loss: 0.3276 (0.3408)  class_acc: 0.8393 (0.8482)  loss_scale: 4096.0000 (5539.0747)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.55 GB / 503.51 GB
Epoch: [13]  [290/893]  eta: 0:15:12  lr: 0.002008  min_lr: 0.000003  loss: 0.3359 (0.3416)  class_acc: 0.8393 (0.8476)  loss_scale: 4096.0000 (5489.4845)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0002  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.55 GB / 503.51 GB
Epoch: [13]  [300/893]  eta: 0:14:56  lr: 0.002007  min_lr: 0.000003  loss: 0.3369 (0.3415)  class_acc: 0.8393 (0.8480)  loss_scale: 4096.0000 (5443.1894)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.96 GB / 503.51 GB
Epoch: [13]  [310/893]  eta: 0:14:40  lr: 0.002007  min_lr: 0.000003  loss: 0.3435 (0.3418)  class_acc: 0.8393 (0.8476)  loss_scale: 4096.0000 (5399.8714)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.66 GB / 503.51 GB
Epoch: [13]  [320/893]  eta: 0:14:24  lr: 0.002007  min_lr: 0.000003  loss: 0.3445 (0.3421)  class_acc: 0.8393 (0.8474)  loss_scale: 4096.0000 (5359.2523)  weight_decay: 0.0500 (0.0500)  time: 1.4733  data: 0.0004  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.01 GB / 503.51 GB
Epoch: [13]  [330/893]  eta: 0:14:08  lr: 0.002006  min_lr: 0.000003  loss: 0.3550 (0.3441)  class_acc: 0.8214 (0.8456)  loss_scale: 4096.0000 (5321.0876)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0004  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.13 GB / 503.51 GB
Epoch: [13]  [340/893]  eta: 0:13:53  lr: 0.002006  min_lr: 0.000003  loss: 0.3530 (0.3433)  class_acc: 0.8393 (0.8461)  loss_scale: 4096.0000 (5285.1613)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.30 GB / 503.51 GB
Epoch: [13]  [350/893]  eta: 0:13:37  lr: 0.002005  min_lr: 0.000003  loss: 0.3481 (0.3439)  class_acc: 0.8393 (0.8452)  loss_scale: 4096.0000 (5251.2821)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0002  max mem: 31081
[2025-03-11 02:51:30,312] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 02:51:30,312] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.64 GB / 503.51 GB
Epoch: [13]  [360/893]  eta: 0:13:21  lr: 0.002005  min_lr: 0.000003  loss: 0.3716 (0.3438)  class_acc: 0.8393 (0.8450)  loss_scale: 4096.0000 (5264.6648)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0002  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.75 GB / 503.51 GB
Epoch: [13]  [370/893]  eta: 0:13:06  lr: 0.002004  min_lr: 0.000003  loss: 0.3215 (0.3429)  class_acc: 0.8393 (0.8457)  loss_scale: 8192.0000 (5343.5687)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.78 GB / 503.51 GB
Epoch: [13]  [380/893]  eta: 0:12:50  lr: 0.002004  min_lr: 0.000003  loss: 0.3408 (0.3439)  class_acc: 0.8393 (0.8449)  loss_scale: 8192.0000 (5418.3307)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.64 GB / 503.51 GB
Epoch: [13]  [390/893]  eta: 0:12:35  lr: 0.002003  min_lr: 0.000003  loss: 0.3716 (0.3448)  class_acc: 0.8214 (0.8445)  loss_scale: 8192.0000 (5489.2685)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0005  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.75 GB / 503.51 GB
Epoch: [13]  [400/893]  eta: 0:12:19  lr: 0.002003  min_lr: 0.000003  loss: 0.3394 (0.3448)  class_acc: 0.8393 (0.8447)  loss_scale: 8192.0000 (5556.6683)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0004  max mem: 31081
[2025-03-11 02:52:37,897] [INFO] [logging.py:129:log_dist] [Rank 0] step=12000, skipped=68, lr=[2.6155104953366277e-06, 2.6155104953366277e-06, 4.35918415889438e-06, 4.35918415889438e-06, 7.265306931490632e-06, 7.265306931490632e-06, 1.210884488581772e-05, 1.210884488581772e-05, 2.0181408143029538e-05, 2.0181408143029538e-05, 3.363568023838256e-05, 3.363568023838256e-05, 5.605946706397094e-05, 5.605946706397094e-05, 9.343244510661824e-05, 9.343244510661824e-05, 0.00015572074184436372, 0.00015572074184436372, 0.00025953456974060627, 0.00025953456974060627, 0.00043255761623434374, 0.00043255761623434374, 0.000720929360390573, 0.000720929360390573, 0.0012015489339842883, 0.0012015489339842883, 0.0020025815566404805, 0.0020025815566404805], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 02:52:37,897] [INFO] [timer.py:264:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=61.02701020602296, CurrSamplesPerSec=61.64734605530707, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
[2025-03-11 02:52:39,346] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 12000
[2025-03-11 02:52:39,346] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 02:52:39,346] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.19 GB / 503.51 GB
Epoch: [13]  [410/893]  eta: 0:12:04  lr: 0.002002  min_lr: 0.000003  loss: 0.3374 (0.3447)  class_acc: 0.8571 (0.8452)  loss_scale: 8192.0000 (5551.0268)  weight_decay: 0.0500 (0.0500)  time: 1.4699  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.15 GB / 503.51 GB
Epoch: [13]  [420/893]  eta: 0:11:49  lr: 0.002002  min_lr: 0.000003  loss: 0.3711 (0.3462)  class_acc: 0.8393 (0.8446)  loss_scale: 4096.0000 (5516.4656)  weight_decay: 0.0500 (0.0500)  time: 1.4749  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.41 GB / 503.51 GB
Epoch: [13]  [430/893]  eta: 0:11:33  lr: 0.002001  min_lr: 0.000003  loss: 0.3711 (0.3471)  class_acc: 0.8214 (0.8441)  loss_scale: 4096.0000 (5483.5081)  weight_decay: 0.0500 (0.0500)  time: 1.4739  data: 0.0003  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.46 GB / 503.51 GB
Epoch: [13]  [440/893]  eta: 0:11:18  lr: 0.002001  min_lr: 0.000003  loss: 0.3704 (0.3471)  class_acc: 0.8214 (0.8439)  loss_scale: 4096.0000 (5452.0454)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0002  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.87 GB / 503.51 GB
Epoch: [13]  [450/893]  eta: 0:11:03  lr: 0.002000  min_lr: 0.000003  loss: 0.3721 (0.3478)  class_acc: 0.8393 (0.8438)  loss_scale: 4096.0000 (5421.9778)  weight_decay: 0.0500 (0.0500)  time: 1.4595  data: 0.0002  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.46 GB / 503.51 GB
Epoch: [13]  [460/893]  eta: 0:10:47  lr: 0.002000  min_lr: 0.000003  loss: 0.3704 (0.3481)  class_acc: 0.8393 (0.8438)  loss_scale: 4096.0000 (5393.2148)  weight_decay: 0.0500 (0.0500)  time: 1.4591  data: 0.0002  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.47 GB / 503.51 GB
Epoch: [13]  [470/893]  eta: 0:10:32  lr: 0.001999  min_lr: 0.000003  loss: 0.3157 (0.3464)  class_acc: 0.8750 (0.8447)  loss_scale: 4096.0000 (5365.6730)  weight_decay: 0.0500 (0.0500)  time: 1.4602  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.97 GB / 503.51 GB
Epoch: [13]  [480/893]  eta: 0:10:17  lr: 0.001999  min_lr: 0.000003  loss: 0.2988 (0.3466)  class_acc: 0.8750 (0.8445)  loss_scale: 4096.0000 (5339.2765)  weight_decay: 0.0500 (0.0500)  time: 1.4608  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.30 GB / 503.51 GB
Epoch: [13]  [490/893]  eta: 0:10:01  lr: 0.001998  min_lr: 0.000003  loss: 0.3384 (0.3468)  class_acc: 0.8393 (0.8447)  loss_scale: 4096.0000 (5313.9552)  weight_decay: 0.0500 (0.0500)  time: 1.4582  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.90 GB / 503.51 GB
Epoch: [13]  [500/893]  eta: 0:09:46  lr: 0.001998  min_lr: 0.000003  loss: 0.3540 (0.3471)  class_acc: 0.8571 (0.8448)  loss_scale: 4096.0000 (5289.6447)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.03 GB / 503.51 GB
Epoch: [13]  [510/893]  eta: 0:09:31  lr: 0.001997  min_lr: 0.000003  loss: 0.3711 (0.3476)  class_acc: 0.8393 (0.8446)  loss_scale: 4096.0000 (5266.2857)  weight_decay: 0.0500 (0.0500)  time: 1.4628  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.09 GB / 503.51 GB
Epoch: [13]  [520/893]  eta: 0:09:16  lr: 0.001997  min_lr: 0.000003  loss: 0.3303 (0.3472)  class_acc: 0.8571 (0.8451)  loss_scale: 4096.0000 (5243.8234)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.29 GB / 503.51 GB
Epoch: [13]  [530/893]  eta: 0:09:01  lr: 0.001996  min_lr: 0.000003  loss: 0.3240 (0.3470)  class_acc: 0.8571 (0.8450)  loss_scale: 4096.0000 (5222.2072)  weight_decay: 0.0500 (0.0500)  time: 1.4713  data: 0.0004  max mem: 31081
[2025-03-11 02:55:48,278] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 02:55:48,278] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.71 GB / 503.51 GB
Epoch: [13]  [540/893]  eta: 0:08:46  lr: 0.001996  min_lr: 0.000003  loss: 0.3291 (0.3469)  class_acc: 0.8393 (0.8449)  loss_scale: 4096.0000 (5261.9593)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0004  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.13 GB / 503.51 GB
Epoch: [13]  [550/893]  eta: 0:08:31  lr: 0.001996  min_lr: 0.000003  loss: 0.3291 (0.3460)  class_acc: 0.8571 (0.8454)  loss_scale: 8192.0000 (5315.1361)  weight_decay: 0.0500 (0.0500)  time: 1.4522  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.34 GB / 503.51 GB
Epoch: [13]  [560/893]  eta: 0:08:16  lr: 0.001995  min_lr: 0.000003  loss: 0.3198 (0.3461)  class_acc: 0.8571 (0.8455)  loss_scale: 8192.0000 (5366.4171)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.05 GB / 503.51 GB
Epoch: [13]  [570/893]  eta: 0:08:01  lr: 0.001995  min_lr: 0.000003  loss: 0.3499 (0.3462)  class_acc: 0.8214 (0.8451)  loss_scale: 8192.0000 (5415.9019)  weight_decay: 0.0500 (0.0500)  time: 1.4733  data: 0.0002  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.26 GB / 503.51 GB
Epoch: [13]  [580/893]  eta: 0:07:46  lr: 0.001994  min_lr: 0.000003  loss: 0.3286 (0.3464)  class_acc: 0.8393 (0.8451)  loss_scale: 8192.0000 (5463.6833)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.50 GB / 503.51 GB
Epoch: [13]  [590/893]  eta: 0:07:31  lr: 0.001994  min_lr: 0.000003  loss: 0.3286 (0.3462)  class_acc: 0.8571 (0.8452)  loss_scale: 8192.0000 (5509.8477)  weight_decay: 0.0500 (0.0500)  time: 1.4608  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.89 GB / 503.51 GB
Epoch: [13]  [600/893]  eta: 0:07:16  lr: 0.001993  min_lr: 0.000003  loss: 0.3252 (0.3459)  class_acc: 0.8571 (0.8454)  loss_scale: 8192.0000 (5554.4759)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0002  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.71 GB / 503.51 GB
Epoch: [13]  [610/893]  eta: 0:07:01  lr: 0.001993  min_lr: 0.000003  loss: 0.3252 (0.3458)  class_acc: 0.8393 (0.8454)  loss_scale: 8192.0000 (5597.6432)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0004  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.84 GB / 503.51 GB
Epoch: [13]  [620/893]  eta: 0:06:46  lr: 0.001992  min_lr: 0.000003  loss: 0.3354 (0.3457)  class_acc: 0.8393 (0.8456)  loss_scale: 8192.0000 (5639.4203)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0004  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.29 GB / 503.51 GB
Epoch: [13]  [630/893]  eta: 0:06:31  lr: 0.001992  min_lr: 0.000003  loss: 0.3184 (0.3454)  class_acc: 0.8393 (0.8457)  loss_scale: 8192.0000 (5679.8732)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.99 GB / 503.51 GB
Epoch: [13]  [640/893]  eta: 0:06:16  lr: 0.001991  min_lr: 0.000003  loss: 0.3169 (0.3457)  class_acc: 0.8393 (0.8456)  loss_scale: 8192.0000 (5719.0640)  weight_decay: 0.0500 (0.0500)  time: 1.4590  data: 0.0002  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.51 GB / 503.51 GB
Epoch: [13]  [650/893]  eta: 0:06:01  lr: 0.001991  min_lr: 0.000003  loss: 0.3142 (0.3452)  class_acc: 0.8393 (0.8459)  loss_scale: 8192.0000 (5757.0507)  weight_decay: 0.0500 (0.0500)  time: 1.4595  data: 0.0002  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.28 GB / 503.51 GB
Epoch: [13]  [660/893]  eta: 0:05:46  lr: 0.001990  min_lr: 0.000003  loss: 0.3286 (0.3455)  class_acc: 0.8393 (0.8456)  loss_scale: 8192.0000 (5793.8880)  weight_decay: 0.0500 (0.0500)  time: 1.4720  data: 0.0003  max mem: 31081
[2025-03-11 02:58:55,712] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 02:58:55,713] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-03-11 02:58:58,640] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 12259
[2025-03-11 02:58:58,640] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 02:58:58,640] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.94 GB / 503.51 GB
Epoch: [13]  [670/893]  eta: 0:05:31  lr: 0.001990  min_lr: 0.000003  loss: 0.3650 (0.3458)  class_acc: 0.8393 (0.8455)  loss_scale: 8192.0000 (5854.0447)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
[2025-03-11 02:59:17,743] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 12272
[2025-03-11 02:59:17,743] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 02:59:17,743] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.58 GB / 503.51 GB
Epoch: [13]  [680/893]  eta: 0:05:16  lr: 0.001989  min_lr: 0.000003  loss: 0.3508 (0.3462)  class_acc: 0.8214 (0.8450)  loss_scale: 8192.0000 (5858.3025)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.66 GB / 503.51 GB
Epoch: [13]  [690/893]  eta: 0:05:01  lr: 0.001989  min_lr: 0.000003  loss: 0.3418 (0.3460)  class_acc: 0.8214 (0.8452)  loss_scale: 4096.0000 (5832.7988)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.75 GB / 503.51 GB
Epoch: [13]  [700/893]  eta: 0:04:46  lr: 0.001988  min_lr: 0.000003  loss: 0.3306 (0.3459)  class_acc: 0.8571 (0.8453)  loss_scale: 4096.0000 (5808.0228)  weight_decay: 0.0500 (0.0500)  time: 1.4696  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.92 GB / 503.51 GB
Epoch: [13]  [710/893]  eta: 0:04:31  lr: 0.001988  min_lr: 0.000003  loss: 0.3306 (0.3459)  class_acc: 0.8571 (0.8453)  loss_scale: 4096.0000 (5783.9437)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0002  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.07 GB / 503.51 GB
Epoch: [13]  [720/893]  eta: 0:04:16  lr: 0.001987  min_lr: 0.000003  loss: 0.3438 (0.3460)  class_acc: 0.8571 (0.8453)  loss_scale: 4096.0000 (5760.5326)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0004  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.80 GB / 503.51 GB
Epoch: [13]  [730/893]  eta: 0:04:01  lr: 0.001987  min_lr: 0.000003  loss: 0.3391 (0.3460)  class_acc: 0.8393 (0.8453)  loss_scale: 4096.0000 (5737.7620)  weight_decay: 0.0500 (0.0500)  time: 1.4628  data: 0.0004  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.36 GB / 503.51 GB
Epoch: [13]  [740/893]  eta: 0:03:47  lr: 0.001986  min_lr: 0.000003  loss: 0.3391 (0.3462)  class_acc: 0.8393 (0.8453)  loss_scale: 4096.0000 (5715.6059)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.94 GB / 503.51 GB
Epoch: [13]  [750/893]  eta: 0:03:32  lr: 0.001986  min_lr: 0.000003  loss: 0.3557 (0.3464)  class_acc: 0.8571 (0.8453)  loss_scale: 4096.0000 (5694.0399)  weight_decay: 0.0500 (0.0500)  time: 1.4658  data: 0.0002  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.10 GB / 503.51 GB
Epoch: [13]  [760/893]  eta: 0:03:17  lr: 0.001985  min_lr: 0.000003  loss: 0.3296 (0.3456)  class_acc: 0.8571 (0.8458)  loss_scale: 4096.0000 (5673.0407)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0002  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.72 GB / 503.51 GB
Epoch: [13]  [770/893]  eta: 0:03:02  lr: 0.001985  min_lr: 0.000003  loss: 0.2964 (0.3455)  class_acc: 0.8571 (0.8459)  loss_scale: 4096.0000 (5652.5863)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0002  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.89 GB / 503.51 GB
Epoch: [13]  [780/893]  eta: 0:02:47  lr: 0.001984  min_lr: 0.000003  loss: 0.3323 (0.3454)  class_acc: 0.8571 (0.8461)  loss_scale: 4096.0000 (5632.6556)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0002  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.93 GB / 503.51 GB
Epoch: [13]  [790/893]  eta: 0:02:32  lr: 0.001984  min_lr: 0.000003  loss: 0.3672 (0.3458)  class_acc: 0.8214 (0.8457)  loss_scale: 4096.0000 (5613.2288)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0002  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.40 GB / 503.51 GB
Epoch: [13]  [800/893]  eta: 0:02:17  lr: 0.001983  min_lr: 0.000003  loss: 0.3413 (0.3455)  class_acc: 0.8393 (0.8460)  loss_scale: 4096.0000 (5594.2871)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0002  max mem: 31081
[2025-03-11 03:02:26,849] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:02:26,849] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.23 GB / 503.51 GB
Epoch: [13]  [810/893]  eta: 0:02:03  lr: 0.001983  min_lr: 0.000003  loss: 0.2959 (0.3452)  class_acc: 0.8571 (0.8462)  loss_scale: 4096.0000 (5606.1159)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
[2025-03-11 03:02:41,525] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 12411
[2025-03-11 03:02:41,525] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 03:02:41,525] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.74 GB / 503.51 GB
Epoch: [13]  [820/893]  eta: 0:01:48  lr: 0.001982  min_lr: 0.000003  loss: 0.3127 (0.3451)  class_acc: 0.8571 (0.8464)  loss_scale: 4096.0000 (5607.6784)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.03 GB / 503.51 GB
Epoch: [13]  [830/893]  eta: 0:01:33  lr: 0.001982  min_lr: 0.000003  loss: 0.3696 (0.3456)  class_acc: 0.8571 (0.8463)  loss_scale: 4096.0000 (5589.4874)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.76 GB / 503.51 GB
Epoch: [13]  [840/893]  eta: 0:01:18  lr: 0.001981  min_lr: 0.000003  loss: 0.3701 (0.3460)  class_acc: 0.8214 (0.8461)  loss_scale: 4096.0000 (5571.7289)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0002  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.75 GB / 503.51 GB
Epoch: [13]  [850/893]  eta: 0:01:03  lr: 0.001981  min_lr: 0.000003  loss: 0.3459 (0.3459)  class_acc: 0.8214 (0.8460)  loss_scale: 4096.0000 (5554.3878)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.28 GB / 503.51 GB
Epoch: [13]  [860/893]  eta: 0:00:48  lr: 0.001980  min_lr: 0.000003  loss: 0.3232 (0.3454)  class_acc: 0.8571 (0.8464)  loss_scale: 4096.0000 (5537.4495)  weight_decay: 0.0500 (0.0500)  time: 1.4528  data: 0.0003  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.16 GB / 503.51 GB
Epoch: [13]  [870/893]  eta: 0:00:34  lr: 0.001980  min_lr: 0.000003  loss: 0.3091 (0.3454)  class_acc: 0.8571 (0.8464)  loss_scale: 4096.0000 (5520.9001)  weight_decay: 0.0500 (0.0500)  time: 1.4398  data: 0.0002  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.19 GB / 503.51 GB
Epoch: [13]  [880/893]  eta: 0:00:19  lr: 0.001979  min_lr: 0.000003  loss: 0.3044 (0.3451)  class_acc: 0.8571 (0.8467)  loss_scale: 4096.0000 (5504.7264)  weight_decay: 0.0500 (0.0500)  time: 1.4392  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.23 GB / 503.51 GB
Epoch: [13]  [890/893]  eta: 0:00:04  lr: 0.001979  min_lr: 0.000003  loss: 0.3044 (0.3446)  class_acc: 0.8750 (0.8471)  loss_scale: 4096.0000 (5488.9158)  weight_decay: 0.0500 (0.0500)  time: 1.4375  data: 0.0002  max mem: 31081
Epoch: [13]  [892/893]  eta: 0:00:01  lr: 0.001979  min_lr: 0.000003  loss: 0.2998 (0.3445)  class_acc: 0.8929 (0.8472)  loss_scale: 4096.0000 (5487.3543)  weight_decay: 0.0500 (0.0500)  time: 1.3876  data: 0.0002  max mem: 31081
Epoch: [13] Total time: 0:22:00 (1.4788 s / it)
Averaged stats: lr: 0.001979  min_lr: 0.000003  loss: 0.2998 (0.3445)  class_acc: 0.8929 (0.8472)  loss_scale: 4096.0000 (5487.3543)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:16:29  loss: 0.7793 (0.7793)  acc: 73.8095 (73.8095)  time: 11.2496  data: 10.7120  max mem: 31081
Val:  [ 10/728]  eta: 0:18:30  loss: 0.4596 (0.5141)  acc: 77.3810 (78.1385)  time: 1.5461  data: 1.0220  max mem: 31081
Val:  [ 20/728]  eta: 0:14:08  loss: 0.4596 (0.5211)  acc: 78.5714 (78.2880)  time: 0.6952  data: 0.1722  max mem: 31081
Val:  [ 30/728]  eta: 0:12:38  loss: 0.4938 (0.5131)  acc: 78.5714 (77.9186)  time: 0.8346  data: 0.3120  max mem: 31081
Val:  [ 40/728]  eta: 0:11:46  loss: 0.3597 (0.5063)  acc: 83.3333 (78.1940)  time: 0.8467  data: 0.3238  max mem: 31081
Val:  [ 50/728]  eta: 0:11:10  loss: 0.3714 (0.4959)  acc: 83.3333 (78.7348)  time: 0.8376  data: 0.3126  max mem: 31081
Val:  [ 60/728]  eta: 0:10:20  loss: 0.5292 (0.5179)  acc: 75.0000 (78.1226)  time: 0.7268  data: 0.2021  max mem: 31081
Val:  [ 70/728]  eta: 0:09:47  loss: 0.6653 (0.5426)  acc: 72.6190 (77.3810)  time: 0.6474  data: 0.1255  max mem: 31081
Val:  [ 80/728]  eta: 0:09:35  loss: 0.7018 (0.5678)  acc: 70.2381 (76.8225)  time: 0.7642  data: 0.2438  max mem: 31081
Val:  [ 90/728]  eta: 0:09:20  loss: 0.6666 (0.6006)  acc: 70.2381 (76.1251)  time: 0.8287  data: 0.3092  max mem: 31081
Val:  [100/728]  eta: 0:09:09  loss: 0.5698 (0.5843)  acc: 77.3810 (76.6855)  time: 0.8244  data: 0.3044  max mem: 31081
Val:  [110/728]  eta: 0:09:00  loss: 0.3833 (0.5900)  acc: 79.7619 (76.4586)  time: 0.8523  data: 0.3280  max mem: 31081
Val:  [120/728]  eta: 0:08:47  loss: 0.3833 (0.5740)  acc: 82.1429 (77.0366)  time: 0.8261  data: 0.3024  max mem: 31081
Val:  [130/728]  eta: 0:08:24  loss: 0.4128 (0.5723)  acc: 84.5238 (77.3355)  time: 0.6770  data: 0.1577  max mem: 31081
Val:  [140/728]  eta: 0:08:15  loss: 0.4638 (0.5743)  acc: 79.7619 (77.0601)  time: 0.6979  data: 0.1777  max mem: 31081
Val:  [150/728]  eta: 0:07:59  loss: 0.4640 (0.5789)  acc: 80.9524 (77.1366)  time: 0.7364  data: 0.2126  max mem: 31081
Val:  [160/728]  eta: 0:07:52  loss: 0.4178 (0.5840)  acc: 77.3810 (77.0408)  time: 0.7586  data: 0.2351  max mem: 31081
Val:  [170/728]  eta: 0:07:43  loss: 0.4974 (0.5803)  acc: 77.3810 (77.1582)  time: 0.8339  data: 0.3131  max mem: 31081
Val:  [180/728]  eta: 0:07:33  loss: 0.4843 (0.5789)  acc: 79.7619 (77.2823)  time: 0.7829  data: 0.2614  max mem: 31081
Val:  [190/728]  eta: 0:07:18  loss: 0.4816 (0.5756)  acc: 79.7619 (77.3560)  time: 0.6868  data: 0.1648  max mem: 31081
Val:  [200/728]  eta: 0:07:10  loss: 0.3846 (0.5661)  acc: 79.7619 (77.6119)  time: 0.7008  data: 0.1784  max mem: 31081
Val:  [210/728]  eta: 0:07:04  loss: 0.3846 (0.5696)  acc: 79.7619 (77.5954)  time: 0.8641  data: 0.3423  max mem: 31081
Val:  [220/728]  eta: 0:06:57  loss: 0.3997 (0.5645)  acc: 78.5714 (77.7634)  time: 0.9021  data: 0.3813  max mem: 31081
Val:  [230/728]  eta: 0:06:49  loss: 0.4379 (0.5679)  acc: 79.7619 (77.6696)  time: 0.8393  data: 0.3176  max mem: 31081
Val:  [240/728]  eta: 0:06:36  loss: 0.5202 (0.5693)  acc: 77.3810 (77.7416)  time: 0.7072  data: 0.1868  max mem: 31081
Val:  [250/728]  eta: 0:06:26  loss: 0.4517 (0.5695)  acc: 77.3810 (77.7272)  time: 0.6478  data: 0.1286  max mem: 31081
Val:  [260/728]  eta: 0:06:18  loss: 0.4053 (0.5677)  acc: 79.7619 (77.7732)  time: 0.7552  data: 0.2358  max mem: 31081
Val:  [270/728]  eta: 0:06:10  loss: 0.4053 (0.5693)  acc: 79.7619 (77.6665)  time: 0.8310  data: 0.3112  max mem: 31081
Val:  [280/728]  eta: 0:06:05  loss: 0.4604 (0.5657)  acc: 77.3810 (77.8131)  time: 0.9204  data: 0.3987  max mem: 31081
Val:  [290/728]  eta: 0:05:57  loss: 0.3778 (0.5612)  acc: 80.9524 (77.9251)  time: 0.9103  data: 0.3861  max mem: 31081
Val:  [300/728]  eta: 0:05:45  loss: 0.4711 (0.5606)  acc: 80.9524 (77.8991)  time: 0.6676  data: 0.1442  max mem: 31081
Val:  [310/728]  eta: 0:05:37  loss: 0.5020 (0.5610)  acc: 76.1905 (77.8212)  time: 0.6803  data: 0.1562  max mem: 31081
Val:  [320/728]  eta: 0:05:29  loss: 0.4445 (0.5591)  acc: 79.7619 (77.8668)  time: 0.8090  data: 0.2857  max mem: 31081
Val:  [330/728]  eta: 0:05:21  loss: 0.4253 (0.5564)  acc: 80.9524 (77.9168)  time: 0.8286  data: 0.3075  max mem: 31081
Val:  [340/728]  eta: 0:05:15  loss: 0.4086 (0.5511)  acc: 80.9524 (78.1420)  time: 0.8991  data: 0.3774  max mem: 31081
Val:  [350/728]  eta: 0:05:06  loss: 0.3701 (0.5508)  acc: 84.5238 (78.2696)  time: 0.8575  data: 0.3356  max mem: 31081
Val:  [360/728]  eta: 0:04:55  loss: 0.3701 (0.5478)  acc: 84.5238 (78.3538)  time: 0.6573  data: 0.1362  max mem: 31081
Val:  [370/728]  eta: 0:04:48  loss: 0.4460 (0.5485)  acc: 78.5714 (78.2409)  time: 0.7050  data: 0.1838  max mem: 31081
Val:  [380/728]  eta: 0:04:40  loss: 0.4891 (0.5512)  acc: 78.5714 (78.2402)  time: 0.8626  data: 0.3402  max mem: 31081
Val:  [390/728]  eta: 0:04:32  loss: 0.3659 (0.5461)  acc: 83.3333 (78.3583)  time: 0.8356  data: 0.3118  max mem: 31081
Val:  [400/728]  eta: 0:04:25  loss: 0.3339 (0.5442)  acc: 83.3333 (78.4913)  time: 0.8729  data: 0.3488  max mem: 31081
Val:  [410/728]  eta: 0:04:17  loss: 0.3467 (0.5415)  acc: 83.3333 (78.5946)  time: 0.8812  data: 0.3586  max mem: 31081
Val:  [420/728]  eta: 0:04:07  loss: 0.3999 (0.5425)  acc: 80.9524 (78.5884)  time: 0.6894  data: 0.1651  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4627 (0.5420)  acc: 79.7619 (78.5742)  time: 0.6922  data: 0.1697  max mem: 31081
Val:  [440/728]  eta: 0:03:51  loss: 0.4627 (0.5407)  acc: 80.9524 (78.5633)  time: 0.8280  data: 0.3097  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.5012 (0.5408)  acc: 83.3333 (78.6084)  time: 0.8313  data: 0.3119  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.5012 (0.5440)  acc: 80.9524 (78.5275)  time: 0.8365  data: 0.3160  max mem: 31081
Val:  [470/728]  eta: 0:03:27  loss: 0.4323 (0.5417)  acc: 79.7619 (78.6068)  time: 0.7918  data: 0.2699  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.4323 (0.5408)  acc: 82.1429 (78.6407)  time: 0.6489  data: 0.1258  max mem: 31081
Val:  [490/728]  eta: 0:03:10  loss: 0.5392 (0.5402)  acc: 76.1905 (78.6636)  time: 0.6924  data: 0.1688  max mem: 31081
Val:  [500/728]  eta: 0:03:02  loss: 0.3831 (0.5388)  acc: 83.3333 (78.7568)  time: 0.8485  data: 0.3266  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3297 (0.5360)  acc: 84.5238 (78.8463)  time: 0.8788  data: 0.3597  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4976 (0.5387)  acc: 76.1905 (78.7565)  time: 0.8563  data: 0.3355  max mem: 31081
Val:  [530/728]  eta: 0:02:38  loss: 0.6471 (0.5398)  acc: 76.1905 (78.7822)  time: 0.7482  data: 0.2260  max mem: 31081
Val:  [540/728]  eta: 0:02:29  loss: 0.5403 (0.5419)  acc: 80.9524 (78.6903)  time: 0.6172  data: 0.0925  max mem: 31081
Val:  [550/728]  eta: 0:02:21  loss: 0.5790 (0.5443)  acc: 72.6190 (78.6146)  time: 0.6710  data: 0.1455  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.5598 (0.5440)  acc: 75.0000 (78.6457)  time: 0.7593  data: 0.2367  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.5108 (0.5512)  acc: 77.3810 (78.5631)  time: 0.7965  data: 0.2738  max mem: 31081
Val:  [580/728]  eta: 0:01:57  loss: 0.4369 (0.5509)  acc: 77.3810 (78.5878)  time: 0.8337  data: 0.3108  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.3913 (0.5514)  acc: 79.7619 (78.5593)  time: 0.7421  data: 0.2186  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.5275 (0.5521)  acc: 79.7619 (78.5140)  time: 0.6866  data: 0.1619  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.5819 (0.5555)  acc: 75.0000 (78.4058)  time: 0.6590  data: 0.1333  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.5562 (0.5589)  acc: 75.0000 (78.3836)  time: 0.7666  data: 0.2409  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4864 (0.5576)  acc: 77.3810 (78.3771)  time: 0.8461  data: 0.3210  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3855 (0.5555)  acc: 78.5714 (78.4303)  time: 0.7890  data: 0.2650  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.3859 (0.5543)  acc: 82.1429 (78.4544)  time: 0.7872  data: 0.2658  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.3916 (0.5532)  acc: 83.3333 (78.4886)  time: 0.7932  data: 0.2712  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.3955 (0.5558)  acc: 79.7619 (78.4224)  time: 0.6910  data: 0.1668  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4687 (0.5550)  acc: 77.3810 (78.4263)  time: 0.6587  data: 0.1360  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3824 (0.5542)  acc: 83.3333 (78.4836)  time: 0.8329  data: 0.3138  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4063 (0.5538)  acc: 82.1429 (78.4661)  time: 0.8694  data: 0.3470  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4867 (0.5543)  acc: 75.0000 (78.4308)  time: 0.8342  data: 0.3111  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.5617 (0.5543)  acc: 75.0000 (78.4228)  time: 0.8311  data: 0.3174  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4946 (0.5553)  acc: 75.0000 (78.3685)  time: 0.8072  data: 0.3174  max mem: 31081
Val: Total time: 0:09:34 (0.7890 s / it)
* Acc@1 78.368 AP 0.7892262935638428 loss 0.555
Accuracy of the network on the 61096 val videos: 78.4%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.43 GB / 503.51 GB
Epoch: [14]  [  0/893]  eta: 3:34:00  lr: 0.001979  min_lr: 0.000003  loss: 0.4680 (0.4680)  class_acc: 0.8036 (0.8036)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 14.3792  data: 13.0526  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.83 GB / 503.51 GB
Epoch: [14]  [ 10/893]  eta: 0:39:12  lr: 0.001978  min_lr: 0.000003  loss: 0.3301 (0.3315)  class_acc: 0.8214 (0.8539)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6643  data: 1.1869  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.93 GB / 503.51 GB
Epoch: [14]  [ 20/893]  eta: 0:30:40  lr: 0.001978  min_lr: 0.000003  loss: 0.3298 (0.3296)  class_acc: 0.8393 (0.8546)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4942  data: 0.0004  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.42 GB / 503.51 GB
Epoch: [14]  [ 30/893]  eta: 0:27:30  lr: 0.001977  min_lr: 0.000003  loss: 0.3298 (0.3282)  class_acc: 0.8393 (0.8514)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4991  data: 0.0006  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.04 GB / 503.51 GB
Epoch: [14]  [ 40/893]  eta: 0:25:51  lr: 0.001977  min_lr: 0.000003  loss: 0.3140 (0.3313)  class_acc: 0.8393 (0.8510)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5160  data: 0.0006  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.11 GB / 503.51 GB
Epoch: [14]  [ 50/893]  eta: 0:24:39  lr: 0.001976  min_lr: 0.000003  loss: 0.3247 (0.3314)  class_acc: 0.8571 (0.8498)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5105  data: 0.0005  max mem: 31081
[2025-03-11 03:15:40,443] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:15:40,443] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2025-03-11 03:15:46,271] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 12544
[2025-03-11 03:15:46,271] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 03:15:46,271] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.10 GB / 503.51 GB
Epoch: [14]  [ 60/893]  eta: 0:23:41  lr: 0.001976  min_lr: 0.000003  loss: 0.3342 (0.3329)  class_acc: 0.8571 (0.8492)  loss_scale: 4096.0000 (4364.5902)  weight_decay: 0.0500 (0.0500)  time: 1.4767  data: 0.0004  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.27 GB / 503.51 GB
Epoch: [14]  [ 70/893]  eta: 0:22:56  lr: 0.001975  min_lr: 0.000003  loss: 0.3315 (0.3319)  class_acc: 0.8393 (0.8493)  loss_scale: 4096.0000 (4326.7606)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.31 GB / 503.51 GB
Epoch: [14]  [ 80/893]  eta: 0:22:20  lr: 0.001975  min_lr: 0.000003  loss: 0.3315 (0.3346)  class_acc: 0.8393 (0.8483)  loss_scale: 4096.0000 (4298.2716)  weight_decay: 0.0500 (0.0500)  time: 1.4721  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.97 GB / 503.51 GB
Epoch: [14]  [ 90/893]  eta: 0:21:47  lr: 0.001974  min_lr: 0.000003  loss: 0.3330 (0.3354)  class_acc: 0.8393 (0.8485)  loss_scale: 4096.0000 (4276.0440)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.75 GB / 503.51 GB
Epoch: [14]  [100/893]  eta: 0:21:18  lr: 0.001974  min_lr: 0.000003  loss: 0.3455 (0.3343)  class_acc: 0.8750 (0.8502)  loss_scale: 4096.0000 (4258.2178)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0002  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.09 GB / 503.51 GB
Epoch: [14]  [110/893]  eta: 0:20:51  lr: 0.001973  min_lr: 0.000003  loss: 0.3154 (0.3336)  class_acc: 0.8571 (0.8496)  loss_scale: 4096.0000 (4243.6036)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.44 GB / 503.51 GB
Epoch: [14]  [120/893]  eta: 0:20:27  lr: 0.001973  min_lr: 0.000003  loss: 0.3027 (0.3329)  class_acc: 0.8571 (0.8492)  loss_scale: 4096.0000 (4231.4050)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.63 GB / 503.51 GB
Epoch: [14]  [130/893]  eta: 0:20:04  lr: 0.001972  min_lr: 0.000003  loss: 0.3279 (0.3335)  class_acc: 0.8393 (0.8486)  loss_scale: 4096.0000 (4221.0687)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.49 GB / 503.51 GB
Epoch: [14]  [140/893]  eta: 0:19:42  lr: 0.001972  min_lr: 0.000003  loss: 0.3457 (0.3354)  class_acc: 0.8571 (0.8487)  loss_scale: 4096.0000 (4212.1986)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.14 GB / 503.51 GB
Epoch: [14]  [150/893]  eta: 0:19:21  lr: 0.001971  min_lr: 0.000003  loss: 0.3457 (0.3349)  class_acc: 0.8393 (0.8489)  loss_scale: 4096.0000 (4204.5033)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.70 GB / 503.51 GB
Epoch: [14]  [160/893]  eta: 0:19:01  lr: 0.001971  min_lr: 0.000003  loss: 0.3359 (0.3354)  class_acc: 0.8393 (0.8493)  loss_scale: 4096.0000 (4197.7640)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.14 GB / 503.51 GB
Epoch: [14]  [170/893]  eta: 0:18:42  lr: 0.001970  min_lr: 0.000003  loss: 0.3257 (0.3340)  class_acc: 0.8571 (0.8497)  loss_scale: 4096.0000 (4191.8129)  weight_decay: 0.0500 (0.0500)  time: 1.4703  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.39 GB / 503.51 GB
Epoch: [14]  [180/893]  eta: 0:18:23  lr: 0.001969  min_lr: 0.000003  loss: 0.3152 (0.3340)  class_acc: 0.8571 (0.8501)  loss_scale: 4096.0000 (4186.5193)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0002  max mem: 31081
[2025-03-11 03:18:55,437] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:18:55,437] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.32 GB / 503.51 GB
Epoch: [14]  [190/893]  eta: 0:18:04  lr: 0.001969  min_lr: 0.000003  loss: 0.3152 (0.3334)  class_acc: 0.8571 (0.8514)  loss_scale: 4096.0000 (4310.4503)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0004  max mem: 31081
[2025-03-11 03:19:05,755] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 12680
[2025-03-11 03:19:05,756] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 03:19:05,756] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.47 GB / 503.51 GB
Epoch: [14]  [200/893]  eta: 0:17:46  lr: 0.001968  min_lr: 0.000003  loss: 0.3281 (0.3347)  class_acc: 0.8571 (0.8508)  loss_scale: 4096.0000 (4320.1592)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0004  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.95 GB / 503.51 GB
Epoch: [14]  [210/893]  eta: 0:17:29  lr: 0.001968  min_lr: 0.000003  loss: 0.3374 (0.3344)  class_acc: 0.8571 (0.8516)  loss_scale: 4096.0000 (4309.5355)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.11 GB / 503.51 GB
Epoch: [14]  [220/893]  eta: 0:17:11  lr: 0.001967  min_lr: 0.000003  loss: 0.3152 (0.3323)  class_acc: 0.8750 (0.8525)  loss_scale: 4096.0000 (4299.8733)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.23 GB / 503.51 GB
Epoch: [14]  [230/893]  eta: 0:16:54  lr: 0.001967  min_lr: 0.000003  loss: 0.2766 (0.3316)  class_acc: 0.8571 (0.8529)  loss_scale: 4096.0000 (4291.0476)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.68 GB / 503.51 GB
Epoch: [14]  [240/893]  eta: 0:16:37  lr: 0.001966  min_lr: 0.000003  loss: 0.2766 (0.3311)  class_acc: 0.8750 (0.8542)  loss_scale: 4096.0000 (4282.9544)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0002  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.11 GB / 503.51 GB
Epoch: [14]  [250/893]  eta: 0:16:19  lr: 0.001966  min_lr: 0.000003  loss: 0.3162 (0.3308)  class_acc: 0.8750 (0.8544)  loss_scale: 4096.0000 (4275.5060)  weight_decay: 0.0500 (0.0500)  time: 1.4612  data: 0.0003  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.39 GB / 503.51 GB
Epoch: [14]  [260/893]  eta: 0:16:03  lr: 0.001965  min_lr: 0.000003  loss: 0.3162 (0.3316)  class_acc: 0.8393 (0.8534)  loss_scale: 4096.0000 (4268.6284)  weight_decay: 0.0500 (0.0500)  time: 1.4627  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.04 GB / 503.51 GB
Epoch: [14]  [270/893]  eta: 0:15:46  lr: 0.001965  min_lr: 0.000003  loss: 0.3425 (0.3319)  class_acc: 0.8393 (0.8537)  loss_scale: 4096.0000 (4262.2583)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0002  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.52 GB / 503.51 GB
Epoch: [14]  [280/893]  eta: 0:15:30  lr: 0.001964  min_lr: 0.000003  loss: 0.3301 (0.3317)  class_acc: 0.8571 (0.8542)  loss_scale: 4096.0000 (4256.3416)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0002  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.78 GB / 503.51 GB
Epoch: [14]  [290/893]  eta: 0:15:13  lr: 0.001964  min_lr: 0.000003  loss: 0.3313 (0.3325)  class_acc: 0.8571 (0.8535)  loss_scale: 4096.0000 (4250.8316)  weight_decay: 0.0500 (0.0500)  time: 1.4576  data: 0.0002  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.96 GB / 503.51 GB
Epoch: [14]  [300/893]  eta: 0:14:57  lr: 0.001963  min_lr: 0.000003  loss: 0.3372 (0.3331)  class_acc: 0.8571 (0.8532)  loss_scale: 4096.0000 (4245.6877)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.30 GB / 503.51 GB
Epoch: [14]  [310/893]  eta: 0:14:41  lr: 0.001963  min_lr: 0.000003  loss: 0.3262 (0.3330)  class_acc: 0.8571 (0.8533)  loss_scale: 4096.0000 (4240.8746)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.52 GB / 503.51 GB
Epoch: [14]  [320/893]  eta: 0:14:25  lr: 0.001962  min_lr: 0.000003  loss: 0.3481 (0.3341)  class_acc: 0.8571 (0.8525)  loss_scale: 4096.0000 (4236.3614)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0002  max mem: 31081
[2025-03-11 03:22:14,521] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:22:14,521] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.04 GB / 503.51 GB
Epoch: [14]  [330/893]  eta: 0:14:09  lr: 0.001962  min_lr: 0.000003  loss: 0.3479 (0.3343)  class_acc: 0.8571 (0.8527)  loss_scale: 4096.0000 (4355.8671)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.31 GB / 503.51 GB
Epoch: [14]  [340/893]  eta: 0:13:54  lr: 0.001961  min_lr: 0.000003  loss: 0.3149 (0.3351)  class_acc: 0.8571 (0.8522)  loss_scale: 8192.0000 (4468.3636)  weight_decay: 0.0500 (0.0500)  time: 1.4673  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.45 GB / 503.51 GB
Epoch: [14]  [350/893]  eta: 0:13:38  lr: 0.001961  min_lr: 0.000003  loss: 0.3286 (0.3354)  class_acc: 0.8393 (0.8520)  loss_scale: 8192.0000 (4574.4501)  weight_decay: 0.0500 (0.0500)  time: 1.4650  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.22 GB / 503.51 GB
Epoch: [14]  [360/893]  eta: 0:13:22  lr: 0.001960  min_lr: 0.000003  loss: 0.3435 (0.3369)  class_acc: 0.8393 (0.8515)  loss_scale: 8192.0000 (4674.6593)  weight_decay: 0.0500 (0.0500)  time: 1.4704  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.79 GB / 503.51 GB
Epoch: [14]  [370/893]  eta: 0:13:07  lr: 0.001960  min_lr: 0.000003  loss: 0.3577 (0.3379)  class_acc: 0.8214 (0.8511)  loss_scale: 8192.0000 (4769.4663)  weight_decay: 0.0500 (0.0500)  time: 1.4778  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.13 GB / 503.51 GB
Epoch: [14]  [380/893]  eta: 0:12:51  lr: 0.001959  min_lr: 0.000003  loss: 0.3750 (0.3388)  class_acc: 0.8214 (0.8506)  loss_scale: 8192.0000 (4859.2966)  weight_decay: 0.0500 (0.0500)  time: 1.4741  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.66 GB / 503.51 GB
Epoch: [14]  [390/893]  eta: 0:12:36  lr: 0.001959  min_lr: 0.000003  loss: 0.3708 (0.3398)  class_acc: 0.8214 (0.8496)  loss_scale: 8192.0000 (4944.5320)  weight_decay: 0.0500 (0.0500)  time: 1.4698  data: 0.0003  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.64 GB / 503.51 GB
Epoch: [14]  [400/893]  eta: 0:12:20  lr: 0.001958  min_lr: 0.000003  loss: 0.3367 (0.3394)  class_acc: 0.8393 (0.8499)  loss_scale: 8192.0000 (5025.5162)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0004  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.10 GB / 503.51 GB
Epoch: [14]  [410/893]  eta: 0:12:05  lr: 0.001958  min_lr: 0.000003  loss: 0.3301 (0.3389)  class_acc: 0.8571 (0.8504)  loss_scale: 8192.0000 (5102.5596)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.34 GB / 503.51 GB
Epoch: [14]  [420/893]  eta: 0:11:49  lr: 0.001957  min_lr: 0.000003  loss: 0.3606 (0.3397)  class_acc: 0.8393 (0.8498)  loss_scale: 8192.0000 (5175.9430)  weight_decay: 0.0500 (0.0500)  time: 1.4591  data: 0.0002  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.99 GB / 503.51 GB
Epoch: [14]  [430/893]  eta: 0:11:34  lr: 0.001957  min_lr: 0.000003  loss: 0.3606 (0.3401)  class_acc: 0.8214 (0.8499)  loss_scale: 8192.0000 (5245.9211)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0002  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.20 GB / 503.51 GB
Epoch: [14]  [440/893]  eta: 0:11:19  lr: 0.001956  min_lr: 0.000003  loss: 0.3235 (0.3395)  class_acc: 0.8571 (0.8505)  loss_scale: 8192.0000 (5312.7256)  weight_decay: 0.0500 (0.0500)  time: 1.4800  data: 0.0003  max mem: 31081
[2025-03-11 03:25:22,627] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:25:22,627] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.54 GB / 503.51 GB
Epoch: [14]  [450/893]  eta: 0:11:04  lr: 0.001955  min_lr: 0.000003  loss: 0.3235 (0.3394)  class_acc: 0.8571 (0.8505)  loss_scale: 8192.0000 (5412.8958)  weight_decay: 0.0500 (0.0500)  time: 1.4742  data: 0.0003  max mem: 31081
[2025-03-11 03:25:27,038] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 12940
[2025-03-11 03:25:27,039] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 03:25:27,039] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.51 GB / 503.51 GB
Epoch: [14]  [460/893]  eta: 0:10:48  lr: 0.001955  min_lr: 0.000003  loss: 0.3323 (0.3395)  class_acc: 0.8571 (0.8506)  loss_scale: 8192.0000 (5490.9501)  weight_decay: 0.0500 (0.0500)  time: 1.4695  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.84 GB / 503.51 GB
Epoch: [14]  [470/893]  eta: 0:10:33  lr: 0.001954  min_lr: 0.000003  loss: 0.3323 (0.3396)  class_acc: 0.8571 (0.8504)  loss_scale: 8192.0000 (5548.2972)  weight_decay: 0.0500 (0.0500)  time: 1.4752  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.49 GB / 503.51 GB
Epoch: [14]  [480/893]  eta: 0:10:18  lr: 0.001954  min_lr: 0.000003  loss: 0.3101 (0.3392)  class_acc: 0.8571 (0.8505)  loss_scale: 8192.0000 (5603.2599)  weight_decay: 0.0500 (0.0500)  time: 1.4731  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.80 GB / 503.51 GB
Epoch: [14]  [490/893]  eta: 0:10:03  lr: 0.001953  min_lr: 0.000003  loss: 0.3354 (0.3401)  class_acc: 0.8571 (0.8505)  loss_scale: 8192.0000 (5655.9837)  weight_decay: 0.0500 (0.0500)  time: 1.4731  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.37 GB / 503.51 GB
Epoch: [14]  [500/893]  eta: 0:09:48  lr: 0.001953  min_lr: 0.000003  loss: 0.3623 (0.3406)  class_acc: 0.8571 (0.8502)  loss_scale: 8192.0000 (5706.6028)  weight_decay: 0.0500 (0.0500)  time: 1.4704  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.17 GB / 503.51 GB
Epoch: [14]  [510/893]  eta: 0:09:32  lr: 0.001952  min_lr: 0.000003  loss: 0.3774 (0.3415)  class_acc: 0.8214 (0.8497)  loss_scale: 8192.0000 (5755.2407)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0003  max mem: 31081
[2025-03-11 03:26:53,914] [INFO] [logging.py:129:log_dist] [Rank 0] step=13000, skipped=75, lr=[2.5497393530277436e-06, 2.5497393530277436e-06, 4.2495655883795724e-06, 4.2495655883795724e-06, 7.082609313965955e-06, 7.082609313965955e-06, 1.1804348856609925e-05, 1.1804348856609925e-05, 1.9673914761016545e-05, 1.9673914761016545e-05, 3.278985793502757e-05, 3.278985793502757e-05, 5.464976322504596e-05, 5.464976322504596e-05, 9.108293870840993e-05, 9.108293870840993e-05, 0.0001518048978473499, 0.0001518048978473499, 0.0002530081630789165, 0.0002530081630789165, 0.00042168027179819416, 0.00042168027179819416, 0.0007028004529969903, 0.0007028004529969903, 0.0011713340883283173, 0.0011713340883283173, 0.0019522234805471955, 0.0019522234805471955], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 03:26:53,914] [INFO] [timer.py:264:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=61.021846436665065, CurrSamplesPerSec=61.560081111407854, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.37 GB / 503.51 GB
Epoch: [14]  [520/893]  eta: 0:09:17  lr: 0.001952  min_lr: 0.000003  loss: 0.3662 (0.3415)  class_acc: 0.8393 (0.8497)  loss_scale: 8192.0000 (5802.0115)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.52 GB / 503.51 GB
Epoch: [14]  [530/893]  eta: 0:09:02  lr: 0.001951  min_lr: 0.000003  loss: 0.3472 (0.3415)  class_acc: 0.8393 (0.8496)  loss_scale: 8192.0000 (5847.0207)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0002  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.39 GB / 503.51 GB
Epoch: [14]  [540/893]  eta: 0:08:47  lr: 0.001951  min_lr: 0.000003  loss: 0.3269 (0.3410)  class_acc: 0.8393 (0.8497)  loss_scale: 8192.0000 (5890.3660)  weight_decay: 0.0500 (0.0500)  time: 1.4599  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.87 GB / 503.51 GB
Epoch: [14]  [550/893]  eta: 0:08:32  lr: 0.001950  min_lr: 0.000003  loss: 0.3381 (0.3409)  class_acc: 0.8571 (0.8498)  loss_scale: 8192.0000 (5932.1379)  weight_decay: 0.0500 (0.0500)  time: 1.4685  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.81 GB / 503.51 GB
Epoch: [14]  [560/893]  eta: 0:08:17  lr: 0.001950  min_lr: 0.000003  loss: 0.3381 (0.3402)  class_acc: 0.8571 (0.8500)  loss_scale: 8192.0000 (5972.4207)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.78 GB / 503.51 GB
Epoch: [14]  [570/893]  eta: 0:08:02  lr: 0.001949  min_lr: 0.000003  loss: 0.3096 (0.3404)  class_acc: 0.8571 (0.8500)  loss_scale: 8192.0000 (6011.2925)  weight_decay: 0.0500 (0.0500)  time: 1.4695  data: 0.0002  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.10 GB / 503.51 GB
Epoch: [14]  [580/893]  eta: 0:07:47  lr: 0.001949  min_lr: 0.000003  loss: 0.3110 (0.3401)  class_acc: 0.8571 (0.8499)  loss_scale: 8192.0000 (6048.8262)  weight_decay: 0.0500 (0.0500)  time: 1.4720  data: 0.0003  max mem: 31081
[2025-03-11 03:28:36,441] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:28:36,441] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-03-11 03:28:49,658] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 13078
[2025-03-11 03:28:49,658] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 03:28:49,658] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.43 GB / 503.51 GB
Epoch: [14]  [590/893]  eta: 0:07:31  lr: 0.001948  min_lr: 0.000003  loss: 0.3347 (0.3405)  class_acc: 0.8393 (0.8498)  loss_scale: 8192.0000 (6209.8409)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0003  max mem: 31081
[2025-03-11 03:28:51,132] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 13079
[2025-03-11 03:28:51,132] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 03:28:51,132] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.01 GB / 503.51 GB
Epoch: [14]  [600/893]  eta: 0:07:16  lr: 0.001947  min_lr: 0.000003  loss: 0.3740 (0.3411)  class_acc: 0.8393 (0.8496)  loss_scale: 4096.0000 (6174.6689)  weight_decay: 0.0500 (0.0500)  time: 1.4725  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.22 GB / 503.51 GB
Epoch: [14]  [610/893]  eta: 0:07:01  lr: 0.001947  min_lr: 0.000003  loss: 0.3689 (0.3416)  class_acc: 0.8214 (0.8491)  loss_scale: 4096.0000 (6140.6481)  weight_decay: 0.0500 (0.0500)  time: 1.4739  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.02 GB / 503.51 GB
Epoch: [14]  [620/893]  eta: 0:06:46  lr: 0.001946  min_lr: 0.000003  loss: 0.3684 (0.3418)  class_acc: 0.8214 (0.8491)  loss_scale: 4096.0000 (6107.7230)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.38 GB / 503.51 GB
Epoch: [14]  [630/893]  eta: 0:06:31  lr: 0.001946  min_lr: 0.000003  loss: 0.3416 (0.3421)  class_acc: 0.8571 (0.8490)  loss_scale: 4096.0000 (6075.8415)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.43 GB / 503.51 GB
Epoch: [14]  [640/893]  eta: 0:06:16  lr: 0.001945  min_lr: 0.000003  loss: 0.2986 (0.3412)  class_acc: 0.8750 (0.8494)  loss_scale: 4096.0000 (6044.9548)  weight_decay: 0.0500 (0.0500)  time: 1.4749  data: 0.0004  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.53 GB / 503.51 GB
Epoch: [14]  [650/893]  eta: 0:06:01  lr: 0.001945  min_lr: 0.000003  loss: 0.2854 (0.3408)  class_acc: 0.8571 (0.8493)  loss_scale: 4096.0000 (6015.0169)  weight_decay: 0.0500 (0.0500)  time: 1.4698  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.77 GB / 503.51 GB
Epoch: [14]  [660/893]  eta: 0:05:46  lr: 0.001944  min_lr: 0.000003  loss: 0.3264 (0.3410)  class_acc: 0.8571 (0.8494)  loss_scale: 4096.0000 (5985.9849)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.82 GB / 503.51 GB
Epoch: [14]  [670/893]  eta: 0:05:32  lr: 0.001944  min_lr: 0.000003  loss: 0.3408 (0.3409)  class_acc: 0.8571 (0.8496)  loss_scale: 4096.0000 (5957.8182)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.23 GB / 503.51 GB
Epoch: [14]  [680/893]  eta: 0:05:17  lr: 0.001943  min_lr: 0.000003  loss: 0.3557 (0.3410)  class_acc: 0.8571 (0.8495)  loss_scale: 4096.0000 (5930.4787)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.18 GB / 503.51 GB
Epoch: [14]  [690/893]  eta: 0:05:02  lr: 0.001943  min_lr: 0.000003  loss: 0.3562 (0.3413)  class_acc: 0.8393 (0.8493)  loss_scale: 4096.0000 (5903.9305)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.83 GB / 503.51 GB
Epoch: [14]  [700/893]  eta: 0:04:47  lr: 0.001942  min_lr: 0.000003  loss: 0.3562 (0.3416)  class_acc: 0.8393 (0.8491)  loss_scale: 4096.0000 (5878.1398)  weight_decay: 0.0500 (0.0500)  time: 1.4669  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.40 GB / 503.51 GB
Epoch: [14]  [710/893]  eta: 0:04:32  lr: 0.001942  min_lr: 0.000003  loss: 0.3604 (0.3417)  class_acc: 0.8214 (0.8489)  loss_scale: 4096.0000 (5853.0745)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0003  max mem: 31081
[2025-03-11 03:32:00,460] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:32:00,460] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.17 GB / 503.51 GB
Epoch: [14]  [720/893]  eta: 0:04:17  lr: 0.001941  min_lr: 0.000003  loss: 0.3718 (0.3423)  class_acc: 0.8214 (0.8483)  loss_scale: 4096.0000 (5834.3856)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.04 GB / 503.51 GB
Epoch: [14]  [730/893]  eta: 0:04:02  lr: 0.001940  min_lr: 0.000003  loss: 0.3669 (0.3427)  class_acc: 0.8214 (0.8483)  loss_scale: 8192.0000 (5866.6375)  weight_decay: 0.0500 (0.0500)  time: 1.4777  data: 0.0002  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.01 GB / 503.51 GB
Epoch: [14]  [740/893]  eta: 0:03:47  lr: 0.001940  min_lr: 0.000003  loss: 0.3506 (0.3431)  class_acc: 0.8571 (0.8482)  loss_scale: 8192.0000 (5898.0189)  weight_decay: 0.0500 (0.0500)  time: 1.4813  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.84 GB / 503.51 GB
Epoch: [14]  [750/893]  eta: 0:03:32  lr: 0.001939  min_lr: 0.000003  loss: 0.3328 (0.3429)  class_acc: 0.8393 (0.8482)  loss_scale: 8192.0000 (5928.5646)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0002  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.78 GB / 503.51 GB
Epoch: [14]  [760/893]  eta: 0:03:17  lr: 0.001939  min_lr: 0.000003  loss: 0.3333 (0.3429)  class_acc: 0.8571 (0.8482)  loss_scale: 8192.0000 (5958.3075)  weight_decay: 0.0500 (0.0500)  time: 1.4603  data: 0.0002  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.33 GB / 503.51 GB
Epoch: [14]  [770/893]  eta: 0:03:02  lr: 0.001938  min_lr: 0.000003  loss: 0.3464 (0.3433)  class_acc: 0.8393 (0.8480)  loss_scale: 8192.0000 (5987.2789)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.57 GB / 503.51 GB
Epoch: [14]  [780/893]  eta: 0:02:47  lr: 0.001938  min_lr: 0.000003  loss: 0.3442 (0.3430)  class_acc: 0.8393 (0.8482)  loss_scale: 8192.0000 (6015.5083)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0004  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.31 GB / 503.51 GB
Epoch: [14]  [790/893]  eta: 0:02:33  lr: 0.001937  min_lr: 0.000003  loss: 0.3196 (0.3427)  class_acc: 0.8750 (0.8485)  loss_scale: 8192.0000 (6043.0240)  weight_decay: 0.0500 (0.0500)  time: 1.4609  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.36 GB / 503.51 GB
Epoch: [14]  [800/893]  eta: 0:02:18  lr: 0.001937  min_lr: 0.000003  loss: 0.3118 (0.3424)  class_acc: 0.8750 (0.8487)  loss_scale: 8192.0000 (6069.8527)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0002  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.02 GB / 503.51 GB
Epoch: [14]  [810/893]  eta: 0:02:03  lr: 0.001936  min_lr: 0.000003  loss: 0.3240 (0.3425)  class_acc: 0.8571 (0.8486)  loss_scale: 8192.0000 (6096.0197)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.38 GB / 503.51 GB
Epoch: [14]  [820/893]  eta: 0:01:48  lr: 0.001936  min_lr: 0.000003  loss: 0.3364 (0.3426)  class_acc: 0.8393 (0.8486)  loss_scale: 8192.0000 (6121.5493)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.27 GB / 503.51 GB
Epoch: [14]  [830/893]  eta: 0:01:33  lr: 0.001935  min_lr: 0.000003  loss: 0.3440 (0.3427)  class_acc: 0.8571 (0.8487)  loss_scale: 8192.0000 (6146.4645)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.60 GB / 503.51 GB
Epoch: [14]  [840/893]  eta: 0:01:18  lr: 0.001934  min_lr: 0.000003  loss: 0.3237 (0.3425)  class_acc: 0.8571 (0.8488)  loss_scale: 8192.0000 (6170.7872)  weight_decay: 0.0500 (0.0500)  time: 1.4731  data: 0.0004  max mem: 31081
[2025-03-11 03:35:08,476] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:35:08,476] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.61 GB / 503.51 GB
Epoch: [14]  [850/893]  eta: 0:01:03  lr: 0.001934  min_lr: 0.000003  loss: 0.2905 (0.3423)  class_acc: 0.8393 (0.8488)  loss_scale: 8192.0000 (6223.4172)  weight_decay: 0.0500 (0.0500)  time: 1.4727  data: 0.0004  max mem: 31081
[2025-03-11 03:35:18,591] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 13343
[2025-03-11 03:35:18,591] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 03:35:18,591] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.60 GB / 503.51 GB
Epoch: [14]  [860/893]  eta: 0:00:48  lr: 0.001933  min_lr: 0.000003  loss: 0.3079 (0.3420)  class_acc: 0.8393 (0.8490)  loss_scale: 8192.0000 (6284.3391)  weight_decay: 0.0500 (0.0500)  time: 1.4508  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.89 GB / 503.51 GB
Epoch: [14]  [870/893]  eta: 0:00:34  lr: 0.001933  min_lr: 0.000003  loss: 0.3176 (0.3422)  class_acc: 0.8393 (0.8488)  loss_scale: 8192.0000 (6306.2411)  weight_decay: 0.0500 (0.0500)  time: 1.4388  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.48 GB / 503.51 GB
Epoch: [14]  [880/893]  eta: 0:00:19  lr: 0.001932  min_lr: 0.000003  loss: 0.3684 (0.3423)  class_acc: 0.8393 (0.8486)  loss_scale: 8192.0000 (6327.6459)  weight_decay: 0.0500 (0.0500)  time: 1.4422  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.90 GB / 503.51 GB
Epoch: [14]  [890/893]  eta: 0:00:04  lr: 0.001932  min_lr: 0.000003  loss: 0.3140 (0.3419)  class_acc: 0.8571 (0.8489)  loss_scale: 8192.0000 (6348.5701)  weight_decay: 0.0500 (0.0500)  time: 1.4422  data: 0.0001  max mem: 31081
Epoch: [14]  [892/893]  eta: 0:00:01  lr: 0.001932  min_lr: 0.000003  loss: 0.3140 (0.3420)  class_acc: 0.8571 (0.8489)  loss_scale: 8192.0000 (6350.6368)  weight_decay: 0.0500 (0.0500)  time: 1.3934  data: 0.0001  max mem: 31081
Epoch: [14] Total time: 0:22:03 (1.4815 s / it)
Averaged stats: lr: 0.001932  min_lr: 0.000003  loss: 0.3140 (0.3420)  class_acc: 0.8571 (0.8489)  loss_scale: 8192.0000 (6350.6368)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:14:37  loss: 0.7264 (0.7264)  acc: 71.4286 (71.4286)  time: 11.0954  data: 10.5614  max mem: 31081
Val:  [ 10/728]  eta: 0:18:38  loss: 0.4614 (0.4990)  acc: 80.9524 (78.2468)  time: 1.5584  data: 1.0344  max mem: 31081
Val:  [ 20/728]  eta: 0:14:16  loss: 0.4174 (0.4920)  acc: 80.9524 (78.9116)  time: 0.7150  data: 0.1908  max mem: 31081
Val:  [ 30/728]  eta: 0:12:44  loss: 0.3554 (0.4800)  acc: 83.3333 (78.7250)  time: 0.8403  data: 0.3174  max mem: 31081
Val:  [ 40/728]  eta: 0:11:56  loss: 0.3554 (0.4891)  acc: 76.1905 (78.7747)  time: 0.8638  data: 0.3413  max mem: 31081
Val:  [ 50/728]  eta: 0:11:17  loss: 0.4974 (0.4760)  acc: 76.1905 (79.3184)  time: 0.8505  data: 0.3260  max mem: 31081
Val:  [ 60/728]  eta: 0:10:20  loss: 0.5489 (0.5129)  acc: 77.3810 (77.9859)  time: 0.7013  data: 0.1769  max mem: 31081
Val:  [ 70/728]  eta: 0:09:51  loss: 0.5870 (0.5266)  acc: 76.1905 (77.4983)  time: 0.6413  data: 0.1187  max mem: 31081
Val:  [ 80/728]  eta: 0:09:40  loss: 0.5870 (0.5542)  acc: 75.0000 (76.7784)  time: 0.7957  data: 0.2746  max mem: 31081
Val:  [ 90/728]  eta: 0:09:25  loss: 0.7083 (0.5837)  acc: 73.8095 (76.1381)  time: 0.8406  data: 0.3173  max mem: 31081
Val:  [100/728]  eta: 0:09:13  loss: 0.5204 (0.5699)  acc: 76.1905 (76.6030)  time: 0.8199  data: 0.2965  max mem: 31081
Val:  [110/728]  eta: 0:09:03  loss: 0.4401 (0.5789)  acc: 78.5714 (76.3406)  time: 0.8554  data: 0.3316  max mem: 31081
Val:  [120/728]  eta: 0:08:47  loss: 0.4065 (0.5648)  acc: 82.1429 (76.9284)  time: 0.7952  data: 0.2686  max mem: 31081
Val:  [130/728]  eta: 0:08:27  loss: 0.3394 (0.5679)  acc: 83.3333 (76.8811)  time: 0.6773  data: 0.1524  max mem: 31081
Val:  [140/728]  eta: 0:08:16  loss: 0.4797 (0.5719)  acc: 78.5714 (76.6211)  time: 0.7076  data: 0.1843  max mem: 31081
Val:  [150/728]  eta: 0:08:03  loss: 0.5348 (0.5744)  acc: 78.5714 (76.6399)  time: 0.7492  data: 0.2246  max mem: 31081
Val:  [160/728]  eta: 0:07:53  loss: 0.4364 (0.5809)  acc: 78.5714 (76.6415)  time: 0.7615  data: 0.2365  max mem: 31081
Val:  [170/728]  eta: 0:07:46  loss: 0.4518 (0.5807)  acc: 80.9524 (76.7126)  time: 0.8338  data: 0.3098  max mem: 31081
Val:  [180/728]  eta: 0:07:32  loss: 0.5487 (0.5833)  acc: 80.9524 (76.6772)  time: 0.7663  data: 0.2435  max mem: 31081
Val:  [190/728]  eta: 0:07:21  loss: 0.5103 (0.5811)  acc: 76.1905 (76.6579)  time: 0.6832  data: 0.1621  max mem: 31081
Val:  [200/728]  eta: 0:07:13  loss: 0.4960 (0.5751)  acc: 76.1905 (76.8597)  time: 0.7718  data: 0.2530  max mem: 31081
Val:  [210/728]  eta: 0:07:07  loss: 0.4757 (0.5782)  acc: 78.5714 (76.9409)  time: 0.8771  data: 0.3567  max mem: 31081
Val:  [220/728]  eta: 0:07:00  loss: 0.4125 (0.5735)  acc: 82.1429 (77.1332)  time: 0.8871  data: 0.3636  max mem: 31081
Val:  [230/728]  eta: 0:06:51  loss: 0.4125 (0.5776)  acc: 80.9524 (77.0408)  time: 0.8290  data: 0.3033  max mem: 31081
Val:  [240/728]  eta: 0:06:37  loss: 0.5939 (0.5765)  acc: 78.5714 (77.2476)  time: 0.6800  data: 0.1559  max mem: 31081
Val:  [250/728]  eta: 0:06:28  loss: 0.4333 (0.5757)  acc: 78.5714 (77.3240)  time: 0.6698  data: 0.1459  max mem: 31081
Val:  [260/728]  eta: 0:06:20  loss: 0.4310 (0.5741)  acc: 80.9524 (77.4220)  time: 0.7979  data: 0.2742  max mem: 31081
Val:  [270/728]  eta: 0:06:13  loss: 0.4310 (0.5748)  acc: 79.7619 (77.4029)  time: 0.8333  data: 0.3108  max mem: 31081
Val:  [280/728]  eta: 0:06:08  loss: 0.4637 (0.5727)  acc: 78.5714 (77.4572)  time: 0.9248  data: 0.4002  max mem: 31081
Val:  [290/728]  eta: 0:05:59  loss: 0.4473 (0.5684)  acc: 79.7619 (77.5855)  time: 0.9071  data: 0.3846  max mem: 31081
Val:  [300/728]  eta: 0:05:47  loss: 0.4473 (0.5687)  acc: 79.7619 (77.5945)  time: 0.6661  data: 0.1424  max mem: 31081
Val:  [310/728]  eta: 0:05:39  loss: 0.5366 (0.5717)  acc: 77.3810 (77.4537)  time: 0.6728  data: 0.1473  max mem: 31081
Val:  [320/728]  eta: 0:05:30  loss: 0.3903 (0.5691)  acc: 82.1429 (77.5701)  time: 0.8018  data: 0.2775  max mem: 31081
Val:  [330/728]  eta: 0:05:23  loss: 0.3713 (0.5646)  acc: 83.3333 (77.6903)  time: 0.8298  data: 0.3037  max mem: 31081
Val:  [340/728]  eta: 0:05:16  loss: 0.3482 (0.5588)  acc: 83.3333 (77.9395)  time: 0.8879  data: 0.3609  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3288 (0.5584)  acc: 84.5238 (78.0322)  time: 0.8416  data: 0.3177  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.4466 (0.5579)  acc: 78.5714 (78.0834)  time: 0.6549  data: 0.1303  max mem: 31081
Val:  [370/728]  eta: 0:04:49  loss: 0.4614 (0.5583)  acc: 78.5714 (78.0227)  time: 0.7040  data: 0.1795  max mem: 31081
Val:  [380/728]  eta: 0:04:41  loss: 0.5032 (0.5611)  acc: 78.5714 (77.9653)  time: 0.8673  data: 0.3436  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.4281 (0.5561)  acc: 79.7619 (78.1239)  time: 0.8397  data: 0.3157  max mem: 31081
Val:  [400/728]  eta: 0:04:26  loss: 0.3703 (0.5547)  acc: 82.1429 (78.2122)  time: 0.8673  data: 0.3449  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.3772 (0.5548)  acc: 82.1429 (78.1572)  time: 0.8865  data: 0.3636  max mem: 31081
Val:  [420/728]  eta: 0:04:08  loss: 0.4362 (0.5538)  acc: 82.1429 (78.2491)  time: 0.6908  data: 0.1702  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4408 (0.5523)  acc: 83.3333 (78.2676)  time: 0.6919  data: 0.1727  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.4356 (0.5502)  acc: 79.7619 (78.3582)  time: 0.8311  data: 0.3107  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4957 (0.5508)  acc: 78.5714 (78.2995)  time: 0.8301  data: 0.3078  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.5216 (0.5551)  acc: 77.3810 (78.2099)  time: 0.8339  data: 0.3122  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.5783 (0.5557)  acc: 73.8095 (78.2075)  time: 0.7907  data: 0.2704  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.5383 (0.5552)  acc: 73.8095 (78.2398)  time: 0.6492  data: 0.1281  max mem: 31081
Val:  [490/728]  eta: 0:03:11  loss: 0.4830 (0.5569)  acc: 72.6190 (78.1592)  time: 0.6888  data: 0.1685  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.5071 (0.5563)  acc: 76.1905 (78.1722)  time: 0.8451  data: 0.3260  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.4117 (0.5531)  acc: 83.3333 (78.2662)  time: 0.8801  data: 0.3607  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4117 (0.5542)  acc: 83.3333 (78.2515)  time: 0.8589  data: 0.3389  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.5709 (0.5547)  acc: 77.3810 (78.2688)  time: 0.7468  data: 0.2258  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.5709 (0.5564)  acc: 76.1905 (78.1819)  time: 0.6132  data: 0.0901  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.6768 (0.5583)  acc: 75.0000 (78.1264)  time: 0.6701  data: 0.1451  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.5079 (0.5588)  acc: 76.1905 (78.1216)  time: 0.7525  data: 0.2302  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.5079 (0.5666)  acc: 77.3810 (78.0335)  time: 0.8011  data: 0.2800  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4324 (0.5668)  acc: 77.3810 (78.0530)  time: 0.8477  data: 0.3236  max mem: 31081
Val:  [590/728]  eta: 0:01:50  loss: 0.5647 (0.5666)  acc: 78.5714 (78.0658)  time: 0.7422  data: 0.2173  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.5647 (0.5678)  acc: 76.1905 (78.0168)  time: 0.6462  data: 0.1255  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.6391 (0.5714)  acc: 73.8095 (77.8914)  time: 0.6585  data: 0.1376  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.7192 (0.5767)  acc: 71.4286 (77.7874)  time: 0.7963  data: 0.2709  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.5302 (0.5748)  acc: 77.3810 (77.8394)  time: 0.8608  data: 0.3348  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3465 (0.5734)  acc: 84.5238 (77.8954)  time: 0.8015  data: 0.2772  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4638 (0.5723)  acc: 83.3333 (77.9040)  time: 0.7826  data: 0.2557  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4044 (0.5702)  acc: 80.9524 (77.9591)  time: 0.7400  data: 0.2141  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4448 (0.5714)  acc: 79.7619 (77.9469)  time: 0.6779  data: 0.1567  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4636 (0.5696)  acc: 79.7619 (77.9998)  time: 0.7134  data: 0.1926  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.4091 (0.5680)  acc: 83.3333 (78.0684)  time: 0.8304  data: 0.3094  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4558 (0.5686)  acc: 79.7619 (78.0382)  time: 0.8627  data: 0.3393  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.5044 (0.5702)  acc: 77.3810 (77.9988)  time: 0.8108  data: 0.2892  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.5166 (0.5706)  acc: 76.1905 (77.9787)  time: 0.7587  data: 0.2443  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.5166 (0.5719)  acc: 76.1905 (77.9364)  time: 0.7353  data: 0.2443  max mem: 31081
Val: Total time: 0:09:33 (0.7884 s / it)
* Acc@1 77.936 AP 0.7950761318206787 loss 0.572
Accuracy of the network on the 61096 val videos: 77.9%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.48 GB / 503.51 GB
Epoch: [15]  [  0/893]  eta: 3:44:06  lr: 0.001932  min_lr: 0.000003  loss: 0.3977 (0.3977)  class_acc: 0.8036 (0.8036)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 15.0575  data: 13.6539  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.10 GB / 503.51 GB
Epoch: [15]  [ 10/893]  eta: 0:40:12  lr: 0.001931  min_lr: 0.000003  loss: 0.3137 (0.3249)  class_acc: 0.8929 (0.8653)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.7321  data: 1.2417  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.46 GB / 503.51 GB
Epoch: [15]  [ 20/893]  eta: 0:31:11  lr: 0.001931  min_lr: 0.000003  loss: 0.3137 (0.3315)  class_acc: 0.8750 (0.8614)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4984  data: 0.0005  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.68 GB / 503.51 GB
Epoch: [15]  [ 30/893]  eta: 0:27:54  lr: 0.001930  min_lr: 0.000003  loss: 0.3462 (0.3441)  class_acc: 0.8393 (0.8497)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5051  data: 0.0005  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.78 GB / 503.51 GB
Epoch: [15]  [ 40/893]  eta: 0:26:03  lr: 0.001929  min_lr: 0.000003  loss: 0.3806 (0.3469)  class_acc: 0.8214 (0.8476)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5073  data: 0.0004  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.55 GB / 503.51 GB
Epoch: [15]  [ 50/893]  eta: 0:24:47  lr: 0.001929  min_lr: 0.000003  loss: 0.3457 (0.3482)  class_acc: 0.8393 (0.8459)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4917  data: 0.0005  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.31 GB / 503.51 GB
Epoch: [15]  [ 60/893]  eta: 0:23:50  lr: 0.001928  min_lr: 0.000003  loss: 0.3110 (0.3435)  class_acc: 0.8571 (0.8498)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4787  data: 0.0005  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.94 GB / 503.51 GB
Epoch: [15]  [ 70/893]  eta: 0:23:03  lr: 0.001928  min_lr: 0.000003  loss: 0.3240 (0.3418)  class_acc: 0.8929 (0.8524)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4677  data: 0.0003  max mem: 31081
[2025-03-11 03:47:49,981] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 13453
[2025-03-11 03:47:49,981] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 03:47:49,981] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.21 GB / 503.51 GB
Epoch: [15]  [ 80/893]  eta: 0:22:23  lr: 0.001927  min_lr: 0.000003  loss: 0.3313 (0.3435)  class_acc: 0.8750 (0.8530)  loss_scale: 8192.0000 (7787.4568)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.84 GB / 503.51 GB
Epoch: [15]  [ 90/893]  eta: 0:21:51  lr: 0.001927  min_lr: 0.000003  loss: 0.3423 (0.3455)  class_acc: 0.8571 (0.8530)  loss_scale: 4096.0000 (7381.8022)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.99 GB / 503.51 GB
Epoch: [15]  [100/893]  eta: 0:21:22  lr: 0.001926  min_lr: 0.000003  loss: 0.3352 (0.3429)  class_acc: 0.8571 (0.8541)  loss_scale: 4096.0000 (7056.4752)  weight_decay: 0.0500 (0.0500)  time: 1.4731  data: 0.0002  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.67 GB / 503.51 GB
Epoch: [15]  [110/893]  eta: 0:20:55  lr: 0.001926  min_lr: 0.000003  loss: 0.2808 (0.3377)  class_acc: 0.8750 (0.8573)  loss_scale: 4096.0000 (6789.7658)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0002  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.40 GB / 503.51 GB
Epoch: [15]  [120/893]  eta: 0:20:29  lr: 0.001925  min_lr: 0.000003  loss: 0.2935 (0.3379)  class_acc: 0.8750 (0.8567)  loss_scale: 4096.0000 (6567.1405)  weight_decay: 0.0500 (0.0500)  time: 1.4593  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.22 GB / 503.51 GB
Epoch: [15]  [130/893]  eta: 0:20:06  lr: 0.001924  min_lr: 0.000003  loss: 0.3494 (0.3415)  class_acc: 0.8393 (0.8540)  loss_scale: 4096.0000 (6378.5038)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0004  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.64 GB / 503.51 GB
Epoch: [15]  [140/893]  eta: 0:19:44  lr: 0.001924  min_lr: 0.000003  loss: 0.3276 (0.3393)  class_acc: 0.8571 (0.8547)  loss_scale: 4096.0000 (6216.6241)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.47 GB / 503.51 GB
Epoch: [15]  [150/893]  eta: 0:19:24  lr: 0.001923  min_lr: 0.000003  loss: 0.3179 (0.3402)  class_acc: 0.8571 (0.8536)  loss_scale: 4096.0000 (6076.1854)  weight_decay: 0.0500 (0.0500)  time: 1.4688  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.58 GB / 503.51 GB
Epoch: [15]  [160/893]  eta: 0:19:04  lr: 0.001923  min_lr: 0.000003  loss: 0.3494 (0.3418)  class_acc: 0.8571 (0.8533)  loss_scale: 4096.0000 (5953.1925)  weight_decay: 0.0500 (0.0500)  time: 1.4746  data: 0.0002  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.61 GB / 503.51 GB
Epoch: [15]  [170/893]  eta: 0:18:44  lr: 0.001922  min_lr: 0.000003  loss: 0.3381 (0.3408)  class_acc: 0.8393 (0.8533)  loss_scale: 4096.0000 (5844.5848)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.81 GB / 503.51 GB
Epoch: [15]  [180/893]  eta: 0:18:25  lr: 0.001922  min_lr: 0.000003  loss: 0.3140 (0.3394)  class_acc: 0.8393 (0.8544)  loss_scale: 4096.0000 (5747.9779)  weight_decay: 0.0500 (0.0500)  time: 1.4701  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.16 GB / 503.51 GB
Epoch: [15]  [190/893]  eta: 0:18:06  lr: 0.001921  min_lr: 0.000003  loss: 0.3210 (0.3396)  class_acc: 0.8571 (0.8539)  loss_scale: 4096.0000 (5661.4869)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.0003  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.20 GB / 503.51 GB
Epoch: [15]  [200/893]  eta: 0:17:48  lr: 0.001921  min_lr: 0.000003  loss: 0.3433 (0.3387)  class_acc: 0.8750 (0.8542)  loss_scale: 4096.0000 (5583.6020)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0003  max mem: 31081
[2025-03-11 03:50:59,123] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:50:59,123] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.97 GB / 503.51 GB
Epoch: [15]  [210/893]  eta: 0:17:30  lr: 0.001920  min_lr: 0.000003  loss: 0.3489 (0.3406)  class_acc: 0.8393 (0.8532)  loss_scale: 4096.0000 (5687.8104)  weight_decay: 0.0500 (0.0500)  time: 1.4650  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.72 GB / 503.51 GB
Epoch: [15]  [220/893]  eta: 0:17:12  lr: 0.001919  min_lr: 0.000003  loss: 0.3730 (0.3411)  class_acc: 0.8393 (0.8528)  loss_scale: 8192.0000 (5801.1222)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.76 GB / 503.51 GB
Epoch: [15]  [230/893]  eta: 0:16:55  lr: 0.001919  min_lr: 0.000003  loss: 0.3198 (0.3404)  class_acc: 0.8571 (0.8534)  loss_scale: 8192.0000 (5904.6234)  weight_decay: 0.0500 (0.0500)  time: 1.4577  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.91 GB / 503.51 GB
Epoch: [15]  [240/893]  eta: 0:16:38  lr: 0.001918  min_lr: 0.000003  loss: 0.3069 (0.3390)  class_acc: 0.8571 (0.8534)  loss_scale: 8192.0000 (5999.5353)  weight_decay: 0.0500 (0.0500)  time: 1.4615  data: 0.0003  max mem: 31081
[2025-03-11 03:52:05,012] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 13627
[2025-03-11 03:52:05,012] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 03:52:05,012] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.03 GB / 503.51 GB
Epoch: [15]  [250/893]  eta: 0:16:21  lr: 0.001918  min_lr: 0.000003  loss: 0.3188 (0.3396)  class_acc: 0.8571 (0.8530)  loss_scale: 8192.0000 (6021.6096)  weight_decay: 0.0500 (0.0500)  time: 1.4688  data: 0.0004  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.78 GB / 503.51 GB
Epoch: [15]  [260/893]  eta: 0:16:04  lr: 0.001917  min_lr: 0.000003  loss: 0.3403 (0.3394)  class_acc: 0.8393 (0.8526)  loss_scale: 4096.0000 (5947.8314)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0004  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.13 GB / 503.51 GB
Epoch: [15]  [270/893]  eta: 0:15:48  lr: 0.001917  min_lr: 0.000003  loss: 0.3232 (0.3383)  class_acc: 0.8393 (0.8529)  loss_scale: 4096.0000 (5879.4982)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0004  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.41 GB / 503.51 GB
Epoch: [15]  [280/893]  eta: 0:15:31  lr: 0.001916  min_lr: 0.000003  loss: 0.3340 (0.3384)  class_acc: 0.8571 (0.8528)  loss_scale: 4096.0000 (5816.0285)  weight_decay: 0.0500 (0.0500)  time: 1.4616  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.78 GB / 503.51 GB
Epoch: [15]  [290/893]  eta: 0:15:15  lr: 0.001915  min_lr: 0.000003  loss: 0.3286 (0.3382)  class_acc: 0.8571 (0.8522)  loss_scale: 4096.0000 (5756.9210)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.30 GB / 503.51 GB
Epoch: [15]  [300/893]  eta: 0:14:58  lr: 0.001915  min_lr: 0.000003  loss: 0.3135 (0.3376)  class_acc: 0.8571 (0.8523)  loss_scale: 4096.0000 (5701.7409)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.08 GB / 503.51 GB
Epoch: [15]  [310/893]  eta: 0:14:42  lr: 0.001914  min_lr: 0.000003  loss: 0.2793 (0.3370)  class_acc: 0.8571 (0.8524)  loss_scale: 4096.0000 (5650.1093)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.11 GB / 503.51 GB
Epoch: [15]  [320/893]  eta: 0:14:26  lr: 0.001914  min_lr: 0.000002  loss: 0.3264 (0.3376)  class_acc: 0.8214 (0.8518)  loss_scale: 4096.0000 (5601.6947)  weight_decay: 0.0500 (0.0500)  time: 1.4708  data: 0.0003  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.98 GB / 503.51 GB
Epoch: [15]  [330/893]  eta: 0:14:11  lr: 0.001913  min_lr: 0.000002  loss: 0.3411 (0.3379)  class_acc: 0.8214 (0.8521)  loss_scale: 4096.0000 (5556.2054)  weight_decay: 0.0500 (0.0500)  time: 1.4791  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.19 GB / 503.51 GB
Epoch: [15]  [340/893]  eta: 0:13:55  lr: 0.001913  min_lr: 0.000002  loss: 0.3176 (0.3376)  class_acc: 0.8571 (0.8520)  loss_scale: 4096.0000 (5513.3842)  weight_decay: 0.0500 (0.0500)  time: 1.4725  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.76 GB / 503.51 GB
Epoch: [15]  [350/893]  eta: 0:13:39  lr: 0.001912  min_lr: 0.000002  loss: 0.3374 (0.3377)  class_acc: 0.8393 (0.8520)  loss_scale: 4096.0000 (5473.0028)  weight_decay: 0.0500 (0.0500)  time: 1.4589  data: 0.0002  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.33 GB / 503.51 GB
Epoch: [15]  [360/893]  eta: 0:13:23  lr: 0.001911  min_lr: 0.000002  loss: 0.3286 (0.3363)  class_acc: 0.8750 (0.8528)  loss_scale: 4096.0000 (5434.8587)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.06 GB / 503.51 GB
Epoch: [15]  [370/893]  eta: 0:13:07  lr: 0.001911  min_lr: 0.000002  loss: 0.2930 (0.3352)  class_acc: 0.8750 (0.8534)  loss_scale: 4096.0000 (5398.7709)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
[2025-03-11 03:55:13,933] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:55:13,933] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.94 GB / 503.51 GB
Epoch: [15]  [380/893]  eta: 0:12:52  lr: 0.001910  min_lr: 0.000002  loss: 0.3142 (0.3349)  class_acc: 0.8750 (0.8537)  loss_scale: 4096.0000 (5418.3307)  weight_decay: 0.0500 (0.0500)  time: 1.4545  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.00 GB / 503.51 GB
Epoch: [15]  [390/893]  eta: 0:12:36  lr: 0.001910  min_lr: 0.000002  loss: 0.3296 (0.3347)  class_acc: 0.8571 (0.8538)  loss_scale: 8192.0000 (5489.2685)  weight_decay: 0.0500 (0.0500)  time: 1.4614  data: 0.0003  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.07 GB / 503.51 GB
Epoch: [15]  [400/893]  eta: 0:12:21  lr: 0.001909  min_lr: 0.000002  loss: 0.3308 (0.3346)  class_acc: 0.8750 (0.8541)  loss_scale: 8192.0000 (5556.6683)  weight_decay: 0.0500 (0.0500)  time: 1.4725  data: 0.0003  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.44 GB / 503.51 GB
Epoch: [15]  [410/893]  eta: 0:12:05  lr: 0.001909  min_lr: 0.000002  loss: 0.3308 (0.3349)  class_acc: 0.8571 (0.8538)  loss_scale: 8192.0000 (5620.7883)  weight_decay: 0.0500 (0.0500)  time: 1.4698  data: 0.0004  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.92 GB / 503.51 GB
Epoch: [15]  [420/893]  eta: 0:11:50  lr: 0.001908  min_lr: 0.000002  loss: 0.3296 (0.3346)  class_acc: 0.8571 (0.8540)  loss_scale: 8192.0000 (5681.8622)  weight_decay: 0.0500 (0.0500)  time: 1.4832  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.95 GB / 503.51 GB
Epoch: [15]  [430/893]  eta: 0:11:35  lr: 0.001908  min_lr: 0.000002  loss: 0.3359 (0.3347)  class_acc: 0.8571 (0.8540)  loss_scale: 8192.0000 (5740.1021)  weight_decay: 0.0500 (0.0500)  time: 1.4874  data: 0.0004  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.62 GB / 503.51 GB
Epoch: [15]  [440/893]  eta: 0:11:20  lr: 0.001907  min_lr: 0.000002  loss: 0.3308 (0.3342)  class_acc: 0.8571 (0.8541)  loss_scale: 8192.0000 (5795.7007)  weight_decay: 0.0500 (0.0500)  time: 1.4771  data: 0.0004  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.60 GB / 503.51 GB
Epoch: [15]  [450/893]  eta: 0:11:04  lr: 0.001906  min_lr: 0.000002  loss: 0.3433 (0.3350)  class_acc: 0.8393 (0.8535)  loss_scale: 8192.0000 (5848.8337)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.30 GB / 503.51 GB
Epoch: [15]  [460/893]  eta: 0:10:49  lr: 0.001906  min_lr: 0.000002  loss: 0.3193 (0.3346)  class_acc: 0.8393 (0.8535)  loss_scale: 8192.0000 (5899.6616)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0002  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.59 GB / 503.51 GB
Epoch: [15]  [470/893]  eta: 0:10:34  lr: 0.001905  min_lr: 0.000002  loss: 0.3186 (0.3341)  class_acc: 0.8571 (0.8538)  loss_scale: 8192.0000 (5948.3312)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0002  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.78 GB / 503.51 GB
Epoch: [15]  [480/893]  eta: 0:10:18  lr: 0.001905  min_lr: 0.000002  loss: 0.2981 (0.3331)  class_acc: 0.8571 (0.8542)  loss_scale: 8192.0000 (5994.9771)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0002  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.14 GB / 503.51 GB
Epoch: [15]  [490/893]  eta: 0:10:03  lr: 0.001904  min_lr: 0.000002  loss: 0.2947 (0.3331)  class_acc: 0.8571 (0.8541)  loss_scale: 8192.0000 (6039.7230)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0004  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.82 GB / 503.51 GB
Epoch: [15]  [500/893]  eta: 0:09:48  lr: 0.001903  min_lr: 0.000002  loss: 0.2966 (0.3324)  class_acc: 0.8393 (0.8539)  loss_scale: 8192.0000 (6082.6826)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0003  max mem: 31081
[2025-03-11 03:58:22,337] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 03:58:22,337] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-03-11 03:58:23,816] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 13885
[2025-03-11 03:58:23,816] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 03:58:23,816] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.92 GB / 503.51 GB
Epoch: [15]  [510/893]  eta: 0:09:33  lr: 0.001903  min_lr: 0.000002  loss: 0.3308 (0.3336)  class_acc: 0.8393 (0.8537)  loss_scale: 8192.0000 (6139.9922)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.21 GB / 503.51 GB
Epoch: [15]  [520/893]  eta: 0:09:18  lr: 0.001902  min_lr: 0.000002  loss: 0.3391 (0.3334)  class_acc: 0.8393 (0.8538)  loss_scale: 8192.0000 (6179.3781)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0004  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.91 GB / 503.51 GB
Epoch: [15]  [530/893]  eta: 0:09:02  lr: 0.001902  min_lr: 0.000002  loss: 0.3206 (0.3329)  class_acc: 0.8750 (0.8542)  loss_scale: 8192.0000 (6217.2806)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0005  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.70 GB / 503.51 GB
Epoch: [15]  [540/893]  eta: 0:08:47  lr: 0.001901  min_lr: 0.000002  loss: 0.3206 (0.3331)  class_acc: 0.8750 (0.8542)  loss_scale: 8192.0000 (6253.7819)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0004  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.09 GB / 503.51 GB
Epoch: [15]  [550/893]  eta: 0:08:32  lr: 0.001901  min_lr: 0.000002  loss: 0.3018 (0.3322)  class_acc: 0.8750 (0.8546)  loss_scale: 8192.0000 (6288.9583)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.06 GB / 503.51 GB
Epoch: [15]  [560/893]  eta: 0:08:17  lr: 0.001900  min_lr: 0.000002  loss: 0.3018 (0.3323)  class_acc: 0.8571 (0.8545)  loss_scale: 8192.0000 (6322.8806)  weight_decay: 0.0500 (0.0500)  time: 1.4740  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.35 GB / 503.51 GB
Epoch: [15]  [570/893]  eta: 0:08:02  lr: 0.001899  min_lr: 0.000002  loss: 0.3037 (0.3319)  class_acc: 0.8571 (0.8546)  loss_scale: 8192.0000 (6355.6147)  weight_decay: 0.0500 (0.0500)  time: 1.4782  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.31 GB / 503.51 GB
Epoch: [15]  [580/893]  eta: 0:07:47  lr: 0.001899  min_lr: 0.000002  loss: 0.3049 (0.3321)  class_acc: 0.8571 (0.8544)  loss_scale: 8192.0000 (6387.2220)  weight_decay: 0.0500 (0.0500)  time: 1.4720  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.22 GB / 503.51 GB
Epoch: [15]  [590/893]  eta: 0:07:32  lr: 0.001898  min_lr: 0.000002  loss: 0.3396 (0.3331)  class_acc: 0.8393 (0.8539)  loss_scale: 8192.0000 (6417.7597)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.83 GB / 503.51 GB
Epoch: [15]  [600/893]  eta: 0:07:17  lr: 0.001898  min_lr: 0.000002  loss: 0.3396 (0.3334)  class_acc: 0.8393 (0.8536)  loss_scale: 8192.0000 (6447.2812)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0002  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.02 GB / 503.51 GB
Epoch: [15]  [610/893]  eta: 0:07:02  lr: 0.001897  min_lr: 0.000002  loss: 0.3250 (0.3328)  class_acc: 0.8571 (0.8541)  loss_scale: 8192.0000 (6475.8363)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0002  max mem: 31081
[2025-03-11 04:01:11,325] [INFO] [logging.py:129:log_dist] [Rank 0] step=14000, skipped=81, lr=[2.477104466197364e-06, 2.477104466197364e-06, 4.128507443662273e-06, 4.128507443662273e-06, 6.880845739437123e-06, 6.880845739437123e-06, 1.1468076232395204e-05, 1.1468076232395204e-05, 1.9113460387325344e-05, 1.9113460387325344e-05, 3.18557673122089e-05, 3.18557673122089e-05, 5.309294552034818e-05, 5.309294552034818e-05, 8.848824253391363e-05, 8.848824253391363e-05, 0.0001474804042231894, 0.0001474804042231894, 0.00024580067370531567, 0.00024580067370531567, 0.00040966778950885945, 0.00040966778950885945, 0.0006827796491814325, 0.0006827796491814325, 0.0011379660819690542, 0.0011379660819690542, 0.0018966101366150903, 0.0018966101366150903], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 04:01:11,327] [INFO] [timer.py:264:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=61.017569100363836, CurrSamplesPerSec=61.59464406665406, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.53 GB / 503.51 GB
Epoch: [15]  [620/893]  eta: 0:06:47  lr: 0.001897  min_lr: 0.000002  loss: 0.3145 (0.3332)  class_acc: 0.8571 (0.8534)  loss_scale: 8192.0000 (6503.4718)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.26 GB / 503.51 GB
Epoch: [15]  [630/893]  eta: 0:06:32  lr: 0.001896  min_lr: 0.000002  loss: 0.3259 (0.3331)  class_acc: 0.8393 (0.8535)  loss_scale: 8192.0000 (6530.2314)  weight_decay: 0.0500 (0.0500)  time: 1.4719  data: 0.0003  max mem: 31081
[2025-03-11 04:01:33,338] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 04:01:33,338] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.81 GB / 503.51 GB
Epoch: [15]  [640/893]  eta: 0:06:17  lr: 0.001895  min_lr: 0.000002  loss: 0.2996 (0.3327)  class_acc: 0.8571 (0.8536)  loss_scale: 8192.0000 (6645.6162)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0003  max mem: 31081
[2025-03-11 04:01:43,562] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 14021
[2025-03-11 04:01:43,563] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 04:01:43,563] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.88 GB / 503.51 GB
Epoch: [15]  [650/893]  eta: 0:06:02  lr: 0.001895  min_lr: 0.000002  loss: 0.2869 (0.3325)  class_acc: 0.8571 (0.8538)  loss_scale: 8192.0000 (6669.3702)  weight_decay: 0.0500 (0.0500)  time: 1.4560  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.39 GB / 503.51 GB
Epoch: [15]  [660/893]  eta: 0:05:47  lr: 0.001894  min_lr: 0.000002  loss: 0.3271 (0.3329)  class_acc: 0.8571 (0.8535)  loss_scale: 8192.0000 (6692.4054)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0004  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.13 GB / 503.51 GB
Epoch: [15]  [670/893]  eta: 0:05:32  lr: 0.001894  min_lr: 0.000002  loss: 0.3655 (0.3336)  class_acc: 0.8393 (0.8531)  loss_scale: 8192.0000 (6714.7541)  weight_decay: 0.0500 (0.0500)  time: 1.4757  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.23 GB / 503.51 GB
Epoch: [15]  [680/893]  eta: 0:05:17  lr: 0.001893  min_lr: 0.000002  loss: 0.3381 (0.3333)  class_acc: 0.8393 (0.8531)  loss_scale: 8192.0000 (6736.4464)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.80 GB / 503.51 GB
Epoch: [15]  [690/893]  eta: 0:05:02  lr: 0.001892  min_lr: 0.000002  loss: 0.3101 (0.3332)  class_acc: 0.8571 (0.8533)  loss_scale: 8192.0000 (6757.5109)  weight_decay: 0.0500 (0.0500)  time: 1.4761  data: 0.0002  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.00 GB / 503.51 GB
Epoch: [15]  [700/893]  eta: 0:04:47  lr: 0.001892  min_lr: 0.000002  loss: 0.3264 (0.3333)  class_acc: 0.8571 (0.8533)  loss_scale: 8192.0000 (6777.9743)  weight_decay: 0.0500 (0.0500)  time: 1.4766  data: 0.0002  max mem: 31081
[2025-03-11 04:03:17,716] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 14085
[2025-03-11 04:03:17,716] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 04:03:17,716] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.74 GB / 503.51 GB
Epoch: [15]  [710/893]  eta: 0:04:32  lr: 0.001891  min_lr: 0.000002  loss: 0.3494 (0.3339)  class_acc: 0.8393 (0.8530)  loss_scale: 8192.0000 (6763.2968)  weight_decay: 0.0500 (0.0500)  time: 1.4723  data: 0.0003  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.32 GB / 503.51 GB
Epoch: [15]  [720/893]  eta: 0:04:17  lr: 0.001891  min_lr: 0.000002  loss: 0.3408 (0.3339)  class_acc: 0.8393 (0.8530)  loss_scale: 4096.0000 (6726.3024)  weight_decay: 0.0500 (0.0500)  time: 1.4726  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.55 GB / 503.51 GB
Epoch: [15]  [730/893]  eta: 0:04:02  lr: 0.001890  min_lr: 0.000002  loss: 0.3386 (0.3342)  class_acc: 0.8571 (0.8527)  loss_scale: 4096.0000 (6690.3201)  weight_decay: 0.0500 (0.0500)  time: 1.4688  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.37 GB / 503.51 GB
Epoch: [15]  [740/893]  eta: 0:03:47  lr: 0.001890  min_lr: 0.000002  loss: 0.3425 (0.3340)  class_acc: 0.8393 (0.8528)  loss_scale: 4096.0000 (6655.3090)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.86 GB / 503.51 GB
Epoch: [15]  [750/893]  eta: 0:03:32  lr: 0.001889  min_lr: 0.000002  loss: 0.3069 (0.3338)  class_acc: 0.8393 (0.8527)  loss_scale: 4096.0000 (6621.2304)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.68 GB / 503.51 GB
Epoch: [15]  [760/893]  eta: 0:03:17  lr: 0.001888  min_lr: 0.000002  loss: 0.3010 (0.3336)  class_acc: 0.8571 (0.8528)  loss_scale: 4096.0000 (6588.0473)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.45 GB / 503.51 GB
Epoch: [15]  [770/893]  eta: 0:03:02  lr: 0.001888  min_lr: 0.000002  loss: 0.2761 (0.3333)  class_acc: 0.8929 (0.8533)  loss_scale: 4096.0000 (6555.7250)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0004  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.35 GB / 503.51 GB
Epoch: [15]  [780/893]  eta: 0:02:48  lr: 0.001887  min_lr: 0.000002  loss: 0.2727 (0.3329)  class_acc: 0.8750 (0.8536)  loss_scale: 4096.0000 (6524.2305)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0004  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.78 GB / 503.51 GB
Epoch: [15]  [790/893]  eta: 0:02:33  lr: 0.001887  min_lr: 0.000002  loss: 0.3171 (0.3329)  class_acc: 0.8750 (0.8537)  loss_scale: 4096.0000 (6493.5322)  weight_decay: 0.0500 (0.0500)  time: 1.4586  data: 0.0004  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.26 GB / 503.51 GB
Epoch: [15]  [800/893]  eta: 0:02:18  lr: 0.001886  min_lr: 0.000002  loss: 0.3167 (0.3332)  class_acc: 0.8571 (0.8536)  loss_scale: 4096.0000 (6463.6005)  weight_decay: 0.0500 (0.0500)  time: 1.4603  data: 0.0004  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.85 GB / 503.51 GB
Epoch: [15]  [810/893]  eta: 0:02:03  lr: 0.001885  min_lr: 0.000002  loss: 0.3352 (0.3337)  class_acc: 0.8571 (0.8534)  loss_scale: 4096.0000 (6434.4069)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.47 GB / 503.51 GB
Epoch: [15]  [820/893]  eta: 0:01:48  lr: 0.001885  min_lr: 0.000002  loss: 0.3352 (0.3335)  class_acc: 0.8571 (0.8534)  loss_scale: 4096.0000 (6405.9245)  weight_decay: 0.0500 (0.0500)  time: 1.4615  data: 0.0002  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.17 GB / 503.51 GB
Epoch: [15]  [830/893]  eta: 0:01:33  lr: 0.001884  min_lr: 0.000002  loss: 0.3093 (0.3335)  class_acc: 0.8571 (0.8532)  loss_scale: 4096.0000 (6378.1276)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0002  max mem: 31081
[2025-03-11 04:06:26,893] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 04:06:26,893] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.53 GB / 503.51 GB
Epoch: [15]  [840/893]  eta: 0:01:18  lr: 0.001884  min_lr: 0.000002  loss: 0.3179 (0.3334)  class_acc: 0.8571 (0.8533)  loss_scale: 4096.0000 (6385.0844)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.48 GB / 503.51 GB
Epoch: [15]  [850/893]  eta: 0:01:03  lr: 0.001883  min_lr: 0.000002  loss: 0.3179 (0.3338)  class_acc: 0.8571 (0.8531)  loss_scale: 8192.0000 (6406.3173)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.32 GB / 503.51 GB
Epoch: [15]  [860/893]  eta: 0:00:48  lr: 0.001882  min_lr: 0.000002  loss: 0.3062 (0.3337)  class_acc: 0.8571 (0.8531)  loss_scale: 8192.0000 (6427.0569)  weight_decay: 0.0500 (0.0500)  time: 1.4524  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.48 GB / 503.51 GB
Epoch: [15]  [870/893]  eta: 0:00:34  lr: 0.001882  min_lr: 0.000002  loss: 0.3108 (0.3340)  class_acc: 0.8571 (0.8529)  loss_scale: 8192.0000 (6447.3203)  weight_decay: 0.0500 (0.0500)  time: 1.4438  data: 0.0002  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.14 GB / 503.51 GB
Epoch: [15]  [880/893]  eta: 0:00:19  lr: 0.001881  min_lr: 0.000002  loss: 0.3328 (0.3341)  class_acc: 0.8393 (0.8529)  loss_scale: 8192.0000 (6467.1237)  weight_decay: 0.0500 (0.0500)  time: 1.4434  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.32 GB / 503.51 GB
Epoch: [15]  [890/893]  eta: 0:00:04  lr: 0.001881  min_lr: 0.000002  loss: 0.3079 (0.3339)  class_acc: 0.8571 (0.8530)  loss_scale: 8192.0000 (6486.4826)  weight_decay: 0.0500 (0.0500)  time: 1.4403  data: 0.0001  max mem: 31081
Epoch: [15]  [892/893]  eta: 0:00:01  lr: 0.001881  min_lr: 0.000002  loss: 0.3218 (0.3340)  class_acc: 0.8571 (0.8529)  loss_scale: 8192.0000 (6488.3946)  weight_decay: 0.0500 (0.0500)  time: 1.3907  data: 0.0001  max mem: 31081
Epoch: [15] Total time: 0:22:03 (1.4824 s / it)
Averaged stats: lr: 0.001881  min_lr: 0.000002  loss: 0.3218 (0.3340)  class_acc: 0.8571 (0.8529)  loss_scale: 8192.0000 (6488.3946)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:14:31  loss: 0.5154 (0.5154)  acc: 77.3810 (77.3810)  time: 11.0876  data: 10.5530  max mem: 31081
Val:  [ 10/728]  eta: 0:18:20  loss: 0.4201 (0.4125)  acc: 80.9524 (80.3030)  time: 1.5326  data: 1.0072  max mem: 31081
Val:  [ 20/728]  eta: 0:14:03  loss: 0.3734 (0.4161)  acc: 80.9524 (80.8390)  time: 0.6973  data: 0.1740  max mem: 31081
Val:  [ 30/728]  eta: 0:12:35  loss: 0.3734 (0.4491)  acc: 80.9524 (79.5315)  time: 0.8342  data: 0.3128  max mem: 31081
Val:  [ 40/728]  eta: 0:11:49  loss: 0.4932 (0.4653)  acc: 75.0000 (78.6005)  time: 0.8616  data: 0.3395  max mem: 31081
Val:  [ 50/728]  eta: 0:11:11  loss: 0.4214 (0.4487)  acc: 77.3810 (79.5985)  time: 0.8497  data: 0.3268  max mem: 31081
Val:  [ 60/728]  eta: 0:10:16  loss: 0.4776 (0.4757)  acc: 77.3810 (78.7080)  time: 0.7035  data: 0.1812  max mem: 31081
Val:  [ 70/728]  eta: 0:09:49  loss: 0.5412 (0.4786)  acc: 76.1905 (78.4205)  time: 0.6514  data: 0.1283  max mem: 31081
Val:  [ 80/728]  eta: 0:09:34  loss: 0.5524 (0.5015)  acc: 78.5714 (77.5426)  time: 0.7766  data: 0.2543  max mem: 31081
Val:  [ 90/728]  eta: 0:09:21  loss: 0.5926 (0.5203)  acc: 73.8095 (76.8577)  time: 0.8285  data: 0.3068  max mem: 31081
Val:  [100/728]  eta: 0:09:09  loss: 0.4362 (0.5124)  acc: 77.3810 (77.1334)  time: 0.8221  data: 0.3007  max mem: 31081
Val:  [110/728]  eta: 0:08:59  loss: 0.4016 (0.5147)  acc: 79.7619 (77.1879)  time: 0.8416  data: 0.3212  max mem: 31081
Val:  [120/728]  eta: 0:08:46  loss: 0.3859 (0.5064)  acc: 80.9524 (77.7843)  time: 0.8212  data: 0.2982  max mem: 31081
Val:  [130/728]  eta: 0:08:24  loss: 0.3442 (0.5043)  acc: 84.5238 (77.8989)  time: 0.6757  data: 0.1517  max mem: 31081
Val:  [140/728]  eta: 0:08:13  loss: 0.4870 (0.5100)  acc: 77.3810 (77.6258)  time: 0.6853  data: 0.1604  max mem: 31081
Val:  [150/728]  eta: 0:08:00  loss: 0.4933 (0.5184)  acc: 77.3810 (77.4756)  time: 0.7517  data: 0.2261  max mem: 31081
Val:  [160/728]  eta: 0:07:53  loss: 0.4954 (0.5246)  acc: 76.1905 (77.3883)  time: 0.7852  data: 0.2615  max mem: 31081
Val:  [170/728]  eta: 0:07:43  loss: 0.4954 (0.5240)  acc: 77.3810 (77.3392)  time: 0.8231  data: 0.2982  max mem: 31081
Val:  [180/728]  eta: 0:07:31  loss: 0.4559 (0.5204)  acc: 78.5714 (77.4665)  time: 0.7540  data: 0.2288  max mem: 31081
Val:  [190/728]  eta: 0:07:18  loss: 0.4340 (0.5211)  acc: 78.5714 (77.4994)  time: 0.6863  data: 0.1629  max mem: 31081
Val:  [200/728]  eta: 0:07:10  loss: 0.4360 (0.5160)  acc: 78.5714 (77.6179)  time: 0.7246  data: 0.2017  max mem: 31081
Val:  [210/728]  eta: 0:07:04  loss: 0.4564 (0.5192)  acc: 78.5714 (77.6179)  time: 0.8616  data: 0.3363  max mem: 31081
Val:  [220/728]  eta: 0:06:57  loss: 0.4448 (0.5145)  acc: 79.7619 (77.7688)  time: 0.8997  data: 0.3721  max mem: 31081
Val:  [230/728]  eta: 0:06:47  loss: 0.4300 (0.5208)  acc: 77.3810 (77.6283)  time: 0.8104  data: 0.2856  max mem: 31081
Val:  [240/728]  eta: 0:06:36  loss: 0.5340 (0.5234)  acc: 77.3810 (77.7860)  time: 0.7072  data: 0.1857  max mem: 31081
Val:  [250/728]  eta: 0:06:25  loss: 0.5288 (0.5245)  acc: 80.9524 (77.8031)  time: 0.6650  data: 0.1367  max mem: 31081
Val:  [260/728]  eta: 0:06:19  loss: 0.4844 (0.5245)  acc: 80.9524 (77.8462)  time: 0.7833  data: 0.2523  max mem: 31081
Val:  [270/728]  eta: 0:06:10  loss: 0.5403 (0.5285)  acc: 73.8095 (77.7236)  time: 0.8326  data: 0.3081  max mem: 31081
Val:  [280/728]  eta: 0:06:05  loss: 0.5302 (0.5286)  acc: 72.6190 (77.6987)  time: 0.8810  data: 0.3561  max mem: 31081
Val:  [290/728]  eta: 0:05:57  loss: 0.4384 (0.5236)  acc: 80.9524 (77.9128)  time: 0.9065  data: 0.3805  max mem: 31081
Val:  [300/728]  eta: 0:05:44  loss: 0.3979 (0.5228)  acc: 82.1429 (77.9663)  time: 0.6664  data: 0.1418  max mem: 31081
Val:  [310/728]  eta: 0:05:37  loss: 0.6238 (0.5273)  acc: 72.6190 (77.7255)  time: 0.6811  data: 0.1572  max mem: 31081
Val:  [320/728]  eta: 0:05:28  loss: 0.6297 (0.5277)  acc: 72.6190 (77.6665)  time: 0.8129  data: 0.2910  max mem: 31081
Val:  [330/728]  eta: 0:05:21  loss: 0.3913 (0.5233)  acc: 80.9524 (77.8449)  time: 0.8280  data: 0.3058  max mem: 31081
Val:  [340/728]  eta: 0:05:14  loss: 0.3326 (0.5182)  acc: 83.3333 (78.0268)  time: 0.8890  data: 0.3669  max mem: 31081
Val:  [350/728]  eta: 0:05:06  loss: 0.3478 (0.5181)  acc: 83.3333 (78.0593)  time: 0.8527  data: 0.3324  max mem: 31081
Val:  [360/728]  eta: 0:04:55  loss: 0.4229 (0.5152)  acc: 78.5714 (78.1130)  time: 0.6601  data: 0.1374  max mem: 31081
Val:  [370/728]  eta: 0:04:48  loss: 0.4348 (0.5154)  acc: 77.3810 (78.0612)  time: 0.7006  data: 0.1777  max mem: 31081
Val:  [380/728]  eta: 0:04:40  loss: 0.4685 (0.5173)  acc: 77.3810 (77.9996)  time: 0.8565  data: 0.3346  max mem: 31081
Val:  [390/728]  eta: 0:04:32  loss: 0.3718 (0.5129)  acc: 82.1429 (78.1391)  time: 0.8321  data: 0.3099  max mem: 31081
Val:  [400/728]  eta: 0:04:25  loss: 0.3654 (0.5124)  acc: 83.3333 (78.2300)  time: 0.8672  data: 0.3445  max mem: 31081
Val:  [410/728]  eta: 0:04:17  loss: 0.3665 (0.5106)  acc: 80.9524 (78.2586)  time: 0.8753  data: 0.3511  max mem: 31081
Val:  [420/728]  eta: 0:04:07  loss: 0.4318 (0.5096)  acc: 79.7619 (78.3141)  time: 0.6841  data: 0.1595  max mem: 31081
Val:  [430/728]  eta: 0:03:59  loss: 0.4318 (0.5094)  acc: 79.7619 (78.2593)  time: 0.6922  data: 0.1702  max mem: 31081
Val:  [440/728]  eta: 0:03:51  loss: 0.3931 (0.5085)  acc: 79.7619 (78.2502)  time: 0.8221  data: 0.3009  max mem: 31081
Val:  [450/728]  eta: 0:03:43  loss: 0.4425 (0.5100)  acc: 77.3810 (78.1913)  time: 0.8211  data: 0.2985  max mem: 31081
Val:  [460/728]  eta: 0:03:35  loss: 0.4984 (0.5113)  acc: 76.1905 (78.1737)  time: 0.8333  data: 0.3140  max mem: 31081
Val:  [470/728]  eta: 0:03:27  loss: 0.4050 (0.5091)  acc: 79.7619 (78.2530)  time: 0.7895  data: 0.2703  max mem: 31081
Val:  [480/728]  eta: 0:03:17  loss: 0.4050 (0.5079)  acc: 78.5714 (78.2596)  time: 0.6454  data: 0.1241  max mem: 31081
Val:  [490/728]  eta: 0:03:10  loss: 0.4734 (0.5075)  acc: 76.1905 (78.2635)  time: 0.6884  data: 0.1682  max mem: 31081
Val:  [500/728]  eta: 0:03:02  loss: 0.4649 (0.5070)  acc: 78.5714 (78.2887)  time: 0.8472  data: 0.3258  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3544 (0.5047)  acc: 82.1429 (78.3734)  time: 0.8992  data: 0.3596  max mem: 31081
Val:  [520/728]  eta: 0:02:46  loss: 0.3960 (0.5063)  acc: 82.1429 (78.2881)  time: 0.8557  data: 0.3172  max mem: 31081
Val:  [530/728]  eta: 0:02:38  loss: 0.5921 (0.5064)  acc: 75.0000 (78.3046)  time: 0.7251  data: 0.2041  max mem: 31081
Val:  [540/728]  eta: 0:02:29  loss: 0.5451 (0.5088)  acc: 75.0000 (78.1995)  time: 0.6102  data: 0.0889  max mem: 31081
Val:  [550/728]  eta: 0:02:21  loss: 0.4755 (0.5091)  acc: 75.0000 (78.1458)  time: 0.6620  data: 0.1379  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.4603 (0.5090)  acc: 78.5714 (78.1534)  time: 0.7420  data: 0.2159  max mem: 31081
Val:  [570/728]  eta: 0:02:05  loss: 0.4978 (0.5135)  acc: 77.3810 (78.0648)  time: 0.7887  data: 0.2649  max mem: 31081
Val:  [580/728]  eta: 0:01:57  loss: 0.4211 (0.5133)  acc: 77.3810 (78.0756)  time: 0.8348  data: 0.3116  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.4155 (0.5139)  acc: 79.7619 (78.0799)  time: 0.7291  data: 0.2053  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4894 (0.5137)  acc: 79.7619 (78.0822)  time: 0.6331  data: 0.1104  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.5192 (0.5166)  acc: 71.4286 (77.9460)  time: 0.6594  data: 0.1376  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.5406 (0.5194)  acc: 71.4286 (77.9139)  time: 0.8355  data: 0.3151  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4456 (0.5187)  acc: 79.7619 (77.9413)  time: 0.8679  data: 0.3467  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3865 (0.5172)  acc: 82.1429 (78.0124)  time: 0.7833  data: 0.2609  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.3865 (0.5157)  acc: 84.5238 (78.1051)  time: 0.7953  data: 0.2729  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4064 (0.5147)  acc: 84.5238 (78.1572)  time: 0.7299  data: 0.2077  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4469 (0.5160)  acc: 80.9524 (78.1261)  time: 0.6793  data: 0.1570  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.3810 (0.5146)  acc: 80.9524 (78.1921)  time: 0.7423  data: 0.2189  max mem: 31081
Val:  [690/728]  eta: 0:00:29  loss: 0.3746 (0.5143)  acc: 84.5238 (78.2045)  time: 0.8282  data: 0.3029  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4280 (0.5145)  acc: 79.7619 (78.1418)  time: 0.8421  data: 0.3160  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.3945 (0.5142)  acc: 80.9524 (78.1411)  time: 0.8000  data: 0.2775  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.3971 (0.5137)  acc: 78.5714 (78.1372)  time: 0.7467  data: 0.2317  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4366 (0.5144)  acc: 79.7619 (78.1508)  time: 0.7237  data: 0.2317  max mem: 31081
Val: Total time: 0:09:31 (0.7848 s / it)
* Acc@1 78.151 AP 0.7895311117172241 loss 0.514
Accuracy of the network on the 61096 val videos: 78.2%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.64 GB / 503.51 GB
Epoch: [16]  [  0/893]  eta: 3:30:28  lr: 0.001881  min_lr: 0.000002  loss: 0.3337 (0.3337)  class_acc: 0.8214 (0.8214)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 14.1419  data: 12.8867  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.64 GB / 503.51 GB
Epoch: [16]  [ 10/893]  eta: 0:39:12  lr: 0.001880  min_lr: 0.000002  loss: 0.3342 (0.3331)  class_acc: 0.8571 (0.8539)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6642  data: 1.1718  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.92 GB / 503.51 GB
Epoch: [16]  [ 20/893]  eta: 0:30:44  lr: 0.001879  min_lr: 0.000002  loss: 0.3237 (0.3206)  class_acc: 0.8571 (0.8597)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5108  data: 0.0004  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.04 GB / 503.51 GB
Epoch: [16]  [ 30/893]  eta: 0:27:35  lr: 0.001879  min_lr: 0.000002  loss: 0.3157 (0.3209)  class_acc: 0.8571 (0.8589)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5073  data: 0.0004  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.92 GB / 503.51 GB
Epoch: [16]  [ 40/893]  eta: 0:25:50  lr: 0.001878  min_lr: 0.000002  loss: 0.3162 (0.3173)  class_acc: 0.8571 (0.8606)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5075  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.51 GB / 503.51 GB
Epoch: [16]  [ 50/893]  eta: 0:24:38  lr: 0.001878  min_lr: 0.000002  loss: 0.3213 (0.3264)  class_acc: 0.8393 (0.8561)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4999  data: 0.0005  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.15 GB / 503.51 GB
Epoch: [16]  [ 60/893]  eta: 0:23:42  lr: 0.001877  min_lr: 0.000002  loss: 0.3547 (0.3301)  class_acc: 0.8393 (0.8545)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4841  data: 0.0004  max mem: 31081
[2025-03-11 04:19:21,440] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 04:19:21,440] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.04 GB / 503.51 GB
Epoch: [16]  [ 70/893]  eta: 0:22:57  lr: 0.001876  min_lr: 0.000002  loss: 0.3411 (0.3305)  class_acc: 0.8393 (0.8544)  loss_scale: 8192.0000 (8307.3803)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0002  max mem: 31081
[2025-03-11 04:19:25,792] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 14345
[2025-03-11 04:19:25,793] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 04:19:25,793] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.30 GB / 503.51 GB
Epoch: [16]  [ 80/893]  eta: 0:22:20  lr: 0.001876  min_lr: 0.000002  loss: 0.3257 (0.3323)  class_acc: 0.8393 (0.8505)  loss_scale: 8192.0000 (8495.4074)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.85 GB / 503.51 GB
Epoch: [16]  [ 90/893]  eta: 0:21:47  lr: 0.001875  min_lr: 0.000002  loss: 0.3250 (0.3325)  class_acc: 0.8393 (0.8487)  loss_scale: 8192.0000 (8462.0659)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0002  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.19 GB / 503.51 GB
Epoch: [16]  [100/893]  eta: 0:21:17  lr: 0.001875  min_lr: 0.000002  loss: 0.3225 (0.3311)  class_acc: 0.8393 (0.8492)  loss_scale: 8192.0000 (8435.3267)  weight_decay: 0.0500 (0.0500)  time: 1.4606  data: 0.0002  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.61 GB / 503.51 GB
Epoch: [16]  [110/893]  eta: 0:20:50  lr: 0.001874  min_lr: 0.000002  loss: 0.3298 (0.3302)  class_acc: 0.8393 (0.8502)  loss_scale: 8192.0000 (8413.4054)  weight_decay: 0.0500 (0.0500)  time: 1.4582  data: 0.0002  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.12 GB / 503.51 GB
Epoch: [16]  [120/893]  eta: 0:20:26  lr: 0.001873  min_lr: 0.000002  loss: 0.3313 (0.3320)  class_acc: 0.8393 (0.8495)  loss_scale: 8192.0000 (8395.1074)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0003  max mem: 31081
[2025-03-11 04:20:47,656] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 14401
[2025-03-11 04:20:47,656] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 04:20:47,656] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.61 GB / 503.51 GB
Epoch: [16]  [130/893]  eta: 0:20:02  lr: 0.001873  min_lr: 0.000002  loss: 0.3416 (0.3356)  class_acc: 0.8393 (0.8484)  loss_scale: 8192.0000 (8317.0687)  weight_decay: 0.0500 (0.0500)  time: 1.4591  data: 0.0004  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.64 GB / 503.51 GB
Epoch: [16]  [140/893]  eta: 0:19:40  lr: 0.001872  min_lr: 0.000002  loss: 0.3428 (0.3374)  class_acc: 0.8393 (0.8484)  loss_scale: 4096.0000 (8017.7021)  weight_decay: 0.0500 (0.0500)  time: 1.4589  data: 0.0004  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.58 GB / 503.51 GB
Epoch: [16]  [150/893]  eta: 0:19:20  lr: 0.001872  min_lr: 0.000002  loss: 0.3101 (0.3358)  class_acc: 0.8750 (0.8497)  loss_scale: 4096.0000 (7757.9868)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.19 GB / 503.51 GB
Epoch: [16]  [160/893]  eta: 0:18:59  lr: 0.001871  min_lr: 0.000002  loss: 0.2954 (0.3354)  class_acc: 0.8750 (0.8506)  loss_scale: 4096.0000 (7530.5342)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.77 GB / 503.51 GB
Epoch: [16]  [170/893]  eta: 0:18:40  lr: 0.001870  min_lr: 0.000002  loss: 0.2986 (0.3344)  class_acc: 0.8750 (0.8517)  loss_scale: 4096.0000 (7329.6842)  weight_decay: 0.0500 (0.0500)  time: 1.4619  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.22 GB / 503.51 GB
Epoch: [16]  [180/893]  eta: 0:18:22  lr: 0.001870  min_lr: 0.000002  loss: 0.2986 (0.3331)  class_acc: 0.8571 (0.8518)  loss_scale: 4096.0000 (7151.0276)  weight_decay: 0.0500 (0.0500)  time: 1.4729  data: 0.0002  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.87 GB / 503.51 GB
Epoch: [16]  [190/893]  eta: 0:18:04  lr: 0.001869  min_lr: 0.000002  loss: 0.3245 (0.3321)  class_acc: 0.8571 (0.8525)  loss_scale: 4096.0000 (6991.0785)  weight_decay: 0.0500 (0.0500)  time: 1.4787  data: 0.0003  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.74 GB / 503.51 GB
Epoch: [16]  [200/893]  eta: 0:17:46  lr: 0.001869  min_lr: 0.000002  loss: 0.3096 (0.3308)  class_acc: 0.8571 (0.8531)  loss_scale: 4096.0000 (6847.0448)  weight_decay: 0.0500 (0.0500)  time: 1.4698  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.77 GB / 503.51 GB
Epoch: [16]  [210/893]  eta: 0:17:28  lr: 0.001868  min_lr: 0.000002  loss: 0.2781 (0.3282)  class_acc: 0.8750 (0.8545)  loss_scale: 4096.0000 (6716.6635)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.17 GB / 503.51 GB
Epoch: [16]  [220/893]  eta: 0:17:11  lr: 0.001867  min_lr: 0.000002  loss: 0.2830 (0.3267)  class_acc: 0.8750 (0.8554)  loss_scale: 4096.0000 (6598.0814)  weight_decay: 0.0500 (0.0500)  time: 1.4732  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.07 GB / 503.51 GB
Epoch: [16]  [230/893]  eta: 0:16:54  lr: 0.001867  min_lr: 0.000002  loss: 0.2852 (0.3266)  class_acc: 0.8750 (0.8559)  loss_scale: 4096.0000 (6489.7662)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0002  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.83 GB / 503.51 GB
Epoch: [16]  [240/893]  eta: 0:16:37  lr: 0.001866  min_lr: 0.000002  loss: 0.2949 (0.3264)  class_acc: 0.8750 (0.8559)  loss_scale: 4096.0000 (6390.4398)  weight_decay: 0.0500 (0.0500)  time: 1.4740  data: 0.0002  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.97 GB / 503.51 GB
Epoch: [16]  [250/893]  eta: 0:16:20  lr: 0.001866  min_lr: 0.000002  loss: 0.3245 (0.3273)  class_acc: 0.8393 (0.8551)  loss_scale: 4096.0000 (6299.0279)  weight_decay: 0.0500 (0.0500)  time: 1.4719  data: 0.0002  max mem: 31081
[2025-03-11 04:23:57,210] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 04:23:57,210] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.03 GB / 503.51 GB
Epoch: [16]  [260/893]  eta: 0:16:04  lr: 0.001865  min_lr: 0.000002  loss: 0.3535 (0.3285)  class_acc: 0.8393 (0.8552)  loss_scale: 4096.0000 (6261.7011)  weight_decay: 0.0500 (0.0500)  time: 1.4688  data: 0.0002  max mem: 31081
[2025-03-11 04:24:04,589] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 14535
[2025-03-11 04:24:04,589] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 04:24:04,589] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.28 GB / 503.51 GB
Epoch: [16]  [270/893]  eta: 0:15:47  lr: 0.001864  min_lr: 0.000002  loss: 0.3679 (0.3297)  class_acc: 0.8393 (0.8550)  loss_scale: 4096.0000 (6212.0148)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.97 GB / 503.51 GB
Epoch: [16]  [280/893]  eta: 0:15:31  lr: 0.001864  min_lr: 0.000002  loss: 0.3621 (0.3301)  class_acc: 0.8393 (0.8548)  loss_scale: 4096.0000 (6136.7117)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.22 GB / 503.51 GB
Epoch: [16]  [290/893]  eta: 0:15:15  lr: 0.001863  min_lr: 0.000002  loss: 0.3140 (0.3292)  class_acc: 0.8393 (0.8551)  loss_scale: 4096.0000 (6066.5842)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0002  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.64 GB / 503.51 GB
Epoch: [16]  [300/893]  eta: 0:14:58  lr: 0.001863  min_lr: 0.000002  loss: 0.3105 (0.3296)  class_acc: 0.8571 (0.8550)  loss_scale: 4096.0000 (6001.1163)  weight_decay: 0.0500 (0.0500)  time: 1.4681  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.38 GB / 503.51 GB
Epoch: [16]  [310/893]  eta: 0:14:42  lr: 0.001862  min_lr: 0.000002  loss: 0.2920 (0.3282)  class_acc: 0.8571 (0.8560)  loss_scale: 4096.0000 (5939.8585)  weight_decay: 0.0500 (0.0500)  time: 1.4723  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.69 GB / 503.51 GB
Epoch: [16]  [320/893]  eta: 0:14:27  lr: 0.001861  min_lr: 0.000002  loss: 0.2864 (0.3286)  class_acc: 0.8571 (0.8555)  loss_scale: 4096.0000 (5882.4174)  weight_decay: 0.0500 (0.0500)  time: 1.4746  data: 0.0003  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.80 GB / 503.51 GB
Epoch: [16]  [330/893]  eta: 0:14:11  lr: 0.001861  min_lr: 0.000002  loss: 0.3279 (0.3292)  class_acc: 0.8393 (0.8554)  loss_scale: 4096.0000 (5828.4471)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0002  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.52 GB / 503.51 GB
Epoch: [16]  [340/893]  eta: 0:13:55  lr: 0.001860  min_lr: 0.000002  loss: 0.3245 (0.3291)  class_acc: 0.8571 (0.8550)  loss_scale: 4096.0000 (5777.6422)  weight_decay: 0.0500 (0.0500)  time: 1.4723  data: 0.0002  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.42 GB / 503.51 GB
Epoch: [16]  [350/893]  eta: 0:13:39  lr: 0.001859  min_lr: 0.000002  loss: 0.3171 (0.3284)  class_acc: 0.8393 (0.8550)  loss_scale: 4096.0000 (5729.7322)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.51 GB / 503.51 GB
Epoch: [16]  [360/893]  eta: 0:13:23  lr: 0.001859  min_lr: 0.000002  loss: 0.3257 (0.3291)  class_acc: 0.8571 (0.8550)  loss_scale: 4096.0000 (5684.4765)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.59 GB / 503.51 GB
Epoch: [16]  [370/893]  eta: 0:13:08  lr: 0.001858  min_lr: 0.000002  loss: 0.3208 (0.3294)  class_acc: 0.8571 (0.8551)  loss_scale: 4096.0000 (5641.6604)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.65 GB / 503.51 GB
Epoch: [16]  [380/893]  eta: 0:12:52  lr: 0.001858  min_lr: 0.000002  loss: 0.3308 (0.3298)  class_acc: 0.8393 (0.8548)  loss_scale: 4096.0000 (5601.0919)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.44 GB / 503.51 GB
Epoch: [16]  [390/893]  eta: 0:12:37  lr: 0.001857  min_lr: 0.000002  loss: 0.3123 (0.3300)  class_acc: 0.8393 (0.8544)  loss_scale: 4096.0000 (5562.5985)  weight_decay: 0.0500 (0.0500)  time: 1.4797  data: 0.0005  max mem: 31081
[2025-03-11 04:27:14,308] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 04:27:14,308] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.54 GB / 503.51 GB
Epoch: [16]  [400/893]  eta: 0:12:21  lr: 0.001856  min_lr: 0.000002  loss: 0.3074 (0.3301)  class_acc: 0.8571 (0.8542)  loss_scale: 4096.0000 (5617.9551)  weight_decay: 0.0500 (0.0500)  time: 1.4794  data: 0.0005  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.60 GB / 503.51 GB
Epoch: [16]  [410/893]  eta: 0:12:06  lr: 0.001856  min_lr: 0.000002  loss: 0.3252 (0.3305)  class_acc: 0.8393 (0.8542)  loss_scale: 8192.0000 (5680.5839)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0004  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.84 GB / 503.51 GB
Epoch: [16]  [420/893]  eta: 0:11:50  lr: 0.001855  min_lr: 0.000002  loss: 0.3127 (0.3297)  class_acc: 0.8750 (0.8549)  loss_scale: 8192.0000 (5740.2375)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.28 GB / 503.51 GB
Epoch: [16]  [430/893]  eta: 0:11:35  lr: 0.001855  min_lr: 0.000002  loss: 0.2974 (0.3295)  class_acc: 0.8750 (0.8549)  loss_scale: 8192.0000 (5797.1230)  weight_decay: 0.0500 (0.0500)  time: 1.4712  data: 0.0002  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.39 GB / 503.51 GB
Epoch: [16]  [440/893]  eta: 0:11:20  lr: 0.001854  min_lr: 0.000002  loss: 0.3352 (0.3307)  class_acc: 0.8571 (0.8544)  loss_scale: 8192.0000 (5851.4286)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0002  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.06 GB / 503.51 GB
Epoch: [16]  [450/893]  eta: 0:11:04  lr: 0.001853  min_lr: 0.000002  loss: 0.3367 (0.3305)  class_acc: 0.8571 (0.8546)  loss_scale: 8192.0000 (5903.3259)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
[2025-03-11 04:28:42,490] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 14724
[2025-03-11 04:28:42,490] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 04:28:42,490] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.38 GB / 503.51 GB
Epoch: [16]  [460/893]  eta: 0:10:49  lr: 0.001853  min_lr: 0.000002  loss: 0.2925 (0.3298)  class_acc: 0.8571 (0.8548)  loss_scale: 8192.0000 (5873.0065)  weight_decay: 0.0500 (0.0500)  time: 1.4792  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.14 GB / 503.51 GB
Epoch: [16]  [470/893]  eta: 0:10:34  lr: 0.001852  min_lr: 0.000002  loss: 0.2930 (0.3302)  class_acc: 0.8571 (0.8547)  loss_scale: 4096.0000 (5835.2781)  weight_decay: 0.0500 (0.0500)  time: 1.4815  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.30 GB / 503.51 GB
Epoch: [16]  [480/893]  eta: 0:10:19  lr: 0.001851  min_lr: 0.000002  loss: 0.3162 (0.3295)  class_acc: 0.8571 (0.8549)  loss_scale: 4096.0000 (5799.1185)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.32 GB / 503.51 GB
Epoch: [16]  [490/893]  eta: 0:10:03  lr: 0.001851  min_lr: 0.000002  loss: 0.3162 (0.3295)  class_acc: 0.8750 (0.8553)  loss_scale: 4096.0000 (5764.4318)  weight_decay: 0.0500 (0.0500)  time: 1.4716  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.60 GB / 503.51 GB
Epoch: [16]  [500/893]  eta: 0:09:48  lr: 0.001850  min_lr: 0.000002  loss: 0.3228 (0.3294)  class_acc: 0.8750 (0.8556)  loss_scale: 4096.0000 (5731.1297)  weight_decay: 0.0500 (0.0500)  time: 1.4711  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.07 GB / 503.51 GB
Epoch: [16]  [510/893]  eta: 0:09:33  lr: 0.001850  min_lr: 0.000002  loss: 0.3228 (0.3294)  class_acc: 0.8750 (0.8556)  loss_scale: 4096.0000 (5699.1311)  weight_decay: 0.0500 (0.0500)  time: 1.4693  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.15 GB / 503.51 GB
Epoch: [16]  [520/893]  eta: 0:09:18  lr: 0.001849  min_lr: 0.000002  loss: 0.3267 (0.3296)  class_acc: 0.8571 (0.8555)  loss_scale: 4096.0000 (5668.3608)  weight_decay: 0.0500 (0.0500)  time: 1.4760  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.62 GB / 503.51 GB
Epoch: [16]  [530/893]  eta: 0:09:03  lr: 0.001848  min_lr: 0.000002  loss: 0.3389 (0.3300)  class_acc: 0.8393 (0.8551)  loss_scale: 4096.0000 (5638.7495)  weight_decay: 0.0500 (0.0500)  time: 1.4791  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.73 GB / 503.51 GB
Epoch: [16]  [540/893]  eta: 0:08:48  lr: 0.001848  min_lr: 0.000002  loss: 0.3428 (0.3304)  class_acc: 0.8393 (0.8549)  loss_scale: 4096.0000 (5610.2329)  weight_decay: 0.0500 (0.0500)  time: 1.4750  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.05 GB / 503.51 GB
Epoch: [16]  [550/893]  eta: 0:08:33  lr: 0.001847  min_lr: 0.000002  loss: 0.3428 (0.3307)  class_acc: 0.8393 (0.8548)  loss_scale: 4096.0000 (5582.7514)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.94 GB / 503.51 GB
Epoch: [16]  [560/893]  eta: 0:08:17  lr: 0.001847  min_lr: 0.000002  loss: 0.3057 (0.3300)  class_acc: 0.8393 (0.8550)  loss_scale: 4096.0000 (5556.2496)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.24 GB / 503.51 GB
Epoch: [16]  [570/893]  eta: 0:08:02  lr: 0.001846  min_lr: 0.000002  loss: 0.3311 (0.3306)  class_acc: 0.8393 (0.8545)  loss_scale: 4096.0000 (5530.6760)  weight_decay: 0.0500 (0.0500)  time: 1.4627  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.78 GB / 503.51 GB
Epoch: [16]  [580/893]  eta: 0:07:47  lr: 0.001845  min_lr: 0.000002  loss: 0.3479 (0.3304)  class_acc: 0.8393 (0.8546)  loss_scale: 4096.0000 (5505.9828)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
[2025-03-11 04:31:52,399] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 04:31:52,399] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.41 GB / 503.51 GB
Epoch: [16]  [590/893]  eta: 0:07:32  lr: 0.001845  min_lr: 0.000002  loss: 0.3010 (0.3298)  class_acc: 0.8750 (0.8550)  loss_scale: 4096.0000 (5551.4315)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.40 GB / 503.51 GB
Epoch: [16]  [600/893]  eta: 0:07:17  lr: 0.001844  min_lr: 0.000002  loss: 0.3008 (0.3299)  class_acc: 0.8750 (0.8551)  loss_scale: 8192.0000 (5595.3677)  weight_decay: 0.0500 (0.0500)  time: 1.4710  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.20 GB / 503.51 GB
Epoch: [16]  [610/893]  eta: 0:07:02  lr: 0.001843  min_lr: 0.000002  loss: 0.3254 (0.3296)  class_acc: 0.8571 (0.8552)  loss_scale: 8192.0000 (5637.8658)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0004  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.59 GB / 503.51 GB
Epoch: [16]  [620/893]  eta: 0:06:47  lr: 0.001843  min_lr: 0.000002  loss: 0.3228 (0.3292)  class_acc: 0.8571 (0.8553)  loss_scale: 8192.0000 (5678.9952)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
[2025-03-11 04:32:51,026] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 14893
[2025-03-11 04:32:51,026] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 04:32:51,026] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.75 GB / 503.51 GB
Epoch: [16]  [630/893]  eta: 0:06:32  lr: 0.001842  min_lr: 0.000002  loss: 0.3403 (0.3297)  class_acc: 0.8571 (0.8552)  loss_scale: 4096.0000 (5653.9081)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0002  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.00 GB / 503.51 GB
Epoch: [16]  [640/893]  eta: 0:06:17  lr: 0.001842  min_lr: 0.000002  loss: 0.3667 (0.3309)  class_acc: 0.8393 (0.8546)  loss_scale: 4096.0000 (5629.6037)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0002  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.78 GB / 503.51 GB
Epoch: [16]  [650/893]  eta: 0:06:02  lr: 0.001841  min_lr: 0.000002  loss: 0.3740 (0.3316)  class_acc: 0.8214 (0.8541)  loss_scale: 4096.0000 (5606.0461)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0002  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.55 GB / 503.51 GB
Epoch: [16]  [660/893]  eta: 0:05:47  lr: 0.001840  min_lr: 0.000002  loss: 0.3594 (0.3318)  class_acc: 0.8214 (0.8538)  loss_scale: 4096.0000 (5583.2012)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0002  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.03 GB / 503.51 GB
Epoch: [16]  [670/893]  eta: 0:05:32  lr: 0.001840  min_lr: 0.000002  loss: 0.3369 (0.3317)  class_acc: 0.8571 (0.8540)  loss_scale: 4096.0000 (5561.0373)  weight_decay: 0.0500 (0.0500)  time: 1.4752  data: 0.0002  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.88 GB / 503.51 GB
Epoch: [16]  [680/893]  eta: 0:05:17  lr: 0.001839  min_lr: 0.000002  loss: 0.3230 (0.3316)  class_acc: 0.8571 (0.8542)  loss_scale: 4096.0000 (5539.5242)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0002  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.44 GB / 503.51 GB
Epoch: [16]  [690/893]  eta: 0:05:02  lr: 0.001838  min_lr: 0.000002  loss: 0.2722 (0.3309)  class_acc: 0.8750 (0.8547)  loss_scale: 4096.0000 (5518.6339)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.73 GB / 503.51 GB
Epoch: [16]  [700/893]  eta: 0:04:47  lr: 0.001838  min_lr: 0.000002  loss: 0.2993 (0.3310)  class_acc: 0.8750 (0.8544)  loss_scale: 4096.0000 (5498.3395)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0004  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.42 GB / 503.51 GB
Epoch: [16]  [710/893]  eta: 0:04:32  lr: 0.001837  min_lr: 0.000002  loss: 0.3083 (0.3303)  class_acc: 0.8393 (0.8546)  loss_scale: 4096.0000 (5478.6160)  weight_decay: 0.0500 (0.0500)  time: 1.4729  data: 0.0003  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.15 GB / 503.51 GB
Epoch: [16]  [720/893]  eta: 0:04:17  lr: 0.001837  min_lr: 0.000002  loss: 0.2991 (0.3302)  class_acc: 0.8571 (0.8546)  loss_scale: 4096.0000 (5459.4397)  weight_decay: 0.0500 (0.0500)  time: 1.4752  data: 0.0003  max mem: 31081
[2025-03-11 04:35:26,709] [INFO] [logging.py:129:log_dist] [Rank 0] step=15000, skipped=88, lr=[2.3980505363215516e-06, 2.3980505363215516e-06, 3.996750893869253e-06, 3.996750893869253e-06, 6.661251489782089e-06, 6.661251489782089e-06, 1.1102085816303482e-05, 1.1102085816303482e-05, 1.8503476360505805e-05, 1.8503476360505805e-05, 3.0839127267509676e-05, 3.0839127267509676e-05, 5.139854544584946e-05, 5.139854544584946e-05, 8.56642424097491e-05, 8.56642424097491e-05, 0.00014277373734958185, 0.00014277373734958185, 0.00023795622891596978, 0.00023795622891596978, 0.0003965937148599496, 0.0003965937148599496, 0.0006609895247665827, 0.0006609895247665827, 0.0011016492079443045, 0.0011016492079443045, 0.0018360820132405076, 0.0018360820132405076], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 04:35:26,710] [INFO] [timer.py:264:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=61.01552417573729, CurrSamplesPerSec=61.54653127289846, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.35 GB / 503.51 GB
Epoch: [16]  [730/893]  eta: 0:04:02  lr: 0.001836  min_lr: 0.000002  loss: 0.3340 (0.3302)  class_acc: 0.8571 (0.8548)  loss_scale: 4096.0000 (5440.7880)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.67 GB / 503.51 GB
Epoch: [16]  [740/893]  eta: 0:03:47  lr: 0.001835  min_lr: 0.000002  loss: 0.3369 (0.3304)  class_acc: 0.8571 (0.8548)  loss_scale: 4096.0000 (5422.6397)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0003  max mem: 31081
[2025-03-11 04:36:00,410] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 04:36:00,410] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.72 GB / 503.51 GB
Epoch: [16]  [750/893]  eta: 0:03:32  lr: 0.001835  min_lr: 0.000002  loss: 0.3225 (0.3304)  class_acc: 0.8750 (0.8549)  loss_scale: 4096.0000 (5410.4288)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.50 GB / 503.51 GB
Epoch: [16]  [760/893]  eta: 0:03:17  lr: 0.001834  min_lr: 0.000002  loss: 0.3267 (0.3305)  class_acc: 0.8393 (0.8548)  loss_scale: 8192.0000 (5446.9803)  weight_decay: 0.0500 (0.0500)  time: 1.4728  data: 0.0003  max mem: 31081
[2025-03-11 04:36:29,842] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 15042
[2025-03-11 04:36:29,842] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 04:36:29,842] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.30 GB / 503.51 GB
Epoch: [16]  [770/893]  eta: 0:03:03  lr: 0.001833  min_lr: 0.000002  loss: 0.3113 (0.3305)  class_acc: 0.8393 (0.8549)  loss_scale: 8192.0000 (5477.2711)  weight_decay: 0.0500 (0.0500)  time: 1.4711  data: 0.0004  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.43 GB / 503.51 GB
Epoch: [16]  [780/893]  eta: 0:02:48  lr: 0.001833  min_lr: 0.000002  loss: 0.3174 (0.3307)  class_acc: 0.8571 (0.8548)  loss_scale: 4096.0000 (5459.5851)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.44 GB / 503.51 GB
Epoch: [16]  [790/893]  eta: 0:02:33  lr: 0.001832  min_lr: 0.000002  loss: 0.3174 (0.3305)  class_acc: 0.8571 (0.8549)  loss_scale: 4096.0000 (5442.3464)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0002  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.65 GB / 503.51 GB
Epoch: [16]  [800/893]  eta: 0:02:18  lr: 0.001831  min_lr: 0.000002  loss: 0.3215 (0.3306)  class_acc: 0.8393 (0.8548)  loss_scale: 4096.0000 (5425.5381)  weight_decay: 0.0500 (0.0500)  time: 1.4616  data: 0.0002  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.22 GB / 503.51 GB
Epoch: [16]  [810/893]  eta: 0:02:03  lr: 0.001831  min_lr: 0.000002  loss: 0.3289 (0.3306)  class_acc: 0.8393 (0.8548)  loss_scale: 4096.0000 (5409.1443)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0002  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.89 GB / 503.51 GB
Epoch: [16]  [820/893]  eta: 0:01:48  lr: 0.001830  min_lr: 0.000002  loss: 0.3267 (0.3310)  class_acc: 0.8571 (0.8547)  loss_scale: 4096.0000 (5393.1498)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0002  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.00 GB / 503.51 GB
Epoch: [16]  [830/893]  eta: 0:01:33  lr: 0.001830  min_lr: 0.000002  loss: 0.3250 (0.3306)  class_acc: 0.8571 (0.8548)  loss_scale: 4096.0000 (5377.5403)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0002  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.22 GB / 503.51 GB
Epoch: [16]  [840/893]  eta: 0:01:18  lr: 0.001829  min_lr: 0.000002  loss: 0.3020 (0.3306)  class_acc: 0.8571 (0.8547)  loss_scale: 4096.0000 (5362.3020)  weight_decay: 0.0500 (0.0500)  time: 1.4701  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.74 GB / 503.51 GB
Epoch: [16]  [850/893]  eta: 0:01:03  lr: 0.001828  min_lr: 0.000002  loss: 0.3127 (0.3309)  class_acc: 0.8393 (0.8546)  loss_scale: 4096.0000 (5347.4219)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.99 GB / 503.51 GB
Epoch: [16]  [860/893]  eta: 0:00:49  lr: 0.001828  min_lr: 0.000002  loss: 0.3474 (0.3312)  class_acc: 0.8214 (0.8546)  loss_scale: 4096.0000 (5332.8873)  weight_decay: 0.0500 (0.0500)  time: 1.4560  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.11 GB / 503.51 GB
Epoch: [16]  [870/893]  eta: 0:00:34  lr: 0.001827  min_lr: 0.000002  loss: 0.3118 (0.3309)  class_acc: 0.8571 (0.8549)  loss_scale: 4096.0000 (5318.6866)  weight_decay: 0.0500 (0.0500)  time: 1.4478  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.67 GB / 503.51 GB
Epoch: [16]  [880/893]  eta: 0:00:19  lr: 0.001826  min_lr: 0.000002  loss: 0.3188 (0.3309)  class_acc: 0.8393 (0.8547)  loss_scale: 4096.0000 (5304.8082)  weight_decay: 0.0500 (0.0500)  time: 1.4450  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.40 GB / 503.51 GB
Epoch: [16]  [890/893]  eta: 0:00:04  lr: 0.001826  min_lr: 0.000002  loss: 0.3389 (0.3307)  class_acc: 0.8393 (0.8549)  loss_scale: 4096.0000 (5291.2413)  weight_decay: 0.0500 (0.0500)  time: 1.4390  data: 0.0001  max mem: 31081
Epoch: [16]  [892/893]  eta: 0:00:01  lr: 0.001826  min_lr: 0.000002  loss: 0.3389 (0.3306)  class_acc: 0.8393 (0.8550)  loss_scale: 4096.0000 (5289.9013)  weight_decay: 0.0500 (0.0500)  time: 1.3888  data: 0.0001  max mem: 31081
Epoch: [16] Total time: 0:22:04 (1.4829 s / it)
Averaged stats: lr: 0.001826  min_lr: 0.000002  loss: 0.3389 (0.3306)  class_acc: 0.8393 (0.8550)  loss_scale: 4096.0000 (5289.9013)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:15:30  loss: 0.6303 (0.6303)  acc: 73.8095 (73.8095)  time: 11.1681  data: 10.6356  max mem: 31081
Val:  [ 10/728]  eta: 0:18:25  loss: 0.4909 (0.5106)  acc: 76.1905 (78.8961)  time: 1.5390  data: 1.0154  max mem: 31081
Val:  [ 20/728]  eta: 0:14:10  loss: 0.4079 (0.4927)  acc: 79.7619 (79.3084)  time: 0.7032  data: 0.1804  max mem: 31081
Val:  [ 30/728]  eta: 0:12:42  loss: 0.4231 (0.5189)  acc: 77.3810 (77.6882)  time: 0.8469  data: 0.3243  max mem: 31081
Val:  [ 40/728]  eta: 0:11:53  loss: 0.4935 (0.5344)  acc: 72.6190 (76.6841)  time: 0.8648  data: 0.3426  max mem: 31081
Val:  [ 50/728]  eta: 0:11:18  loss: 0.4609 (0.5089)  acc: 79.7619 (78.0579)  time: 0.8570  data: 0.3328  max mem: 31081
Val:  [ 60/728]  eta: 0:10:22  loss: 0.4609 (0.5064)  acc: 80.9524 (78.4934)  time: 0.7156  data: 0.1918  max mem: 31081
Val:  [ 70/728]  eta: 0:09:51  loss: 0.4458 (0.5122)  acc: 78.5714 (78.2193)  time: 0.6402  data: 0.1186  max mem: 31081
Val:  [ 80/728]  eta: 0:09:39  loss: 0.5243 (0.5253)  acc: 77.3810 (77.7778)  time: 0.7788  data: 0.2587  max mem: 31081
Val:  [ 90/728]  eta: 0:09:24  loss: 0.5243 (0.5321)  acc: 73.8095 (77.3810)  time: 0.8372  data: 0.3177  max mem: 31081
Val:  [100/728]  eta: 0:09:12  loss: 0.4567 (0.5242)  acc: 79.7619 (77.6756)  time: 0.8258  data: 0.3038  max mem: 31081
Val:  [110/728]  eta: 0:09:03  loss: 0.4420 (0.5229)  acc: 80.9524 (77.7671)  time: 0.8558  data: 0.3331  max mem: 31081
Val:  [120/728]  eta: 0:08:47  loss: 0.4395 (0.5122)  acc: 82.1429 (78.3255)  time: 0.8050  data: 0.2831  max mem: 31081
Val:  [130/728]  eta: 0:08:27  loss: 0.3584 (0.5141)  acc: 83.3333 (78.3533)  time: 0.6775  data: 0.1556  max mem: 31081
Val:  [140/728]  eta: 0:08:18  loss: 0.4210 (0.5142)  acc: 78.5714 (78.1915)  time: 0.7202  data: 0.1996  max mem: 31081
Val:  [150/728]  eta: 0:08:02  loss: 0.4355 (0.5196)  acc: 78.5714 (78.1615)  time: 0.7467  data: 0.2259  max mem: 31081
Val:  [160/728]  eta: 0:07:55  loss: 0.4766 (0.5246)  acc: 76.1905 (77.9059)  time: 0.7706  data: 0.2463  max mem: 31081
Val:  [170/728]  eta: 0:07:46  loss: 0.4621 (0.5237)  acc: 78.5714 (78.0214)  time: 0.8342  data: 0.3094  max mem: 31081
Val:  [180/728]  eta: 0:07:34  loss: 0.4389 (0.5199)  acc: 80.9524 (78.2097)  time: 0.7690  data: 0.2450  max mem: 31081
Val:  [190/728]  eta: 0:07:21  loss: 0.4443 (0.5177)  acc: 79.7619 (78.1975)  time: 0.6873  data: 0.1645  max mem: 31081
Val:  [200/728]  eta: 0:07:13  loss: 0.4177 (0.5152)  acc: 78.5714 (78.2694)  time: 0.7399  data: 0.2193  max mem: 31081
Val:  [210/728]  eta: 0:07:08  loss: 0.4567 (0.5182)  acc: 77.3810 (78.1708)  time: 0.8977  data: 0.3754  max mem: 31081
Val:  [220/728]  eta: 0:07:01  loss: 0.4376 (0.5142)  acc: 78.5714 (78.1782)  time: 0.9201  data: 0.3964  max mem: 31081
Val:  [230/728]  eta: 0:06:52  loss: 0.4376 (0.5205)  acc: 78.5714 (77.9221)  time: 0.8392  data: 0.3188  max mem: 31081
Val:  [240/728]  eta: 0:06:40  loss: 0.5185 (0.5213)  acc: 79.7619 (77.9737)  time: 0.7120  data: 0.1847  max mem: 31081
Val:  [250/728]  eta: 0:06:29  loss: 0.4932 (0.5224)  acc: 78.5714 (77.9738)  time: 0.6628  data: 0.1340  max mem: 31081
Val:  [260/728]  eta: 0:06:22  loss: 0.4166 (0.5209)  acc: 76.1905 (78.0058)  time: 0.7797  data: 0.2560  max mem: 31081
Val:  [270/728]  eta: 0:06:14  loss: 0.4629 (0.5228)  acc: 79.7619 (77.9301)  time: 0.8508  data: 0.3273  max mem: 31081
Val:  [280/728]  eta: 0:06:09  loss: 0.5046 (0.5247)  acc: 77.3810 (77.9317)  time: 0.9262  data: 0.4029  max mem: 31081
Val:  [290/728]  eta: 0:06:01  loss: 0.4569 (0.5212)  acc: 79.7619 (78.0682)  time: 0.9209  data: 0.3986  max mem: 31081
Val:  [300/728]  eta: 0:05:48  loss: 0.4032 (0.5201)  acc: 80.9524 (78.1285)  time: 0.6742  data: 0.1522  max mem: 31081
Val:  [310/728]  eta: 0:05:40  loss: 0.5024 (0.5229)  acc: 77.3810 (77.9934)  time: 0.6732  data: 0.1487  max mem: 31081
Val:  [320/728]  eta: 0:05:32  loss: 0.4430 (0.5200)  acc: 79.7619 (78.1116)  time: 0.7996  data: 0.2760  max mem: 31081
Val:  [330/728]  eta: 0:05:24  loss: 0.3839 (0.5158)  acc: 82.1429 (78.2298)  time: 0.8358  data: 0.3119  max mem: 31081
Val:  [340/728]  eta: 0:05:17  loss: 0.3871 (0.5130)  acc: 80.9524 (78.2921)  time: 0.8973  data: 0.3749  max mem: 31081
Val:  [350/728]  eta: 0:05:09  loss: 0.3809 (0.5132)  acc: 82.1429 (78.3951)  time: 0.8516  data: 0.3290  max mem: 31081
Val:  [360/728]  eta: 0:04:58  loss: 0.4008 (0.5114)  acc: 80.9524 (78.4428)  time: 0.6621  data: 0.1387  max mem: 31081
Val:  [370/728]  eta: 0:04:50  loss: 0.5103 (0.5137)  acc: 78.5714 (78.3340)  time: 0.7041  data: 0.1809  max mem: 31081
Val:  [380/728]  eta: 0:04:42  loss: 0.5379 (0.5164)  acc: 77.3810 (78.2621)  time: 0.8615  data: 0.3385  max mem: 31081
Val:  [390/728]  eta: 0:04:35  loss: 0.3817 (0.5115)  acc: 80.9524 (78.4101)  time: 0.8420  data: 0.3175  max mem: 31081
Val:  [400/728]  eta: 0:04:27  loss: 0.3433 (0.5111)  acc: 83.3333 (78.4794)  time: 0.8684  data: 0.3407  max mem: 31081
Val:  [410/728]  eta: 0:04:19  loss: 0.4053 (0.5101)  acc: 79.7619 (78.5309)  time: 0.8720  data: 0.3474  max mem: 31081
Val:  [420/728]  eta: 0:04:09  loss: 0.4065 (0.5094)  acc: 83.3333 (78.5743)  time: 0.6882  data: 0.1665  max mem: 31081
Val:  [430/728]  eta: 0:04:01  loss: 0.4995 (0.5095)  acc: 79.7619 (78.4775)  time: 0.6967  data: 0.1753  max mem: 31081
Val:  [440/728]  eta: 0:03:53  loss: 0.5141 (0.5096)  acc: 77.3810 (78.4284)  time: 0.8386  data: 0.3153  max mem: 31081
Val:  [450/728]  eta: 0:03:45  loss: 0.5722 (0.5122)  acc: 77.3810 (78.3497)  time: 0.8355  data: 0.3113  max mem: 31081
Val:  [460/728]  eta: 0:03:37  loss: 0.5661 (0.5143)  acc: 76.1905 (78.3261)  time: 0.8438  data: 0.3229  max mem: 31081
Val:  [470/728]  eta: 0:03:29  loss: 0.4019 (0.5122)  acc: 77.3810 (78.3490)  time: 0.8006  data: 0.2811  max mem: 31081
Val:  [480/728]  eta: 0:03:19  loss: 0.4164 (0.5105)  acc: 77.3810 (78.4056)  time: 0.6511  data: 0.1296  max mem: 31081
Val:  [490/728]  eta: 0:03:11  loss: 0.4791 (0.5099)  acc: 77.3810 (78.4066)  time: 0.6885  data: 0.1675  max mem: 31081
Val:  [500/728]  eta: 0:03:04  loss: 0.4699 (0.5100)  acc: 79.7619 (78.3813)  time: 0.8515  data: 0.3291  max mem: 31081
Val:  [510/728]  eta: 0:02:56  loss: 0.3475 (0.5080)  acc: 84.5238 (78.4643)  time: 0.8918  data: 0.3681  max mem: 31081
Val:  [520/728]  eta: 0:02:48  loss: 0.3871 (0.5099)  acc: 84.5238 (78.3955)  time: 0.8732  data: 0.3514  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.5241 (0.5093)  acc: 75.0000 (78.4212)  time: 0.7504  data: 0.2284  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.5036 (0.5126)  acc: 75.0000 (78.2480)  time: 0.6065  data: 0.0835  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.6106 (0.5150)  acc: 72.6190 (78.0767)  time: 0.6696  data: 0.1467  max mem: 31081
Val:  [560/728]  eta: 0:02:14  loss: 0.5533 (0.5137)  acc: 76.1905 (78.1364)  time: 0.7568  data: 0.2376  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.5016 (0.5182)  acc: 78.5714 (78.0252)  time: 0.8036  data: 0.2830  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4844 (0.5174)  acc: 79.7619 (78.0899)  time: 0.8415  data: 0.3164  max mem: 31081
Val:  [590/728]  eta: 0:01:50  loss: 0.4844 (0.5194)  acc: 80.9524 (78.0074)  time: 0.7344  data: 0.2090  max mem: 31081
Val:  [600/728]  eta: 0:01:42  loss: 0.4953 (0.5193)  acc: 79.7619 (78.0247)  time: 0.6880  data: 0.1636  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4953 (0.5209)  acc: 79.7619 (77.9518)  time: 0.6552  data: 0.1333  max mem: 31081
Val:  [620/728]  eta: 0:01:26  loss: 0.4840 (0.5238)  acc: 77.3810 (77.9541)  time: 0.7457  data: 0.2242  max mem: 31081
Val:  [630/728]  eta: 0:01:18  loss: 0.4815 (0.5230)  acc: 77.3810 (77.9790)  time: 0.8633  data: 0.3433  max mem: 31081
Val:  [640/728]  eta: 0:01:10  loss: 0.4416 (0.5216)  acc: 78.5714 (78.0273)  time: 0.8054  data: 0.2865  max mem: 31081
Val:  [650/728]  eta: 0:01:02  loss: 0.4338 (0.5208)  acc: 82.1429 (78.0813)  time: 0.7759  data: 0.2526  max mem: 31081
Val:  [660/728]  eta: 0:00:54  loss: 0.4229 (0.5203)  acc: 82.1429 (78.0906)  time: 0.8033  data: 0.2786  max mem: 31081
Val:  [670/728]  eta: 0:00:46  loss: 0.4202 (0.5206)  acc: 79.7619 (78.0747)  time: 0.6841  data: 0.1622  max mem: 31081
Val:  [680/728]  eta: 0:00:38  loss: 0.3560 (0.5191)  acc: 82.1429 (78.1396)  time: 0.6597  data: 0.1410  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.4272 (0.5182)  acc: 79.7619 (78.1580)  time: 0.8563  data: 0.3356  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4272 (0.5171)  acc: 78.5714 (78.1808)  time: 0.8640  data: 0.3401  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.3952 (0.5163)  acc: 79.7619 (78.1897)  time: 0.8339  data: 0.3115  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4330 (0.5156)  acc: 79.7619 (78.2148)  time: 0.8044  data: 0.2892  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4446 (0.5173)  acc: 78.5714 (78.1704)  time: 0.7790  data: 0.2892  max mem: 31081
Val: Total time: 0:09:36 (0.7924 s / it)
* Acc@1 78.170 AP 0.802344560623169 loss 0.517
Accuracy of the network on the 61096 val videos: 78.2%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.88 GB / 503.51 GB
Epoch: [17]  [  0/893]  eta: 3:27:33  lr: 0.001826  min_lr: 0.000002  loss: 0.3269 (0.3269)  class_acc: 0.8214 (0.8214)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 13.9452  data: 12.6401  max mem: 31081
[2025-03-11 04:49:29,487] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 04:49:29,487] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.46 GB / 503.51 GB
Epoch: [17]  [ 10/893]  eta: 0:38:47  lr: 0.001825  min_lr: 0.000002  loss: 0.3269 (0.3393)  class_acc: 0.8571 (0.8506)  loss_scale: 4096.0000 (5585.4545)  weight_decay: 0.0500 (0.0500)  time: 2.6358  data: 1.1494  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.49 GB / 503.51 GB
Epoch: [17]  [ 20/893]  eta: 0:30:32  lr: 0.001824  min_lr: 0.000002  loss: 0.3130 (0.3200)  class_acc: 0.8571 (0.8588)  loss_scale: 8192.0000 (6826.6667)  weight_decay: 0.0500 (0.0500)  time: 1.5063  data: 0.0005  max mem: 31081
[2025-03-11 04:49:52,116] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 15186
[2025-03-11 04:49:52,116] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 04:49:52,117] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.07 GB / 503.51 GB
Epoch: [17]  [ 30/893]  eta: 0:27:33  lr: 0.001824  min_lr: 0.000002  loss: 0.3301 (0.3357)  class_acc: 0.8393 (0.8468)  loss_scale: 8192.0000 (6077.9355)  weight_decay: 0.0500 (0.0500)  time: 1.5205  data: 0.0006  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.93 GB / 503.51 GB
Epoch: [17]  [ 40/893]  eta: 0:25:51  lr: 0.001823  min_lr: 0.000002  loss: 0.3345 (0.3297)  class_acc: 0.8393 (0.8510)  loss_scale: 4096.0000 (5594.5366)  weight_decay: 0.0500 (0.0500)  time: 1.5245  data: 0.0011  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.68 GB / 503.51 GB
Epoch: [17]  [ 50/893]  eta: 0:24:38  lr: 0.001822  min_lr: 0.000002  loss: 0.3154 (0.3287)  class_acc: 0.8393 (0.8515)  loss_scale: 4096.0000 (5300.7059)  weight_decay: 0.0500 (0.0500)  time: 1.5025  data: 0.0013  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.87 GB / 503.51 GB
Epoch: [17]  [ 60/893]  eta: 0:23:42  lr: 0.001822  min_lr: 0.000002  loss: 0.3220 (0.3284)  class_acc: 0.8393 (0.8522)  loss_scale: 4096.0000 (5103.2131)  weight_decay: 0.0500 (0.0500)  time: 1.4815  data: 0.0004  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.08 GB / 503.51 GB
Epoch: [17]  [ 70/893]  eta: 0:22:57  lr: 0.001821  min_lr: 0.000002  loss: 0.3767 (0.3343)  class_acc: 0.8393 (0.8488)  loss_scale: 4096.0000 (4961.3521)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0002  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.16 GB / 503.51 GB
Epoch: [17]  [ 80/893]  eta: 0:22:18  lr: 0.001821  min_lr: 0.000002  loss: 0.3210 (0.3306)  class_acc: 0.8393 (0.8510)  loss_scale: 4096.0000 (4854.5185)  weight_decay: 0.0500 (0.0500)  time: 1.4602  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.87 GB / 503.51 GB
Epoch: [17]  [ 90/893]  eta: 0:21:47  lr: 0.001820  min_lr: 0.000002  loss: 0.2913 (0.3276)  class_acc: 0.8750 (0.8534)  loss_scale: 4096.0000 (4771.1648)  weight_decay: 0.0500 (0.0500)  time: 1.4663  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.82 GB / 503.51 GB
Epoch: [17]  [100/893]  eta: 0:21:17  lr: 0.001819  min_lr: 0.000002  loss: 0.3069 (0.3323)  class_acc: 0.8571 (0.8524)  loss_scale: 4096.0000 (4704.3168)  weight_decay: 0.0500 (0.0500)  time: 1.4669  data: 0.0004  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.34 GB / 503.51 GB
Epoch: [17]  [110/893]  eta: 0:20:51  lr: 0.001819  min_lr: 0.000002  loss: 0.3752 (0.3347)  class_acc: 0.8393 (0.8518)  loss_scale: 4096.0000 (4649.5135)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0004  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.46 GB / 503.51 GB
Epoch: [17]  [120/893]  eta: 0:20:28  lr: 0.001818  min_lr: 0.000002  loss: 0.3325 (0.3312)  class_acc: 0.8571 (0.8540)  loss_scale: 4096.0000 (4603.7686)  weight_decay: 0.0500 (0.0500)  time: 1.4749  data: 0.0004  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.10 GB / 503.51 GB
Epoch: [17]  [130/893]  eta: 0:20:05  lr: 0.001817  min_lr: 0.000002  loss: 0.2900 (0.3280)  class_acc: 0.8571 (0.8548)  loss_scale: 4096.0000 (4565.0076)  weight_decay: 0.0500 (0.0500)  time: 1.4766  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.61 GB / 503.51 GB
Epoch: [17]  [140/893]  eta: 0:19:43  lr: 0.001817  min_lr: 0.000002  loss: 0.3003 (0.3284)  class_acc: 0.8571 (0.8544)  loss_scale: 4096.0000 (4531.7447)  weight_decay: 0.0500 (0.0500)  time: 1.4737  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.27 GB / 503.51 GB
Epoch: [17]  [150/893]  eta: 0:19:23  lr: 0.001816  min_lr: 0.000002  loss: 0.3096 (0.3292)  class_acc: 0.8571 (0.8545)  loss_scale: 4096.0000 (4502.8874)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0002  max mem: 31081
[2025-03-11 04:53:02,896] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 04:53:02,896] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.91 GB / 503.51 GB
Epoch: [17]  [160/893]  eta: 0:19:03  lr: 0.001815  min_lr: 0.000002  loss: 0.3125 (0.3288)  class_acc: 0.8393 (0.8554)  loss_scale: 4096.0000 (4732.0248)  weight_decay: 0.0500 (0.0500)  time: 1.4746  data: 0.0002  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.97 GB / 503.51 GB
Epoch: [17]  [170/893]  eta: 0:18:44  lr: 0.001815  min_lr: 0.000002  loss: 0.3125 (0.3279)  class_acc: 0.8571 (0.8556)  loss_scale: 8192.0000 (4934.3626)  weight_decay: 0.0500 (0.0500)  time: 1.4764  data: 0.0002  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.52 GB / 503.51 GB
Epoch: [17]  [180/893]  eta: 0:18:25  lr: 0.001814  min_lr: 0.000002  loss: 0.2922 (0.3287)  class_acc: 0.8571 (0.8551)  loss_scale: 8192.0000 (5114.3425)  weight_decay: 0.0500 (0.0500)  time: 1.4745  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.51 GB / 503.51 GB
Epoch: [17]  [190/893]  eta: 0:18:06  lr: 0.001813  min_lr: 0.000002  loss: 0.3198 (0.3305)  class_acc: 0.8571 (0.8542)  loss_scale: 8192.0000 (5275.4764)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0003  max mem: 31081
[2025-03-11 04:54:03,218] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 15356
[2025-03-11 04:54:03,219] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 04:54:03,219] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.98 GB / 503.51 GB
Epoch: [17]  [200/893]  eta: 0:17:48  lr: 0.001813  min_lr: 0.000002  loss: 0.3606 (0.3324)  class_acc: 0.8571 (0.8538)  loss_scale: 8192.0000 (5237.1741)  weight_decay: 0.0500 (0.0500)  time: 1.4583  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.56 GB / 503.51 GB
Epoch: [17]  [210/893]  eta: 0:17:30  lr: 0.001812  min_lr: 0.000002  loss: 0.3296 (0.3311)  class_acc: 0.8571 (0.8541)  loss_scale: 4096.0000 (5183.0900)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.96 GB / 503.51 GB
Epoch: [17]  [220/893]  eta: 0:17:12  lr: 0.001812  min_lr: 0.000002  loss: 0.3079 (0.3297)  class_acc: 0.8750 (0.8550)  loss_scale: 4096.0000 (5133.9005)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.80 GB / 503.51 GB
Epoch: [17]  [230/893]  eta: 0:16:55  lr: 0.001811  min_lr: 0.000002  loss: 0.3179 (0.3302)  class_acc: 0.8571 (0.8541)  loss_scale: 4096.0000 (5088.9697)  weight_decay: 0.0500 (0.0500)  time: 1.4729  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.03 GB / 503.51 GB
Epoch: [17]  [240/893]  eta: 0:16:38  lr: 0.001810  min_lr: 0.000002  loss: 0.3152 (0.3288)  class_acc: 0.8571 (0.8548)  loss_scale: 4096.0000 (5047.7676)  weight_decay: 0.0500 (0.0500)  time: 1.4695  data: 0.0002  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.99 GB / 503.51 GB
Epoch: [17]  [250/893]  eta: 0:16:21  lr: 0.001810  min_lr: 0.000002  loss: 0.3232 (0.3298)  class_acc: 0.8393 (0.8542)  loss_scale: 4096.0000 (5009.8486)  weight_decay: 0.0500 (0.0500)  time: 1.4599  data: 0.0002  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.15 GB / 503.51 GB
Epoch: [17]  [260/893]  eta: 0:16:05  lr: 0.001809  min_lr: 0.000002  loss: 0.3369 (0.3299)  class_acc: 0.8393 (0.8543)  loss_scale: 4096.0000 (4974.8352)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.17 GB / 503.51 GB
Epoch: [17]  [270/893]  eta: 0:15:48  lr: 0.001808  min_lr: 0.000002  loss: 0.3296 (0.3297)  class_acc: 0.8393 (0.8538)  loss_scale: 4096.0000 (4942.4059)  weight_decay: 0.0500 (0.0500)  time: 1.4771  data: 0.0004  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.50 GB / 503.51 GB
Epoch: [17]  [280/893]  eta: 0:15:31  lr: 0.001808  min_lr: 0.000002  loss: 0.3523 (0.3309)  class_acc: 0.8214 (0.8535)  loss_scale: 4096.0000 (4912.2847)  weight_decay: 0.0500 (0.0500)  time: 1.4611  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.30 GB / 503.51 GB
Epoch: [17]  [290/893]  eta: 0:15:15  lr: 0.001807  min_lr: 0.000002  loss: 0.3699 (0.3324)  class_acc: 0.8214 (0.8528)  loss_scale: 4096.0000 (4884.2337)  weight_decay: 0.0500 (0.0500)  time: 1.4569  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.24 GB / 503.51 GB
Epoch: [17]  [300/893]  eta: 0:14:59  lr: 0.001806  min_lr: 0.000002  loss: 0.3193 (0.3314)  class_acc: 0.8393 (0.8532)  loss_scale: 4096.0000 (4858.0465)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.59 GB / 503.51 GB
Epoch: [17]  [310/893]  eta: 0:14:43  lr: 0.001806  min_lr: 0.000002  loss: 0.3149 (0.3317)  class_acc: 0.8571 (0.8532)  loss_scale: 4096.0000 (4833.5434)  weight_decay: 0.0500 (0.0500)  time: 1.4737  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.57 GB / 503.51 GB
Epoch: [17]  [320/893]  eta: 0:14:27  lr: 0.001805  min_lr: 0.000002  loss: 0.3669 (0.3331)  class_acc: 0.8393 (0.8525)  loss_scale: 4096.0000 (4810.5670)  weight_decay: 0.0500 (0.0500)  time: 1.4784  data: 0.0003  max mem: 31081
[2025-03-11 04:57:12,515] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 04:57:12,515] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.05 GB / 503.51 GB
Epoch: [17]  [330/893]  eta: 0:14:11  lr: 0.001804  min_lr: 0.000002  loss: 0.3516 (0.3340)  class_acc: 0.8393 (0.8519)  loss_scale: 4096.0000 (4912.7251)  weight_decay: 0.0500 (0.0500)  time: 1.4714  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.56 GB / 503.51 GB
Epoch: [17]  [340/893]  eta: 0:13:55  lr: 0.001804  min_lr: 0.000002  loss: 0.3281 (0.3333)  class_acc: 0.8393 (0.8521)  loss_scale: 8192.0000 (5008.8915)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0002  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.22 GB / 503.51 GB
Epoch: [17]  [350/893]  eta: 0:13:39  lr: 0.001803  min_lr: 0.000002  loss: 0.3208 (0.3340)  class_acc: 0.8393 (0.8520)  loss_scale: 8192.0000 (5099.5783)  weight_decay: 0.0500 (0.0500)  time: 1.4588  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.97 GB / 503.51 GB
Epoch: [17]  [360/893]  eta: 0:13:24  lr: 0.001802  min_lr: 0.000002  loss: 0.3201 (0.3333)  class_acc: 0.8393 (0.8524)  loss_scale: 8192.0000 (5185.2410)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.57 GB / 503.51 GB
Epoch: [17]  [370/893]  eta: 0:13:08  lr: 0.001802  min_lr: 0.000002  loss: 0.3333 (0.3337)  class_acc: 0.8571 (0.8525)  loss_scale: 8192.0000 (5266.2857)  weight_decay: 0.0500 (0.0500)  time: 1.4681  data: 0.0004  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.40 GB / 503.51 GB
Epoch: [17]  [380/893]  eta: 0:12:52  lr: 0.001801  min_lr: 0.000002  loss: 0.3633 (0.3350)  class_acc: 0.8393 (0.8518)  loss_scale: 8192.0000 (5343.0761)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
[2025-03-11 04:58:52,117] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 15553
[2025-03-11 04:58:52,117] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 04:58:52,117] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.66 GB / 503.51 GB
Epoch: [17]  [390/893]  eta: 0:12:37  lr: 0.001801  min_lr: 0.000002  loss: 0.3777 (0.3365)  class_acc: 0.8214 (0.8508)  loss_scale: 8192.0000 (5394.9872)  weight_decay: 0.0500 (0.0500)  time: 1.4701  data: 0.0002  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.59 GB / 503.51 GB
Epoch: [17]  [400/893]  eta: 0:12:21  lr: 0.001800  min_lr: 0.000002  loss: 0.3508 (0.3360)  class_acc: 0.8393 (0.8512)  loss_scale: 4096.0000 (5362.5935)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0002  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.06 GB / 503.51 GB
Epoch: [17]  [410/893]  eta: 0:12:06  lr: 0.001799  min_lr: 0.000002  loss: 0.3247 (0.3357)  class_acc: 0.8571 (0.8514)  loss_scale: 4096.0000 (5331.7762)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 99.51 GB / 503.51 GB
Epoch: [17]  [420/893]  eta: 0:11:50  lr: 0.001799  min_lr: 0.000002  loss: 0.3271 (0.3352)  class_acc: 0.8571 (0.8517)  loss_scale: 4096.0000 (5302.4228)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.23 GB / 503.51 GB
Epoch: [17]  [430/893]  eta: 0:11:35  lr: 0.001798  min_lr: 0.000002  loss: 0.3347 (0.3354)  class_acc: 0.8393 (0.8515)  loss_scale: 4096.0000 (5274.4316)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0004  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.03 GB / 503.51 GB
Epoch: [17]  [440/893]  eta: 0:11:19  lr: 0.001797  min_lr: 0.000002  loss: 0.3027 (0.3346)  class_acc: 0.8571 (0.8518)  loss_scale: 4096.0000 (5247.7098)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0004  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.12 GB / 503.51 GB
Epoch: [17]  [450/893]  eta: 0:11:04  lr: 0.001797  min_lr: 0.000002  loss: 0.3167 (0.3346)  class_acc: 0.8571 (0.8520)  loss_scale: 4096.0000 (5222.1729)  weight_decay: 0.0500 (0.0500)  time: 1.4744  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.13 GB / 503.51 GB
Epoch: [17]  [460/893]  eta: 0:10:49  lr: 0.001796  min_lr: 0.000002  loss: 0.3167 (0.3341)  class_acc: 0.8571 (0.8523)  loss_scale: 4096.0000 (5197.7440)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.76 GB / 503.51 GB
Epoch: [17]  [470/893]  eta: 0:10:34  lr: 0.001795  min_lr: 0.000002  loss: 0.2961 (0.3341)  class_acc: 0.8750 (0.8524)  loss_scale: 4096.0000 (5174.3524)  weight_decay: 0.0500 (0.0500)  time: 1.4628  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.27 GB / 503.51 GB
Epoch: [17]  [480/893]  eta: 0:10:18  lr: 0.001795  min_lr: 0.000002  loss: 0.2744 (0.3333)  class_acc: 0.8571 (0.8529)  loss_scale: 4096.0000 (5151.9335)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0002  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.99 GB / 503.51 GB
Epoch: [17]  [490/893]  eta: 0:10:03  lr: 0.001794  min_lr: 0.000002  loss: 0.2947 (0.3328)  class_acc: 0.8571 (0.8531)  loss_scale: 4096.0000 (5130.4277)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.78 GB / 503.51 GB
Epoch: [17]  [500/893]  eta: 0:09:48  lr: 0.001793  min_lr: 0.000002  loss: 0.2947 (0.3323)  class_acc: 0.8750 (0.8536)  loss_scale: 4096.0000 (5109.7804)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.07 GB / 503.51 GB
Epoch: [17]  [510/893]  eta: 0:09:33  lr: 0.001793  min_lr: 0.000002  loss: 0.3201 (0.3328)  class_acc: 0.8571 (0.8534)  loss_scale: 4096.0000 (5089.9413)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0003  max mem: 31081
[2025-03-11 05:02:01,446] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 05:02:01,446] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.27 GB / 503.51 GB
Epoch: [17]  [520/893]  eta: 0:09:17  lr: 0.001792  min_lr: 0.000002  loss: 0.3584 (0.3328)  class_acc: 0.8393 (0.8533)  loss_scale: 4096.0000 (5094.4491)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.96 GB / 503.51 GB
Epoch: [17]  [530/893]  eta: 0:09:02  lr: 0.001791  min_lr: 0.000002  loss: 0.3325 (0.3330)  class_acc: 0.8393 (0.8532)  loss_scale: 8192.0000 (5152.7834)  weight_decay: 0.0500 (0.0500)  time: 1.4594  data: 0.0004  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.73 GB / 503.51 GB
Epoch: [17]  [540/893]  eta: 0:08:47  lr: 0.001791  min_lr: 0.000002  loss: 0.3257 (0.3329)  class_acc: 0.8571 (0.8532)  loss_scale: 8192.0000 (5208.9612)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.71 GB / 503.51 GB
Epoch: [17]  [550/893]  eta: 0:08:32  lr: 0.001790  min_lr: 0.000002  loss: 0.3071 (0.3324)  class_acc: 0.8571 (0.8537)  loss_scale: 8192.0000 (5263.0998)  weight_decay: 0.0500 (0.0500)  time: 1.4762  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.47 GB / 503.51 GB
Epoch: [17]  [560/893]  eta: 0:08:17  lr: 0.001789  min_lr: 0.000002  loss: 0.2795 (0.3317)  class_acc: 0.8750 (0.8540)  loss_scale: 8192.0000 (5315.3084)  weight_decay: 0.0500 (0.0500)  time: 1.4704  data: 0.0002  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.70 GB / 503.51 GB
Epoch: [17]  [570/893]  eta: 0:08:02  lr: 0.001789  min_lr: 0.000002  loss: 0.2861 (0.3313)  class_acc: 0.8571 (0.8541)  loss_scale: 8192.0000 (5365.6883)  weight_decay: 0.0500 (0.0500)  time: 1.4724  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.47 GB / 503.51 GB
Epoch: [17]  [580/893]  eta: 0:07:47  lr: 0.001788  min_lr: 0.000002  loss: 0.3560 (0.3321)  class_acc: 0.8393 (0.8536)  loss_scale: 8192.0000 (5414.3339)  weight_decay: 0.0500 (0.0500)  time: 1.4734  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.41 GB / 503.51 GB
Epoch: [17]  [590/893]  eta: 0:07:32  lr: 0.001787  min_lr: 0.000002  loss: 0.3511 (0.3321)  class_acc: 0.8393 (0.8537)  loss_scale: 8192.0000 (5461.3333)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0002  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.22 GB / 503.51 GB
Epoch: [17]  [600/893]  eta: 0:07:17  lr: 0.001787  min_lr: 0.000002  loss: 0.3186 (0.3316)  class_acc: 0.8571 (0.8540)  loss_scale: 8192.0000 (5506.7687)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.19 GB / 503.51 GB
Epoch: [17]  [610/893]  eta: 0:07:02  lr: 0.001786  min_lr: 0.000002  loss: 0.3330 (0.3322)  class_acc: 0.8571 (0.8539)  loss_scale: 8192.0000 (5550.7169)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.0003  max mem: 31081
[2025-03-11 05:04:21,049] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 15777
[2025-03-11 05:04:21,049] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 05:04:21,049] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.92 GB / 503.51 GB
Epoch: [17]  [620/893]  eta: 0:06:47  lr: 0.001785  min_lr: 0.000002  loss: 0.3484 (0.3319)  class_acc: 0.8393 (0.8540)  loss_scale: 8192.0000 (5540.4831)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0004  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.39 GB / 503.51 GB
Epoch: [17]  [630/893]  eta: 0:06:32  lr: 0.001785  min_lr: 0.000002  loss: 0.3796 (0.3337)  class_acc: 0.8214 (0.8530)  loss_scale: 4096.0000 (5517.5911)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.12 GB / 503.51 GB
Epoch: [17]  [640/893]  eta: 0:06:17  lr: 0.001784  min_lr: 0.000002  loss: 0.3909 (0.3341)  class_acc: 0.8214 (0.8529)  loss_scale: 4096.0000 (5495.4134)  weight_decay: 0.0500 (0.0500)  time: 1.4595  data: 0.0003  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.25 GB / 503.51 GB
Epoch: [17]  [650/893]  eta: 0:06:02  lr: 0.001783  min_lr: 0.000002  loss: 0.3481 (0.3344)  class_acc: 0.8393 (0.8526)  loss_scale: 4096.0000 (5473.9171)  weight_decay: 0.0500 (0.0500)  time: 1.4652  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.83 GB / 503.51 GB
Epoch: [17]  [660/893]  eta: 0:05:47  lr: 0.001783  min_lr: 0.000002  loss: 0.3035 (0.3337)  class_acc: 0.8571 (0.8529)  loss_scale: 4096.0000 (5453.0711)  weight_decay: 0.0500 (0.0500)  time: 1.4716  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.16 GB / 503.51 GB
Epoch: [17]  [670/893]  eta: 0:05:32  lr: 0.001782  min_lr: 0.000002  loss: 0.2932 (0.3336)  class_acc: 0.8571 (0.8526)  loss_scale: 4096.0000 (5432.8465)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.58 GB / 503.51 GB
Epoch: [17]  [680/893]  eta: 0:05:17  lr: 0.001781  min_lr: 0.000002  loss: 0.2922 (0.3330)  class_acc: 0.8571 (0.8528)  loss_scale: 4096.0000 (5413.2159)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.09 GB / 503.51 GB
Epoch: [17]  [690/893]  eta: 0:05:02  lr: 0.001781  min_lr: 0.000002  loss: 0.3010 (0.3328)  class_acc: 0.8750 (0.8529)  loss_scale: 4096.0000 (5394.1534)  weight_decay: 0.0500 (0.0500)  time: 1.4688  data: 0.0002  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.21 GB / 503.51 GB
Epoch: [17]  [700/893]  eta: 0:04:47  lr: 0.001780  min_lr: 0.000002  loss: 0.3137 (0.3328)  class_acc: 0.8393 (0.8527)  loss_scale: 4096.0000 (5375.6348)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0002  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.23 GB / 503.51 GB
Epoch: [17]  [710/893]  eta: 0:04:32  lr: 0.001779  min_lr: 0.000002  loss: 0.3137 (0.3324)  class_acc: 0.8393 (0.8529)  loss_scale: 4096.0000 (5357.6371)  weight_decay: 0.0500 (0.0500)  time: 1.4658  data: 0.0002  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.71 GB / 503.51 GB
Epoch: [17]  [720/893]  eta: 0:04:17  lr: 0.001779  min_lr: 0.000002  loss: 0.2905 (0.3321)  class_acc: 0.8750 (0.8531)  loss_scale: 4096.0000 (5340.1387)  weight_decay: 0.0500 (0.0500)  time: 1.4652  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.46 GB / 503.51 GB
Epoch: [17]  [730/893]  eta: 0:04:02  lr: 0.001778  min_lr: 0.000002  loss: 0.2852 (0.3317)  class_acc: 0.8750 (0.8534)  loss_scale: 4096.0000 (5323.1190)  weight_decay: 0.0500 (0.0500)  time: 1.4612  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.68 GB / 503.51 GB
Epoch: [17]  [740/893]  eta: 0:03:47  lr: 0.001777  min_lr: 0.000002  loss: 0.2825 (0.3313)  class_acc: 0.8750 (0.8537)  loss_scale: 4096.0000 (5306.5587)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0003  max mem: 31081
[2025-03-11 05:07:30,038] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 05:07:30,039] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.82 GB / 503.51 GB
Epoch: [17]  [750/893]  eta: 0:03:32  lr: 0.001777  min_lr: 0.000002  loss: 0.3140 (0.3314)  class_acc: 0.8750 (0.8538)  loss_scale: 4096.0000 (5339.5260)  weight_decay: 0.0500 (0.0500)  time: 1.4608  data: 0.0002  max mem: 31081
[2025-03-11 05:07:50,630] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 15920
[2025-03-11 05:07:50,630] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 05:07:50,630] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.04 GB / 503.51 GB
Epoch: [17]  [760/893]  eta: 0:03:17  lr: 0.001776  min_lr: 0.000002  loss: 0.3491 (0.3317)  class_acc: 0.8571 (0.8537)  loss_scale: 8192.0000 (5350.0972)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0002  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.55 GB / 503.51 GB
Epoch: [17]  [770/893]  eta: 0:03:02  lr: 0.001775  min_lr: 0.000002  loss: 0.3403 (0.3320)  class_acc: 0.8393 (0.8535)  loss_scale: 4096.0000 (5333.8314)  weight_decay: 0.0500 (0.0500)  time: 1.4735  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.42 GB / 503.51 GB
Epoch: [17]  [780/893]  eta: 0:02:47  lr: 0.001775  min_lr: 0.000002  loss: 0.3098 (0.3317)  class_acc: 0.8571 (0.8538)  loss_scale: 4096.0000 (5317.9821)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.06 GB / 503.51 GB
Epoch: [17]  [790/893]  eta: 0:02:33  lr: 0.001774  min_lr: 0.000002  loss: 0.3237 (0.3316)  class_acc: 0.8750 (0.8538)  loss_scale: 4096.0000 (5302.5335)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.53 GB / 503.51 GB
Epoch: [17]  [800/893]  eta: 0:02:18  lr: 0.001773  min_lr: 0.000002  loss: 0.3301 (0.3316)  class_acc: 0.8393 (0.8539)  loss_scale: 4096.0000 (5287.4707)  weight_decay: 0.0500 (0.0500)  time: 1.4713  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.54 GB / 503.51 GB
Epoch: [17]  [810/893]  eta: 0:02:03  lr: 0.001773  min_lr: 0.000002  loss: 0.3503 (0.3319)  class_acc: 0.8571 (0.8539)  loss_scale: 4096.0000 (5272.7793)  weight_decay: 0.0500 (0.0500)  time: 1.4748  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.36 GB / 503.51 GB
Epoch: [17]  [820/893]  eta: 0:01:48  lr: 0.001772  min_lr: 0.000002  loss: 0.3689 (0.3326)  class_acc: 0.8393 (0.8534)  loss_scale: 4096.0000 (5258.4458)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.61 GB / 503.51 GB
Epoch: [17]  [830/893]  eta: 0:01:33  lr: 0.001771  min_lr: 0.000002  loss: 0.3428 (0.3325)  class_acc: 0.8214 (0.8535)  loss_scale: 4096.0000 (5244.4573)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0003  max mem: 31081
[2025-03-11 05:09:46,647] [INFO] [logging.py:129:log_dist] [Rank 0] step=16000, skipped=94, lr=[2.3130615649708804e-06, 2.3130615649708804e-06, 3.855102608284801e-06, 3.855102608284801e-06, 6.425171013808001e-06, 6.425171013808001e-06, 1.070861835634667e-05, 1.070861835634667e-05, 1.7847697260577784e-05, 1.7847697260577784e-05, 2.9746162100962972e-05, 2.9746162100962972e-05, 4.9576936834938294e-05, 4.9576936834938294e-05, 8.262822805823049e-05, 8.262822805823049e-05, 0.00013771371343038416, 0.00013771371343038416, 0.00022952285571730695, 0.00022952285571730695, 0.0003825380928621782, 0.0003825380928621782, 0.0006375634881036305, 0.0006375634881036305, 0.0010626058135060508, 0.0010626058135060508, 0.0017710096891767513, 0.0017710096891767513], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 05:09:46,648] [INFO] [timer.py:264:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=61.015598487068004, CurrSamplesPerSec=61.73966204297699, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.17 GB / 503.51 GB
Epoch: [17]  [840/893]  eta: 0:01:18  lr: 0.001771  min_lr: 0.000002  loss: 0.3235 (0.3327)  class_acc: 0.8571 (0.8535)  loss_scale: 4096.0000 (5230.8014)  weight_decay: 0.0500 (0.0500)  time: 1.4696  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.81 GB / 503.51 GB
Epoch: [17]  [850/893]  eta: 0:01:03  lr: 0.001770  min_lr: 0.000002  loss: 0.3267 (0.3328)  class_acc: 0.8571 (0.8535)  loss_scale: 4096.0000 (5217.4665)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0002  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.05 GB / 503.51 GB
Epoch: [17]  [860/893]  eta: 0:00:48  lr: 0.001769  min_lr: 0.000002  loss: 0.3000 (0.3328)  class_acc: 0.8571 (0.8535)  loss_scale: 4096.0000 (5204.4413)  weight_decay: 0.0500 (0.0500)  time: 1.4508  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.15 GB / 503.51 GB
Epoch: [17]  [870/893]  eta: 0:00:34  lr: 0.001769  min_lr: 0.000002  loss: 0.3301 (0.3330)  class_acc: 0.8571 (0.8535)  loss_scale: 4096.0000 (5191.7153)  weight_decay: 0.0500 (0.0500)  time: 1.4438  data: 0.0002  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.49 GB / 503.51 GB
Epoch: [17]  [880/893]  eta: 0:00:19  lr: 0.001768  min_lr: 0.000002  loss: 0.3293 (0.3324)  class_acc: 0.8571 (0.8538)  loss_scale: 4096.0000 (5179.2781)  weight_decay: 0.0500 (0.0500)  time: 1.4474  data: 0.0002  max mem: 31081
[2025-03-11 05:10:59,209] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 05:10:59,209] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.03 GB / 503.51 GB
Epoch: [17]  [890/893]  eta: 0:00:04  lr: 0.001767  min_lr: 0.000002  loss: 0.3044 (0.3325)  class_acc: 0.8750 (0.8538)  loss_scale: 4096.0000 (5194.7026)  weight_decay: 0.0500 (0.0500)  time: 1.4510  data: 0.0002  max mem: 31081
Epoch: [17]  [892/893]  eta: 0:00:01  lr: 0.001767  min_lr: 0.000002  loss: 0.3115 (0.3325)  class_acc: 0.8571 (0.8538)  loss_scale: 4096.0000 (5198.0628)  weight_decay: 0.0500 (0.0500)  time: 1.3980  data: 0.0002  max mem: 31081
Epoch: [17] Total time: 0:22:03 (1.4821 s / it)
Averaged stats: lr: 0.001767  min_lr: 0.000002  loss: 0.3115 (0.3325)  class_acc: 0.8571 (0.8538)  loss_scale: 4096.0000 (5198.0628)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:16:49  loss: 0.4714 (0.4714)  acc: 76.1905 (76.1905)  time: 11.2769  data: 10.7410  max mem: 31081
Val:  [ 10/728]  eta: 0:18:30  loss: 0.3953 (0.4401)  acc: 80.9524 (80.5195)  time: 1.5461  data: 1.0241  max mem: 31081
Val:  [ 20/728]  eta: 0:14:11  loss: 0.3511 (0.4405)  acc: 82.1429 (80.1587)  time: 0.6985  data: 0.1780  max mem: 31081
Val:  [ 30/728]  eta: 0:12:47  loss: 0.3511 (0.4516)  acc: 79.7619 (79.1859)  time: 0.8536  data: 0.3280  max mem: 31081
Val:  [ 40/728]  eta: 0:11:46  loss: 0.4523 (0.4667)  acc: 78.5714 (78.5424)  time: 0.8437  data: 0.3187  max mem: 31081
Val:  [ 50/728]  eta: 0:11:11  loss: 0.3997 (0.4573)  acc: 76.1905 (79.0383)  time: 0.8223  data: 0.3008  max mem: 31081
Val:  [ 60/728]  eta: 0:10:19  loss: 0.5052 (0.4775)  acc: 76.1905 (78.6885)  time: 0.7228  data: 0.1989  max mem: 31081
Val:  [ 70/728]  eta: 0:09:51  loss: 0.5207 (0.4855)  acc: 76.1905 (78.3702)  time: 0.6638  data: 0.1403  max mem: 31081
Val:  [ 80/728]  eta: 0:09:35  loss: 0.4750 (0.4986)  acc: 75.0000 (78.0423)  time: 0.7687  data: 0.2474  max mem: 31081
Val:  [ 90/728]  eta: 0:09:21  loss: 0.5415 (0.5184)  acc: 75.0000 (77.1193)  time: 0.8170  data: 0.2963  max mem: 31081
Val:  [100/728]  eta: 0:09:11  loss: 0.4587 (0.5079)  acc: 77.3810 (77.5695)  time: 0.8356  data: 0.3144  max mem: 31081
Val:  [110/728]  eta: 0:09:01  loss: 0.4390 (0.5086)  acc: 77.3810 (77.5097)  time: 0.8557  data: 0.3359  max mem: 31081
Val:  [120/728]  eta: 0:08:46  loss: 0.3751 (0.4939)  acc: 82.1429 (78.3058)  time: 0.8103  data: 0.2889  max mem: 31081
Val:  [130/728]  eta: 0:08:26  loss: 0.3387 (0.4972)  acc: 84.5238 (78.3624)  time: 0.6807  data: 0.1589  max mem: 31081
Val:  [140/728]  eta: 0:08:17  loss: 0.4452 (0.5031)  acc: 77.3810 (78.0986)  time: 0.7186  data: 0.1962  max mem: 31081
Val:  [150/728]  eta: 0:08:03  loss: 0.5362 (0.5047)  acc: 73.8095 (78.0590)  time: 0.7655  data: 0.2415  max mem: 31081
Val:  [160/728]  eta: 0:07:54  loss: 0.4120 (0.5044)  acc: 75.0000 (77.9873)  time: 0.7678  data: 0.2448  max mem: 31081
Val:  [170/728]  eta: 0:07:44  loss: 0.4214 (0.5039)  acc: 78.5714 (77.9449)  time: 0.8081  data: 0.2849  max mem: 31081
Val:  [180/728]  eta: 0:07:34  loss: 0.4324 (0.5010)  acc: 80.9524 (78.1044)  time: 0.7755  data: 0.2527  max mem: 31081
Val:  [190/728]  eta: 0:07:19  loss: 0.3984 (0.4961)  acc: 79.7619 (78.2286)  time: 0.6813  data: 0.1612  max mem: 31081
Val:  [200/728]  eta: 0:07:11  loss: 0.3965 (0.4906)  acc: 79.7619 (78.3108)  time: 0.7161  data: 0.1945  max mem: 31081
Val:  [210/728]  eta: 0:07:06  loss: 0.3888 (0.4933)  acc: 80.9524 (78.3570)  time: 0.8828  data: 0.3595  max mem: 31081
Val:  [220/728]  eta: 0:06:58  loss: 0.3888 (0.4900)  acc: 82.1429 (78.4637)  time: 0.8896  data: 0.3678  max mem: 31081
Val:  [230/728]  eta: 0:06:49  loss: 0.4402 (0.4944)  acc: 79.7619 (78.2931)  time: 0.8241  data: 0.3006  max mem: 31081
Val:  [240/728]  eta: 0:06:38  loss: 0.4402 (0.4929)  acc: 80.9524 (78.5023)  time: 0.7269  data: 0.2015  max mem: 31081
Val:  [250/728]  eta: 0:06:26  loss: 0.4761 (0.4940)  acc: 80.9524 (78.5240)  time: 0.6433  data: 0.1181  max mem: 31081
Val:  [260/728]  eta: 0:06:19  loss: 0.3850 (0.4933)  acc: 80.9524 (78.5714)  time: 0.7491  data: 0.2237  max mem: 31081
Val:  [270/728]  eta: 0:06:11  loss: 0.3825 (0.4947)  acc: 80.9524 (78.5099)  time: 0.8453  data: 0.3204  max mem: 31081
Val:  [280/728]  eta: 0:06:06  loss: 0.4955 (0.4948)  acc: 77.3810 (78.5248)  time: 0.9162  data: 0.3934  max mem: 31081
Val:  [290/728]  eta: 0:05:58  loss: 0.3874 (0.4915)  acc: 79.7619 (78.6123)  time: 0.9174  data: 0.3947  max mem: 31081
Val:  [300/728]  eta: 0:05:46  loss: 0.3779 (0.4900)  acc: 78.5714 (78.6861)  time: 0.6724  data: 0.1464  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.5053 (0.4920)  acc: 77.3810 (78.5867)  time: 0.6742  data: 0.1491  max mem: 31081
Val:  [320/728]  eta: 0:05:29  loss: 0.4282 (0.4916)  acc: 78.5714 (78.6271)  time: 0.8001  data: 0.2778  max mem: 31081
Val:  [330/728]  eta: 0:05:22  loss: 0.3667 (0.4889)  acc: 80.9524 (78.7117)  time: 0.8389  data: 0.3165  max mem: 31081
Val:  [340/728]  eta: 0:05:15  loss: 0.3777 (0.4849)  acc: 80.9524 (78.8821)  time: 0.8933  data: 0.3729  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3733 (0.4853)  acc: 84.5238 (78.9276)  time: 0.8449  data: 0.3214  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.3671 (0.4821)  acc: 85.7143 (79.0826)  time: 0.6638  data: 0.1363  max mem: 31081
Val:  [370/728]  eta: 0:04:48  loss: 0.3671 (0.4847)  acc: 84.5238 (78.9661)  time: 0.6992  data: 0.1758  max mem: 31081
Val:  [380/728]  eta: 0:04:41  loss: 0.4752 (0.4864)  acc: 77.3810 (78.8933)  time: 0.8543  data: 0.3338  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.3484 (0.4820)  acc: 80.9524 (79.0373)  time: 0.8430  data: 0.3227  max mem: 31081
Val:  [400/728]  eta: 0:04:26  loss: 0.3049 (0.4818)  acc: 85.7143 (79.1236)  time: 0.8851  data: 0.3640  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.3558 (0.4813)  acc: 80.9524 (79.1594)  time: 0.8920  data: 0.3713  max mem: 31081
Val:  [420/728]  eta: 0:04:08  loss: 0.4019 (0.4809)  acc: 79.7619 (79.1964)  time: 0.6922  data: 0.1720  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4356 (0.4804)  acc: 79.7619 (79.2040)  time: 0.6897  data: 0.1681  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.4356 (0.4796)  acc: 79.7619 (79.1923)  time: 0.8344  data: 0.3104  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4489 (0.4818)  acc: 83.3333 (79.1416)  time: 0.8316  data: 0.3058  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.4778 (0.4836)  acc: 80.9524 (79.0595)  time: 0.8291  data: 0.3050  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.4408 (0.4818)  acc: 80.9524 (79.1401)  time: 0.7908  data: 0.2684  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.3882 (0.4815)  acc: 77.3810 (79.1481)  time: 0.6510  data: 0.1269  max mem: 31081
Val:  [490/728]  eta: 0:03:11  loss: 0.4973 (0.4820)  acc: 77.3810 (79.0951)  time: 0.6896  data: 0.1641  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.4350 (0.4830)  acc: 78.5714 (79.0562)  time: 0.8423  data: 0.3192  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3427 (0.4808)  acc: 82.1429 (79.1632)  time: 0.8750  data: 0.3545  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.4096 (0.4839)  acc: 79.7619 (79.0558)  time: 0.8588  data: 0.3347  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.4877 (0.4832)  acc: 77.3810 (79.1095)  time: 0.7507  data: 0.2228  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.4464 (0.4851)  acc: 77.3810 (79.0313)  time: 0.6097  data: 0.0876  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.5283 (0.4858)  acc: 76.1905 (79.0252)  time: 0.6680  data: 0.1467  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.5124 (0.4854)  acc: 80.9524 (79.0722)  time: 0.7605  data: 0.2375  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4287 (0.4900)  acc: 80.9524 (78.9738)  time: 0.8082  data: 0.2868  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4899 (0.4901)  acc: 77.3810 (78.9853)  time: 0.8498  data: 0.3258  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.5103 (0.4908)  acc: 77.3810 (78.9481)  time: 0.7438  data: 0.2216  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4377 (0.4915)  acc: 79.7619 (78.9478)  time: 0.7117  data: 0.1898  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.5273 (0.4941)  acc: 77.3810 (78.8228)  time: 0.6475  data: 0.1235  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4444 (0.4968)  acc: 78.5714 (78.7900)  time: 0.7497  data: 0.2267  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4166 (0.4964)  acc: 80.9524 (78.8186)  time: 0.8638  data: 0.3401  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.4368 (0.4962)  acc: 80.9524 (78.8240)  time: 0.7858  data: 0.2620  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4030 (0.4951)  acc: 80.9524 (78.8604)  time: 0.7889  data: 0.2655  max mem: 31081
Val:  [660/728]  eta: 0:00:54  loss: 0.3806 (0.4950)  acc: 83.3333 (78.9154)  time: 0.8026  data: 0.2776  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4527 (0.4961)  acc: 80.9524 (78.8837)  time: 0.6796  data: 0.1542  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4996 (0.4952)  acc: 76.1905 (78.9280)  time: 0.6608  data: 0.1394  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3826 (0.4945)  acc: 82.1429 (78.9642)  time: 0.8630  data: 0.3450  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4031 (0.4938)  acc: 79.7619 (78.9807)  time: 0.8756  data: 0.3565  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4031 (0.4938)  acc: 79.7619 (78.9867)  time: 0.8319  data: 0.3128  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.3970 (0.4938)  acc: 78.5714 (79.0057)  time: 0.7991  data: 0.2867  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4236 (0.4946)  acc: 78.5714 (78.9872)  time: 0.7758  data: 0.2867  max mem: 31081
Val: Total time: 0:09:35 (0.7899 s / it)
* Acc@1 78.987 AP 0.8002345561981201 loss 0.495
Accuracy of the network on the 61096 val videos: 79.0%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.78 GB / 503.51 GB
Epoch: [18]  [  0/893]  eta: 3:20:10  lr: 0.001767  min_lr: 0.000002  loss: 0.2588 (0.2588)  class_acc: 0.8571 (0.8571)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 13.4492  data: 12.2298  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.77 GB / 503.51 GB
Epoch: [18]  [ 10/893]  eta: 0:38:41  lr: 0.001766  min_lr: 0.000002  loss: 0.2949 (0.2855)  class_acc: 0.8750 (0.8718)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6286  data: 1.1521  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.09 GB / 503.51 GB
Epoch: [18]  [ 20/893]  eta: 0:30:27  lr: 0.001766  min_lr: 0.000002  loss: 0.2625 (0.2750)  class_acc: 0.8750 (0.8827)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5250  data: 0.0224  max mem: 31081
[2025-03-11 05:21:31,813] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 16078
[2025-03-11 05:21:31,813] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 05:21:31,813] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.98 GB / 503.51 GB
Epoch: [18]  [ 30/893]  eta: 0:27:24  lr: 0.001765  min_lr: 0.000002  loss: 0.2595 (0.2845)  class_acc: 0.8929 (0.8819)  loss_scale: 8192.0000 (7002.8387)  weight_decay: 0.0500 (0.0500)  time: 1.5074  data: 0.0005  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.04 GB / 503.51 GB
Epoch: [18]  [ 40/893]  eta: 0:25:41  lr: 0.001764  min_lr: 0.000002  loss: 0.2827 (0.2933)  class_acc: 0.8571 (0.8750)  loss_scale: 4096.0000 (6293.8537)  weight_decay: 0.0500 (0.0500)  time: 1.5067  data: 0.0006  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.31 GB / 503.51 GB
Epoch: [18]  [ 50/893]  eta: 0:24:31  lr: 0.001764  min_lr: 0.000002  loss: 0.3169 (0.3023)  class_acc: 0.8571 (0.8687)  loss_scale: 4096.0000 (5862.9020)  weight_decay: 0.0500 (0.0500)  time: 1.4969  data: 0.0007  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.18 GB / 503.51 GB
Epoch: [18]  [ 60/893]  eta: 0:23:35  lr: 0.001763  min_lr: 0.000002  loss: 0.3315 (0.3080)  class_acc: 0.8571 (0.8662)  loss_scale: 4096.0000 (5573.2459)  weight_decay: 0.0500 (0.0500)  time: 1.4794  data: 0.0005  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.49 GB / 503.51 GB
Epoch: [18]  [ 70/893]  eta: 0:22:52  lr: 0.001762  min_lr: 0.000002  loss: 0.3208 (0.3095)  class_acc: 0.8571 (0.8654)  loss_scale: 4096.0000 (5365.1831)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.02 GB / 503.51 GB
Epoch: [18]  [ 80/893]  eta: 0:22:16  lr: 0.001762  min_lr: 0.000002  loss: 0.3030 (0.3084)  class_acc: 0.8571 (0.8651)  loss_scale: 4096.0000 (5208.4938)  weight_decay: 0.0500 (0.0500)  time: 1.4721  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.47 GB / 503.51 GB
Epoch: [18]  [ 90/893]  eta: 0:21:44  lr: 0.001761  min_lr: 0.000002  loss: 0.2915 (0.3075)  class_acc: 0.8750 (0.8666)  loss_scale: 4096.0000 (5086.2418)  weight_decay: 0.0500 (0.0500)  time: 1.4724  data: 0.0002  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.79 GB / 503.51 GB
Epoch: [18]  [100/893]  eta: 0:21:16  lr: 0.001760  min_lr: 0.000002  loss: 0.3118 (0.3098)  class_acc: 0.8750 (0.8649)  loss_scale: 4096.0000 (4988.1980)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.71 GB / 503.51 GB
Epoch: [18]  [110/893]  eta: 0:20:50  lr: 0.001760  min_lr: 0.000002  loss: 0.3020 (0.3081)  class_acc: 0.8393 (0.8653)  loss_scale: 4096.0000 (4907.8198)  weight_decay: 0.0500 (0.0500)  time: 1.4720  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.53 GB / 503.51 GB
Epoch: [18]  [120/893]  eta: 0:20:26  lr: 0.001759  min_lr: 0.000002  loss: 0.2808 (0.3091)  class_acc: 0.8571 (0.8650)  loss_scale: 4096.0000 (4840.7273)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0002  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.05 GB / 503.51 GB
Epoch: [18]  [130/893]  eta: 0:20:04  lr: 0.001758  min_lr: 0.000002  loss: 0.3062 (0.3115)  class_acc: 0.8393 (0.8641)  loss_scale: 4096.0000 (4783.8779)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.46 GB / 503.51 GB
Epoch: [18]  [140/893]  eta: 0:19:43  lr: 0.001758  min_lr: 0.000002  loss: 0.3052 (0.3120)  class_acc: 0.8393 (0.8632)  loss_scale: 4096.0000 (4735.0922)  weight_decay: 0.0500 (0.0500)  time: 1.4782  data: 0.0004  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.31 GB / 503.51 GB
Epoch: [18]  [150/893]  eta: 0:19:22  lr: 0.001757  min_lr: 0.000002  loss: 0.3052 (0.3127)  class_acc: 0.8750 (0.8633)  loss_scale: 4096.0000 (4692.7682)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
[2025-03-11 05:24:42,573] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 05:24:42,573] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.17 GB / 503.51 GB
Epoch: [18]  [160/893]  eta: 0:19:02  lr: 0.001756  min_lr: 0.000002  loss: 0.3262 (0.3148)  class_acc: 0.8571 (0.8624)  loss_scale: 4096.0000 (4910.1118)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.54 GB / 503.51 GB
Epoch: [18]  [170/893]  eta: 0:18:42  lr: 0.001756  min_lr: 0.000002  loss: 0.3159 (0.3144)  class_acc: 0.8571 (0.8628)  loss_scale: 8192.0000 (5102.0351)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0002  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.72 GB / 503.51 GB
Epoch: [18]  [180/893]  eta: 0:18:23  lr: 0.001755  min_lr: 0.000002  loss: 0.2971 (0.3145)  class_acc: 0.8571 (0.8626)  loss_scale: 8192.0000 (5272.7514)  weight_decay: 0.0500 (0.0500)  time: 1.4644  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.44 GB / 503.51 GB
Epoch: [18]  [190/893]  eta: 0:18:05  lr: 0.001754  min_lr: 0.000002  loss: 0.2947 (0.3156)  class_acc: 0.8571 (0.8618)  loss_scale: 8192.0000 (5425.5916)  weight_decay: 0.0500 (0.0500)  time: 1.4685  data: 0.0004  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.97 GB / 503.51 GB
Epoch: [18]  [200/893]  eta: 0:17:47  lr: 0.001754  min_lr: 0.000002  loss: 0.3186 (0.3155)  class_acc: 0.8393 (0.8614)  loss_scale: 8192.0000 (5563.2239)  weight_decay: 0.0500 (0.0500)  time: 1.4704  data: 0.0002  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.73 GB / 503.51 GB
Epoch: [18]  [210/893]  eta: 0:17:29  lr: 0.001753  min_lr: 0.000002  loss: 0.3147 (0.3160)  class_acc: 0.8393 (0.8612)  loss_scale: 8192.0000 (5687.8104)  weight_decay: 0.0500 (0.0500)  time: 1.4726  data: 0.0003  max mem: 31081
[2025-03-11 05:26:10,735] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 16267
[2025-03-11 05:26:10,735] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 05:26:10,735] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.29 GB / 503.51 GB
Epoch: [18]  [220/893]  eta: 0:17:12  lr: 0.001752  min_lr: 0.000002  loss: 0.3271 (0.3204)  class_acc: 0.8393 (0.8591)  loss_scale: 4096.0000 (5615.7828)  weight_decay: 0.0500 (0.0500)  time: 1.4743  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.07 GB / 503.51 GB
Epoch: [18]  [230/893]  eta: 0:16:55  lr: 0.001752  min_lr: 0.000002  loss: 0.3804 (0.3211)  class_acc: 0.8214 (0.8589)  loss_scale: 4096.0000 (5549.9913)  weight_decay: 0.0500 (0.0500)  time: 1.4733  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.71 GB / 503.51 GB
Epoch: [18]  [240/893]  eta: 0:16:38  lr: 0.001751  min_lr: 0.000002  loss: 0.3479 (0.3216)  class_acc: 0.8214 (0.8585)  loss_scale: 4096.0000 (5489.6598)  weight_decay: 0.0500 (0.0500)  time: 1.4732  data: 0.0002  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.06 GB / 503.51 GB
Epoch: [18]  [250/893]  eta: 0:16:21  lr: 0.001750  min_lr: 0.000002  loss: 0.3269 (0.3213)  class_acc: 0.8214 (0.8584)  loss_scale: 4096.0000 (5434.1355)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0002  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.42 GB / 503.51 GB
Epoch: [18]  [260/893]  eta: 0:16:04  lr: 0.001749  min_lr: 0.000002  loss: 0.3186 (0.3218)  class_acc: 0.8214 (0.8575)  loss_scale: 4096.0000 (5382.8659)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.55 GB / 503.51 GB
Epoch: [18]  [270/893]  eta: 0:15:48  lr: 0.001749  min_lr: 0.000002  loss: 0.2849 (0.3206)  class_acc: 0.8750 (0.8585)  loss_scale: 4096.0000 (5335.3801)  weight_decay: 0.0500 (0.0500)  time: 1.4698  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.73 GB / 503.51 GB
Epoch: [18]  [280/893]  eta: 0:15:32  lr: 0.001748  min_lr: 0.000002  loss: 0.3213 (0.3217)  class_acc: 0.8571 (0.8582)  loss_scale: 4096.0000 (5291.2740)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.33 GB / 503.51 GB
Epoch: [18]  [290/893]  eta: 0:15:15  lr: 0.001747  min_lr: 0.000002  loss: 0.3257 (0.3224)  class_acc: 0.8571 (0.8576)  loss_scale: 4096.0000 (5250.1993)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.10 GB / 503.51 GB
Epoch: [18]  [300/893]  eta: 0:14:59  lr: 0.001747  min_lr: 0.000002  loss: 0.3025 (0.3217)  class_acc: 0.8571 (0.8580)  loss_scale: 4096.0000 (5211.8538)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.92 GB / 503.51 GB
Epoch: [18]  [310/893]  eta: 0:14:43  lr: 0.001746  min_lr: 0.000002  loss: 0.3210 (0.3228)  class_acc: 0.8571 (0.8574)  loss_scale: 4096.0000 (5175.9743)  weight_decay: 0.0500 (0.0500)  time: 1.4723  data: 0.0002  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.44 GB / 503.51 GB
Epoch: [18]  [320/893]  eta: 0:14:27  lr: 0.001745  min_lr: 0.000002  loss: 0.3345 (0.3233)  class_acc: 0.8393 (0.8566)  loss_scale: 4096.0000 (5142.3302)  weight_decay: 0.0500 (0.0500)  time: 1.4793  data: 0.0002  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.10 GB / 503.51 GB
Epoch: [18]  [330/893]  eta: 0:14:11  lr: 0.001745  min_lr: 0.000002  loss: 0.3306 (0.3228)  class_acc: 0.8393 (0.8571)  loss_scale: 4096.0000 (5110.7190)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0003  max mem: 31081
[2025-03-11 05:29:20,279] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 05:29:20,279] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.49 GB / 503.51 GB
Epoch: [18]  [340/893]  eta: 0:13:55  lr: 0.001744  min_lr: 0.000002  loss: 0.2883 (0.3224)  class_acc: 0.8750 (0.8575)  loss_scale: 4096.0000 (5092.9736)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.52 GB / 503.51 GB
Epoch: [18]  [350/893]  eta: 0:13:40  lr: 0.001743  min_lr: 0.000002  loss: 0.2773 (0.3218)  class_acc: 0.8750 (0.8577)  loss_scale: 8192.0000 (5181.2650)  weight_decay: 0.0500 (0.0500)  time: 1.4712  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.55 GB / 503.51 GB
Epoch: [18]  [360/893]  eta: 0:13:24  lr: 0.001743  min_lr: 0.000002  loss: 0.3062 (0.3226)  class_acc: 0.8571 (0.8576)  loss_scale: 8192.0000 (5264.6648)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.81 GB / 503.51 GB
Epoch: [18]  [370/893]  eta: 0:13:08  lr: 0.001742  min_lr: 0.000002  loss: 0.3552 (0.3235)  class_acc: 0.8571 (0.8577)  loss_scale: 8192.0000 (5343.5687)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0002  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.64 GB / 503.51 GB
Epoch: [18]  [380/893]  eta: 0:12:53  lr: 0.001741  min_lr: 0.000002  loss: 0.3486 (0.3234)  class_acc: 0.8750 (0.8581)  loss_scale: 8192.0000 (5418.3307)  weight_decay: 0.0500 (0.0500)  time: 1.4708  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.26 GB / 503.51 GB
Epoch: [18]  [390/893]  eta: 0:12:37  lr: 0.001741  min_lr: 0.000002  loss: 0.3184 (0.3234)  class_acc: 0.8571 (0.8580)  loss_scale: 8192.0000 (5489.2685)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0002  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.47 GB / 503.51 GB
Epoch: [18]  [400/893]  eta: 0:12:21  lr: 0.001740  min_lr: 0.000002  loss: 0.3296 (0.3235)  class_acc: 0.8393 (0.8578)  loss_scale: 8192.0000 (5556.6683)  weight_decay: 0.0500 (0.0500)  time: 1.4591  data: 0.0003  max mem: 31081
[2025-03-11 05:30:54,115] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 16460
[2025-03-11 05:30:54,115] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 05:30:54,115] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.02 GB / 503.51 GB
Epoch: [18]  [410/893]  eta: 0:12:06  lr: 0.001739  min_lr: 0.000002  loss: 0.3296 (0.3236)  class_acc: 0.8571 (0.8579)  loss_scale: 8192.0000 (5551.0268)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0004  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.45 GB / 503.51 GB
Epoch: [18]  [420/893]  eta: 0:11:50  lr: 0.001738  min_lr: 0.000002  loss: 0.3291 (0.3237)  class_acc: 0.8571 (0.8579)  loss_scale: 4096.0000 (5516.4656)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0004  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.33 GB / 503.51 GB
Epoch: [18]  [430/893]  eta: 0:11:35  lr: 0.001738  min_lr: 0.000002  loss: 0.3035 (0.3232)  class_acc: 0.8750 (0.8582)  loss_scale: 4096.0000 (5483.5081)  weight_decay: 0.0500 (0.0500)  time: 1.4619  data: 0.0002  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.91 GB / 503.51 GB
Epoch: [18]  [440/893]  eta: 0:11:19  lr: 0.001737  min_lr: 0.000002  loss: 0.3008 (0.3230)  class_acc: 0.8571 (0.8582)  loss_scale: 4096.0000 (5452.0454)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.00 GB / 503.51 GB
Epoch: [18]  [450/893]  eta: 0:11:04  lr: 0.001736  min_lr: 0.000002  loss: 0.3040 (0.3220)  class_acc: 0.8571 (0.8583)  loss_scale: 4096.0000 (5421.9778)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0004  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.25 GB / 503.51 GB
Epoch: [18]  [460/893]  eta: 0:10:49  lr: 0.001736  min_lr: 0.000002  loss: 0.3040 (0.3221)  class_acc: 0.8571 (0.8583)  loss_scale: 4096.0000 (5393.2148)  weight_decay: 0.0500 (0.0500)  time: 1.4681  data: 0.0004  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.14 GB / 503.51 GB
Epoch: [18]  [470/893]  eta: 0:10:33  lr: 0.001735  min_lr: 0.000002  loss: 0.3511 (0.3236)  class_acc: 0.8393 (0.8576)  loss_scale: 4096.0000 (5365.6730)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.19 GB / 503.51 GB
Epoch: [18]  [480/893]  eta: 0:10:18  lr: 0.001734  min_lr: 0.000002  loss: 0.3511 (0.3240)  class_acc: 0.8214 (0.8572)  loss_scale: 4096.0000 (5339.2765)  weight_decay: 0.0500 (0.0500)  time: 1.4766  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.11 GB / 503.51 GB
Epoch: [18]  [490/893]  eta: 0:10:03  lr: 0.001734  min_lr: 0.000002  loss: 0.3147 (0.3236)  class_acc: 0.8393 (0.8573)  loss_scale: 4096.0000 (5313.9552)  weight_decay: 0.0500 (0.0500)  time: 1.4776  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.08 GB / 503.51 GB
Epoch: [18]  [500/893]  eta: 0:09:48  lr: 0.001733  min_lr: 0.000002  loss: 0.2966 (0.3235)  class_acc: 0.8750 (0.8576)  loss_scale: 4096.0000 (5289.6447)  weight_decay: 0.0500 (0.0500)  time: 1.4743  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.63 GB / 503.51 GB
Epoch: [18]  [510/893]  eta: 0:09:33  lr: 0.001732  min_lr: 0.000002  loss: 0.2966 (0.3231)  class_acc: 0.8750 (0.8577)  loss_scale: 4096.0000 (5266.2857)  weight_decay: 0.0500 (0.0500)  time: 1.4679  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.90 GB / 503.51 GB
Epoch: [18]  [520/893]  eta: 0:09:18  lr: 0.001732  min_lr: 0.000002  loss: 0.3091 (0.3230)  class_acc: 0.8750 (0.8580)  loss_scale: 4096.0000 (5243.8234)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0004  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.30 GB / 503.51 GB
Epoch: [18]  [530/893]  eta: 0:09:02  lr: 0.001731  min_lr: 0.000002  loss: 0.2952 (0.3226)  class_acc: 0.8750 (0.8583)  loss_scale: 4096.0000 (5222.2072)  weight_decay: 0.0500 (0.0500)  time: 1.4728  data: 0.0003  max mem: 31081
[2025-03-11 05:34:03,680] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 05:34:03,680] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.22 GB / 503.51 GB
Epoch: [18]  [540/893]  eta: 0:08:47  lr: 0.001730  min_lr: 0.000002  loss: 0.3052 (0.3230)  class_acc: 0.8571 (0.8581)  loss_scale: 4096.0000 (5261.9593)  weight_decay: 0.0500 (0.0500)  time: 1.4712  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.14 GB / 503.51 GB
Epoch: [18]  [550/893]  eta: 0:08:32  lr: 0.001729  min_lr: 0.000002  loss: 0.3203 (0.3235)  class_acc: 0.8393 (0.8581)  loss_scale: 8192.0000 (5315.1361)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.79 GB / 503.51 GB
Epoch: [18]  [560/893]  eta: 0:08:17  lr: 0.001729  min_lr: 0.000002  loss: 0.3284 (0.3239)  class_acc: 0.8571 (0.8581)  loss_scale: 8192.0000 (5366.4171)  weight_decay: 0.0500 (0.0500)  time: 1.4714  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.02 GB / 503.51 GB
Epoch: [18]  [570/893]  eta: 0:08:02  lr: 0.001728  min_lr: 0.000002  loss: 0.3198 (0.3240)  class_acc: 0.8750 (0.8583)  loss_scale: 8192.0000 (5415.9019)  weight_decay: 0.0500 (0.0500)  time: 1.4735  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.90 GB / 503.51 GB
Epoch: [18]  [580/893]  eta: 0:07:47  lr: 0.001727  min_lr: 0.000002  loss: 0.2856 (0.3234)  class_acc: 0.8750 (0.8586)  loss_scale: 8192.0000 (5463.6833)  weight_decay: 0.0500 (0.0500)  time: 1.4707  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.73 GB / 503.51 GB
Epoch: [18]  [590/893]  eta: 0:07:32  lr: 0.001727  min_lr: 0.000002  loss: 0.3276 (0.3238)  class_acc: 0.8571 (0.8578)  loss_scale: 8192.0000 (5509.8477)  weight_decay: 0.0500 (0.0500)  time: 1.4725  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.27 GB / 503.51 GB
Epoch: [18]  [600/893]  eta: 0:07:17  lr: 0.001726  min_lr: 0.000002  loss: 0.3357 (0.3235)  class_acc: 0.8214 (0.8579)  loss_scale: 8192.0000 (5554.4759)  weight_decay: 0.0500 (0.0500)  time: 1.4758  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.41 GB / 503.51 GB
Epoch: [18]  [610/893]  eta: 0:07:02  lr: 0.001725  min_lr: 0.000002  loss: 0.3074 (0.3233)  class_acc: 0.8750 (0.8582)  loss_scale: 8192.0000 (5597.6432)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0003  max mem: 31081
[2025-03-11 05:36:07,221] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 16673
[2025-03-11 05:36:07,222] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 05:36:07,222] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.84 GB / 503.51 GB
Epoch: [18]  [620/893]  eta: 0:06:47  lr: 0.001725  min_lr: 0.000002  loss: 0.3276 (0.3237)  class_acc: 0.8393 (0.8578)  loss_scale: 8192.0000 (5613.0370)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.83 GB / 503.51 GB
Epoch: [18]  [630/893]  eta: 0:06:32  lr: 0.001724  min_lr: 0.000002  loss: 0.3335 (0.3234)  class_acc: 0.8393 (0.8581)  loss_scale: 4096.0000 (5588.9952)  weight_decay: 0.0500 (0.0500)  time: 1.4663  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.85 GB / 503.51 GB
Epoch: [18]  [640/893]  eta: 0:06:17  lr: 0.001723  min_lr: 0.000002  loss: 0.2827 (0.3225)  class_acc: 0.8571 (0.8582)  loss_scale: 4096.0000 (5565.7036)  weight_decay: 0.0500 (0.0500)  time: 1.4757  data: 0.0003  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.65 GB / 503.51 GB
Epoch: [18]  [650/893]  eta: 0:06:02  lr: 0.001722  min_lr: 0.000002  loss: 0.3018 (0.3233)  class_acc: 0.8571 (0.8578)  loss_scale: 4096.0000 (5543.1275)  weight_decay: 0.0500 (0.0500)  time: 1.4778  data: 0.0002  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.84 GB / 503.51 GB
Epoch: [18]  [660/893]  eta: 0:05:47  lr: 0.001722  min_lr: 0.000002  loss: 0.3230 (0.3230)  class_acc: 0.8393 (0.8578)  loss_scale: 4096.0000 (5521.2345)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.21 GB / 503.51 GB
Epoch: [18]  [670/893]  eta: 0:05:32  lr: 0.001721  min_lr: 0.000002  loss: 0.3242 (0.3234)  class_acc: 0.8393 (0.8576)  loss_scale: 4096.0000 (5499.9940)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.60 GB / 503.51 GB
Epoch: [18]  [680/893]  eta: 0:05:17  lr: 0.001720  min_lr: 0.000002  loss: 0.3164 (0.3230)  class_acc: 0.8571 (0.8578)  loss_scale: 4096.0000 (5479.3774)  weight_decay: 0.0500 (0.0500)  time: 1.4715  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.58 GB / 503.51 GB
Epoch: [18]  [690/893]  eta: 0:05:02  lr: 0.001720  min_lr: 0.000002  loss: 0.2881 (0.3231)  class_acc: 0.8750 (0.8578)  loss_scale: 4096.0000 (5459.3575)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.62 GB / 503.51 GB
Epoch: [18]  [700/893]  eta: 0:04:47  lr: 0.001719  min_lr: 0.000002  loss: 0.2881 (0.3230)  class_acc: 0.8571 (0.8578)  loss_scale: 4096.0000 (5439.9087)  weight_decay: 0.0500 (0.0500)  time: 1.4595  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 97.96 GB / 503.51 GB
Epoch: [18]  [710/893]  eta: 0:04:32  lr: 0.001718  min_lr: 0.000002  loss: 0.3369 (0.3232)  class_acc: 0.8393 (0.8576)  loss_scale: 4096.0000 (5421.0070)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0004  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.68 GB / 503.51 GB
Epoch: [18]  [720/893]  eta: 0:04:17  lr: 0.001718  min_lr: 0.000002  loss: 0.3328 (0.3232)  class_acc: 0.8393 (0.8576)  loss_scale: 4096.0000 (5402.6297)  weight_decay: 0.0500 (0.0500)  time: 1.4739  data: 0.0004  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.50 GB / 503.51 GB
Epoch: [18]  [730/893]  eta: 0:04:02  lr: 0.001717  min_lr: 0.000002  loss: 0.3328 (0.3234)  class_acc: 0.8571 (0.8576)  loss_scale: 4096.0000 (5384.7551)  weight_decay: 0.0500 (0.0500)  time: 1.4778  data: 0.0005  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.37 GB / 503.51 GB
Epoch: [18]  [740/893]  eta: 0:03:47  lr: 0.001716  min_lr: 0.000002  loss: 0.3125 (0.3231)  class_acc: 0.8571 (0.8577)  loss_scale: 4096.0000 (5367.3630)  weight_decay: 0.0500 (0.0500)  time: 1.4751  data: 0.0004  max mem: 31081
[2025-03-11 05:39:16,932] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 05:39:16,932] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.28 GB / 503.51 GB
Epoch: [18]  [750/893]  eta: 0:03:32  lr: 0.001715  min_lr: 0.000002  loss: 0.2886 (0.3224)  class_acc: 0.8750 (0.8579)  loss_scale: 4096.0000 (5377.7044)  weight_decay: 0.0500 (0.0500)  time: 1.4754  data: 0.0003  max mem: 31081
[2025-03-11 05:39:24,405] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 16807
[2025-03-11 05:39:24,406] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 05:39:24,406] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.03 GB / 503.51 GB
Epoch: [18]  [760/893]  eta: 0:03:17  lr: 0.001715  min_lr: 0.000002  loss: 0.2708 (0.3226)  class_acc: 0.8929 (0.8580)  loss_scale: 4096.0000 (5360.8620)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.52 GB / 503.51 GB
Epoch: [18]  [770/893]  eta: 0:03:03  lr: 0.001714  min_lr: 0.000002  loss: 0.3152 (0.3225)  class_acc: 0.8750 (0.8581)  loss_scale: 4096.0000 (5344.4565)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.42 GB / 503.51 GB
Epoch: [18]  [780/893]  eta: 0:02:48  lr: 0.001713  min_lr: 0.000002  loss: 0.3152 (0.3222)  class_acc: 0.8750 (0.8584)  loss_scale: 4096.0000 (5328.4712)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.51 GB / 503.51 GB
Epoch: [18]  [790/893]  eta: 0:02:33  lr: 0.001713  min_lr: 0.000002  loss: 0.3105 (0.3222)  class_acc: 0.8571 (0.8584)  loss_scale: 4096.0000 (5312.8900)  weight_decay: 0.0500 (0.0500)  time: 1.4650  data: 0.0004  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.90 GB / 503.51 GB
Epoch: [18]  [800/893]  eta: 0:02:18  lr: 0.001712  min_lr: 0.000002  loss: 0.3208 (0.3224)  class_acc: 0.8571 (0.8583)  loss_scale: 4096.0000 (5297.6979)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0004  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.06 GB / 503.51 GB
Epoch: [18]  [810/893]  eta: 0:02:03  lr: 0.001711  min_lr: 0.000002  loss: 0.3096 (0.3221)  class_acc: 0.8571 (0.8587)  loss_scale: 4096.0000 (5282.8804)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.55 GB / 503.51 GB
Epoch: [18]  [820/893]  eta: 0:01:48  lr: 0.001711  min_lr: 0.000002  loss: 0.3076 (0.3223)  class_acc: 0.8750 (0.8587)  loss_scale: 4096.0000 (5268.4239)  weight_decay: 0.0500 (0.0500)  time: 1.4652  data: 0.0002  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.75 GB / 503.51 GB
Epoch: [18]  [830/893]  eta: 0:01:33  lr: 0.001710  min_lr: 0.000002  loss: 0.2986 (0.3220)  class_acc: 0.8929 (0.8590)  loss_scale: 4096.0000 (5254.3153)  weight_decay: 0.0500 (0.0500)  time: 1.4770  data: 0.0002  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.37 GB / 503.51 GB
Epoch: [18]  [840/893]  eta: 0:01:18  lr: 0.001709  min_lr: 0.000002  loss: 0.2969 (0.3218)  class_acc: 0.8750 (0.8590)  loss_scale: 4096.0000 (5240.5422)  weight_decay: 0.0500 (0.0500)  time: 1.4799  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.03 GB / 503.51 GB
Epoch: [18]  [850/893]  eta: 0:01:03  lr: 0.001708  min_lr: 0.000002  loss: 0.2915 (0.3216)  class_acc: 0.8571 (0.8591)  loss_scale: 4096.0000 (5227.0928)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.68 GB / 503.51 GB
Epoch: [18]  [860/893]  eta: 0:00:49  lr: 0.001708  min_lr: 0.000002  loss: 0.2866 (0.3212)  class_acc: 0.8750 (0.8593)  loss_scale: 4096.0000 (5213.9559)  weight_decay: 0.0500 (0.0500)  time: 1.4561  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.73 GB / 503.51 GB
Epoch: [18]  [870/893]  eta: 0:00:34  lr: 0.001707  min_lr: 0.000002  loss: 0.2705 (0.3206)  class_acc: 0.8750 (0.8596)  loss_scale: 4096.0000 (5201.1206)  weight_decay: 0.0500 (0.0500)  time: 1.4450  data: 0.0002  max mem: 31081
[2025-03-11 05:42:33,153] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 05:42:33,153] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.26 GB / 503.51 GB
Epoch: [18]  [880/893]  eta: 0:00:19  lr: 0.001706  min_lr: 0.000002  loss: 0.2815 (0.3209)  class_acc: 0.8750 (0.8595)  loss_scale: 4096.0000 (5193.2259)  weight_decay: 0.0500 (0.0500)  time: 1.4434  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.72 GB / 503.51 GB
Epoch: [18]  [890/893]  eta: 0:00:04  lr: 0.001706  min_lr: 0.000002  loss: 0.3279 (0.3210)  class_acc: 0.8571 (0.8595)  loss_scale: 8192.0000 (5226.8822)  weight_decay: 0.0500 (0.0500)  time: 1.4462  data: 0.0001  max mem: 31081
Epoch: [18]  [892/893]  eta: 0:00:01  lr: 0.001705  min_lr: 0.000002  loss: 0.3171 (0.3210)  class_acc: 0.8571 (0.8595)  loss_scale: 8192.0000 (5230.2063)  weight_decay: 0.0500 (0.0500)  time: 1.3923  data: 0.0001  max mem: 31081
Epoch: [18] Total time: 0:22:04 (1.4835 s / it)
Averaged stats: lr: 0.001705  min_lr: 0.000002  loss: 0.3171 (0.3210)  class_acc: 0.8571 (0.8595)  loss_scale: 8192.0000 (5230.2063)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:17:39  loss: 0.5901 (0.5901)  acc: 76.1905 (76.1905)  time: 11.3452  data: 10.8060  max mem: 31081
Val:  [ 10/728]  eta: 0:18:35  loss: 0.4127 (0.4080)  acc: 84.5238 (81.9264)  time: 1.5532  data: 1.0317  max mem: 31081
Val:  [ 20/728]  eta: 0:14:10  loss: 0.3857 (0.4314)  acc: 79.7619 (80.1020)  time: 0.6941  data: 0.1744  max mem: 31081
Val:  [ 30/728]  eta: 0:12:42  loss: 0.3898 (0.4426)  acc: 79.7619 (79.3779)  time: 0.8379  data: 0.3149  max mem: 31081
Val:  [ 40/728]  eta: 0:11:53  loss: 0.4760 (0.4760)  acc: 75.0000 (77.7584)  time: 0.8660  data: 0.3415  max mem: 31081
Val:  [ 50/728]  eta: 0:11:16  loss: 0.4359 (0.4583)  acc: 79.7619 (78.9449)  time: 0.8538  data: 0.3314  max mem: 31081
Val:  [ 60/728]  eta: 0:10:20  loss: 0.4359 (0.4682)  acc: 82.1429 (78.8056)  time: 0.7057  data: 0.1833  max mem: 31081
Val:  [ 70/728]  eta: 0:09:50  loss: 0.5072 (0.4910)  acc: 76.1905 (78.1858)  time: 0.6405  data: 0.1196  max mem: 31081
Val:  [ 80/728]  eta: 0:09:39  loss: 0.5072 (0.5014)  acc: 75.0000 (77.9248)  time: 0.7865  data: 0.2644  max mem: 31081
Val:  [ 90/728]  eta: 0:09:23  loss: 0.5597 (0.5197)  acc: 76.1905 (77.3417)  time: 0.8321  data: 0.3089  max mem: 31081
Val:  [100/728]  eta: 0:09:12  loss: 0.4408 (0.5050)  acc: 79.7619 (77.9232)  time: 0.8212  data: 0.2991  max mem: 31081
Val:  [110/728]  eta: 0:09:02  loss: 0.3832 (0.5016)  acc: 82.1429 (78.0566)  time: 0.8521  data: 0.3289  max mem: 31081
Val:  [120/728]  eta: 0:08:45  loss: 0.3297 (0.4908)  acc: 83.3333 (78.5813)  time: 0.7927  data: 0.2678  max mem: 31081
Val:  [130/728]  eta: 0:08:26  loss: 0.3086 (0.4933)  acc: 84.5238 (78.6714)  time: 0.6752  data: 0.1499  max mem: 31081
Val:  [140/728]  eta: 0:08:15  loss: 0.4106 (0.4977)  acc: 77.3810 (78.3181)  time: 0.7067  data: 0.1806  max mem: 31081
Val:  [150/728]  eta: 0:08:01  loss: 0.4654 (0.4995)  acc: 77.3810 (78.3507)  time: 0.7473  data: 0.2217  max mem: 31081
Val:  [160/728]  eta: 0:07:54  loss: 0.4731 (0.5026)  acc: 75.0000 (78.1574)  time: 0.7849  data: 0.2608  max mem: 31081
Val:  [170/728]  eta: 0:07:44  loss: 0.4679 (0.5038)  acc: 78.5714 (78.0284)  time: 0.8289  data: 0.3047  max mem: 31081
Val:  [180/728]  eta: 0:07:32  loss: 0.4456 (0.5019)  acc: 78.5714 (78.0847)  time: 0.7464  data: 0.2238  max mem: 31081
Val:  [190/728]  eta: 0:07:19  loss: 0.4321 (0.4999)  acc: 78.5714 (78.2037)  time: 0.6808  data: 0.1575  max mem: 31081
Val:  [200/728]  eta: 0:07:10  loss: 0.3807 (0.4934)  acc: 79.7619 (78.4234)  time: 0.7275  data: 0.2038  max mem: 31081
Val:  [210/728]  eta: 0:07:05  loss: 0.3981 (0.4979)  acc: 79.7619 (78.3796)  time: 0.8550  data: 0.3296  max mem: 31081
Val:  [220/728]  eta: 0:06:58  loss: 0.4413 (0.4954)  acc: 78.5714 (78.4637)  time: 0.9094  data: 0.3846  max mem: 31081
Val:  [230/728]  eta: 0:06:49  loss: 0.4413 (0.5022)  acc: 76.1905 (78.2416)  time: 0.8357  data: 0.3143  max mem: 31081
Val:  [240/728]  eta: 0:06:37  loss: 0.4424 (0.5014)  acc: 80.9524 (78.3541)  time: 0.7084  data: 0.1865  max mem: 31081
Val:  [250/728]  eta: 0:06:26  loss: 0.4529 (0.5031)  acc: 78.5714 (78.3248)  time: 0.6617  data: 0.1402  max mem: 31081
Val:  [260/728]  eta: 0:06:19  loss: 0.4450 (0.5019)  acc: 78.5714 (78.3571)  time: 0.7754  data: 0.2542  max mem: 31081
Val:  [270/728]  eta: 0:06:11  loss: 0.4708 (0.5035)  acc: 76.1905 (78.2376)  time: 0.8444  data: 0.3215  max mem: 31081
Val:  [280/728]  eta: 0:06:07  loss: 0.4798 (0.5042)  acc: 76.1905 (78.2156)  time: 0.9196  data: 0.3974  max mem: 31081
Val:  [290/728]  eta: 0:05:58  loss: 0.3920 (0.4996)  acc: 82.1429 (78.3914)  time: 0.9184  data: 0.3951  max mem: 31081
Val:  [300/728]  eta: 0:05:46  loss: 0.3920 (0.4990)  acc: 82.1429 (78.4370)  time: 0.6688  data: 0.1436  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.5668 (0.5011)  acc: 77.3810 (78.3226)  time: 0.6728  data: 0.1505  max mem: 31081
Val:  [320/728]  eta: 0:05:29  loss: 0.5098 (0.5008)  acc: 76.1905 (78.3230)  time: 0.8005  data: 0.2797  max mem: 31081
Val:  [330/728]  eta: 0:05:22  loss: 0.4156 (0.4970)  acc: 79.7619 (78.4240)  time: 0.8278  data: 0.3045  max mem: 31081
Val:  [340/728]  eta: 0:05:15  loss: 0.3705 (0.4928)  acc: 80.9524 (78.6413)  time: 0.8854  data: 0.3615  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3705 (0.4925)  acc: 84.5238 (78.7173)  time: 0.8468  data: 0.3208  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.3737 (0.4891)  acc: 82.1429 (78.8319)  time: 0.6621  data: 0.1346  max mem: 31081
Val:  [370/728]  eta: 0:04:48  loss: 0.3978 (0.4908)  acc: 82.1429 (78.7768)  time: 0.6973  data: 0.1732  max mem: 31081
Val:  [380/728]  eta: 0:04:40  loss: 0.4786 (0.4918)  acc: 78.5714 (78.7027)  time: 0.8487  data: 0.3250  max mem: 31081
Val:  [390/728]  eta: 0:04:32  loss: 0.3994 (0.4891)  acc: 79.7619 (78.7815)  time: 0.8266  data: 0.3048  max mem: 31081
Val:  [400/728]  eta: 0:04:25  loss: 0.3994 (0.4897)  acc: 80.9524 (78.8386)  time: 0.8681  data: 0.3481  max mem: 31081
Val:  [410/728]  eta: 0:04:17  loss: 0.4122 (0.4902)  acc: 79.7619 (78.8147)  time: 0.8817  data: 0.3599  max mem: 31081
Val:  [420/728]  eta: 0:04:07  loss: 0.4241 (0.4898)  acc: 78.5714 (78.8372)  time: 0.6861  data: 0.1649  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4592 (0.4893)  acc: 78.5714 (78.8145)  time: 0.6941  data: 0.1745  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.4224 (0.4881)  acc: 79.7619 (78.8171)  time: 0.8407  data: 0.3168  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4025 (0.4886)  acc: 80.9524 (78.7905)  time: 0.8428  data: 0.3160  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.5325 (0.4911)  acc: 72.6190 (78.6876)  time: 0.8433  data: 0.3183  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.4277 (0.4890)  acc: 77.3810 (78.7458)  time: 0.7984  data: 0.2743  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.3941 (0.4877)  acc: 79.7619 (78.8264)  time: 0.6515  data: 0.1297  max mem: 31081
Val:  [490/728]  eta: 0:03:10  loss: 0.4309 (0.4885)  acc: 78.5714 (78.8187)  time: 0.6915  data: 0.1714  max mem: 31081
Val:  [500/728]  eta: 0:03:03  loss: 0.4045 (0.4890)  acc: 78.5714 (78.8376)  time: 0.8575  data: 0.3363  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3391 (0.4871)  acc: 84.5238 (78.8976)  time: 0.8871  data: 0.3670  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.3769 (0.4898)  acc: 78.5714 (78.7702)  time: 0.8639  data: 0.3436  max mem: 31081
Val:  [530/728]  eta: 0:02:39  loss: 0.5194 (0.4899)  acc: 76.1905 (78.7911)  time: 0.7505  data: 0.2296  max mem: 31081
Val:  [540/728]  eta: 0:02:30  loss: 0.4966 (0.4919)  acc: 76.1905 (78.7387)  time: 0.6092  data: 0.0881  max mem: 31081
Val:  [550/728]  eta: 0:02:22  loss: 0.5798 (0.4934)  acc: 73.8095 (78.6449)  time: 0.6632  data: 0.1403  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.4629 (0.4934)  acc: 77.3810 (78.6627)  time: 0.7535  data: 0.2323  max mem: 31081
Val:  [570/728]  eta: 0:02:06  loss: 0.4753 (0.4979)  acc: 77.3810 (78.5589)  time: 0.8115  data: 0.2918  max mem: 31081
Val:  [580/728]  eta: 0:01:58  loss: 0.4753 (0.4989)  acc: 79.7619 (78.5981)  time: 0.8652  data: 0.3436  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.4640 (0.5004)  acc: 82.1429 (78.5634)  time: 0.7337  data: 0.2099  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4665 (0.5012)  acc: 79.7619 (78.5358)  time: 0.6521  data: 0.1276  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4975 (0.5029)  acc: 79.7619 (78.4331)  time: 0.6562  data: 0.1329  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4975 (0.5057)  acc: 77.3810 (78.4142)  time: 0.7737  data: 0.2503  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4207 (0.5047)  acc: 80.9524 (78.4262)  time: 0.8493  data: 0.3262  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3995 (0.5041)  acc: 79.7619 (78.4359)  time: 0.7932  data: 0.2704  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4193 (0.5032)  acc: 79.7619 (78.4489)  time: 0.7945  data: 0.2702  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4193 (0.5038)  acc: 79.7619 (78.4526)  time: 0.7738  data: 0.2498  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4653 (0.5049)  acc: 79.7619 (78.4472)  time: 0.6798  data: 0.1572  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.3970 (0.5033)  acc: 80.9524 (78.5242)  time: 0.6763  data: 0.1566  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3797 (0.5028)  acc: 83.3333 (78.5232)  time: 0.8382  data: 0.3162  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3839 (0.5021)  acc: 80.9524 (78.5527)  time: 0.8727  data: 0.3484  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4130 (0.5022)  acc: 82.1429 (78.5396)  time: 0.8363  data: 0.3148  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4392 (0.5024)  acc: 76.1905 (78.5202)  time: 0.7931  data: 0.2791  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4454 (0.5041)  acc: 76.1905 (78.4830)  time: 0.7685  data: 0.2791  max mem: 31081
Val: Total time: 0:09:34 (0.7890 s / it)
* Acc@1 78.483 AP 0.7958301305770874 loss 0.504
Accuracy of the network on the 61096 val videos: 78.5%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.57 GB / 503.51 GB
Epoch: [19]  [  0/893]  eta: 3:19:44  lr: 0.001705  min_lr: 0.000002  loss: 0.2747 (0.2747)  class_acc: 0.8929 (0.8929)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 13.4207  data: 12.0330  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.66 GB / 503.51 GB
Epoch: [19]  [ 10/893]  eta: 0:37:54  lr: 0.001705  min_lr: 0.000002  loss: 0.2915 (0.2895)  class_acc: 0.8750 (0.8831)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5758  data: 1.0945  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.08 GB / 503.51 GB
Epoch: [19]  [ 20/893]  eta: 0:30:03  lr: 0.001704  min_lr: 0.000002  loss: 0.3281 (0.3290)  class_acc: 0.8750 (0.8631)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4980  data: 0.0006  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.38 GB / 503.51 GB
Epoch: [19]  [ 30/893]  eta: 0:27:06  lr: 0.001703  min_lr: 0.000002  loss: 0.3416 (0.3170)  class_acc: 0.8571 (0.8635)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5051  data: 0.0005  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.84 GB / 503.51 GB
Epoch: [19]  [ 40/893]  eta: 0:25:28  lr: 0.001703  min_lr: 0.000002  loss: 0.3103 (0.3197)  class_acc: 0.8571 (0.8615)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5052  data: 0.0006  max mem: 31081
[2025-03-11 05:53:40,249] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 16989
[2025-03-11 05:53:40,249] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 05:53:40,249] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.32 GB / 503.51 GB
Epoch: [19]  [ 50/893]  eta: 0:24:22  lr: 0.001702  min_lr: 0.000002  loss: 0.3103 (0.3219)  class_acc: 0.8571 (0.8585)  loss_scale: 4096.0000 (7388.8627)  weight_decay: 0.0500 (0.0500)  time: 1.5026  data: 0.0005  max mem: 31081
[2025-03-11 05:53:55,216] [INFO] [logging.py:129:log_dist] [Rank 0] step=17000, skipped=100, lr=[2.2226578905482567e-06, 2.2226578905482567e-06, 3.704429817580428e-06, 3.704429817580428e-06, 6.174049695967381e-06, 6.174049695967381e-06, 1.0290082826612302e-05, 1.0290082826612302e-05, 1.7150138044353838e-05, 1.7150138044353838e-05, 2.8583563407256393e-05, 2.8583563407256393e-05, 4.763927234542733e-05, 4.763927234542733e-05, 7.939878724237889e-05, 7.939878724237889e-05, 0.00013233131207063148, 0.00013233131207063148, 0.00022055218678438584, 0.00022055218678438584, 0.00036758697797397637, 0.00036758697797397637, 0.0006126449632899607, 0.0006126449632899607, 0.0010210749388166011, 0.0010210749388166011, 0.0017017915646943352, 0.0017017915646943352], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 05:53:55,216] [INFO] [timer.py:264:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=61.005932155989946, CurrSamplesPerSec=62.54239043286704, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.39 GB / 503.51 GB
Epoch: [19]  [ 60/893]  eta: 0:23:27  lr: 0.001701  min_lr: 0.000002  loss: 0.2859 (0.3154)  class_acc: 0.8571 (0.8589)  loss_scale: 4096.0000 (6849.0492)  weight_decay: 0.0500 (0.0500)  time: 1.4798  data: 0.0003  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.19 GB / 503.51 GB
Epoch: [19]  [ 70/893]  eta: 0:22:44  lr: 0.001700  min_lr: 0.000002  loss: 0.3345 (0.3226)  class_acc: 0.8393 (0.8549)  loss_scale: 4096.0000 (6461.2958)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.19 GB / 503.51 GB
Epoch: [19]  [ 80/893]  eta: 0:22:09  lr: 0.001700  min_lr: 0.000002  loss: 0.3494 (0.3217)  class_acc: 0.8393 (0.8567)  loss_scale: 4096.0000 (6169.2840)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.10 GB / 503.51 GB
Epoch: [19]  [ 90/893]  eta: 0:21:37  lr: 0.001699  min_lr: 0.000002  loss: 0.2881 (0.3173)  class_acc: 0.8750 (0.8601)  loss_scale: 4096.0000 (5941.4505)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.69 GB / 503.51 GB
Epoch: [19]  [100/893]  eta: 0:21:08  lr: 0.001698  min_lr: 0.000002  loss: 0.2634 (0.3171)  class_acc: 0.8750 (0.8605)  loss_scale: 4096.0000 (5758.7327)  weight_decay: 0.0500 (0.0500)  time: 1.4585  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.23 GB / 503.51 GB
Epoch: [19]  [110/893]  eta: 0:20:43  lr: 0.001698  min_lr: 0.000002  loss: 0.3362 (0.3181)  class_acc: 0.8393 (0.8592)  loss_scale: 4096.0000 (5608.9369)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.0004  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.16 GB / 503.51 GB
Epoch: [19]  [120/893]  eta: 0:20:19  lr: 0.001697  min_lr: 0.000002  loss: 0.2859 (0.3120)  class_acc: 0.8571 (0.8613)  loss_scale: 4096.0000 (5483.9008)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0004  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.78 GB / 503.51 GB
Epoch: [19]  [130/893]  eta: 0:19:58  lr: 0.001696  min_lr: 0.000002  loss: 0.2966 (0.3172)  class_acc: 0.8571 (0.8571)  loss_scale: 4096.0000 (5377.9542)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.58 GB / 503.51 GB
Epoch: [19]  [140/893]  eta: 0:19:36  lr: 0.001695  min_lr: 0.000002  loss: 0.3877 (0.3200)  class_acc: 0.8214 (0.8566)  loss_scale: 4096.0000 (5287.0355)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0004  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.87 GB / 503.51 GB
Epoch: [19]  [150/893]  eta: 0:19:17  lr: 0.001695  min_lr: 0.000002  loss: 0.3247 (0.3217)  class_acc: 0.8571 (0.8569)  loss_scale: 4096.0000 (5208.1589)  weight_decay: 0.0500 (0.0500)  time: 1.4732  data: 0.0004  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.30 GB / 503.51 GB
Epoch: [19]  [160/893]  eta: 0:18:57  lr: 0.001694  min_lr: 0.000002  loss: 0.3406 (0.3246)  class_acc: 0.8571 (0.8560)  loss_scale: 4096.0000 (5139.0807)  weight_decay: 0.0500 (0.0500)  time: 1.4744  data: 0.0003  max mem: 31081
[2025-03-11 05:56:49,719] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 05:56:49,719] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.23 GB / 503.51 GB
Epoch: [19]  [170/893]  eta: 0:18:38  lr: 0.001693  min_lr: 0.000002  loss: 0.3674 (0.3268)  class_acc: 0.8393 (0.8547)  loss_scale: 4096.0000 (5102.0351)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.06 GB / 503.51 GB
Epoch: [19]  [180/893]  eta: 0:18:19  lr: 0.001693  min_lr: 0.000002  loss: 0.3159 (0.3256)  class_acc: 0.8571 (0.8560)  loss_scale: 8192.0000 (5272.7514)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.80 GB / 503.51 GB
Epoch: [19]  [190/893]  eta: 0:18:01  lr: 0.001692  min_lr: 0.000002  loss: 0.3125 (0.3253)  class_acc: 0.8571 (0.8566)  loss_scale: 8192.0000 (5425.5916)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0004  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.85 GB / 503.51 GB
Epoch: [19]  [200/893]  eta: 0:17:43  lr: 0.001691  min_lr: 0.000002  loss: 0.3391 (0.3263)  class_acc: 0.8571 (0.8556)  loss_scale: 8192.0000 (5563.2239)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.34 GB / 503.51 GB
Epoch: [19]  [210/893]  eta: 0:17:26  lr: 0.001690  min_lr: 0.000002  loss: 0.3074 (0.3248)  class_acc: 0.8571 (0.8566)  loss_scale: 8192.0000 (5687.8104)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0002  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.48 GB / 503.51 GB
Epoch: [19]  [220/893]  eta: 0:17:09  lr: 0.001690  min_lr: 0.000002  loss: 0.2981 (0.3247)  class_acc: 0.8571 (0.8565)  loss_scale: 8192.0000 (5801.1222)  weight_decay: 0.0500 (0.0500)  time: 1.4792  data: 0.0003  max mem: 31081
[2025-03-11 05:58:06,330] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 17170
[2025-03-11 05:58:06,330] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 05:58:06,330] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.44 GB / 503.51 GB
Epoch: [19]  [230/893]  eta: 0:16:53  lr: 0.001689  min_lr: 0.000002  loss: 0.2986 (0.3237)  class_acc: 0.8571 (0.8568)  loss_scale: 8192.0000 (5745.0390)  weight_decay: 0.0500 (0.0500)  time: 1.4884  data: 0.0004  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.77 GB / 503.51 GB
Epoch: [19]  [240/893]  eta: 0:16:36  lr: 0.001688  min_lr: 0.000002  loss: 0.3320 (0.3239)  class_acc: 0.8393 (0.8566)  loss_scale: 4096.0000 (5676.6141)  weight_decay: 0.0500 (0.0500)  time: 1.4805  data: 0.0004  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.01 GB / 503.51 GB
Epoch: [19]  [250/893]  eta: 0:16:19  lr: 0.001688  min_lr: 0.000002  loss: 0.3110 (0.3231)  class_acc: 0.8571 (0.8574)  loss_scale: 4096.0000 (5613.6414)  weight_decay: 0.0500 (0.0500)  time: 1.4747  data: 0.0004  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.17 GB / 503.51 GB
Epoch: [19]  [260/893]  eta: 0:16:03  lr: 0.001687  min_lr: 0.000002  loss: 0.2930 (0.3223)  class_acc: 0.8571 (0.8575)  loss_scale: 4096.0000 (5555.4943)  weight_decay: 0.0500 (0.0500)  time: 1.4688  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.51 GB / 503.51 GB
Epoch: [19]  [270/893]  eta: 0:15:46  lr: 0.001686  min_lr: 0.000002  loss: 0.3201 (0.3230)  class_acc: 0.8571 (0.8569)  loss_scale: 4096.0000 (5501.6384)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.23 GB / 503.51 GB
Epoch: [19]  [280/893]  eta: 0:15:30  lr: 0.001685  min_lr: 0.000002  loss: 0.3220 (0.3225)  class_acc: 0.8571 (0.8575)  loss_scale: 4096.0000 (5451.6157)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0004  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.50 GB / 503.51 GB
Epoch: [19]  [290/893]  eta: 0:15:13  lr: 0.001685  min_lr: 0.000002  loss: 0.2815 (0.3215)  class_acc: 0.8750 (0.8579)  loss_scale: 4096.0000 (5405.0309)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.64 GB / 503.51 GB
Epoch: [19]  [300/893]  eta: 0:14:57  lr: 0.001684  min_lr: 0.000002  loss: 0.2795 (0.3206)  class_acc: 0.8750 (0.8582)  loss_scale: 4096.0000 (5361.5415)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0003  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.02 GB / 503.51 GB
Epoch: [19]  [310/893]  eta: 0:14:41  lr: 0.001683  min_lr: 0.000002  loss: 0.2690 (0.3203)  class_acc: 0.8750 (0.8587)  loss_scale: 4096.0000 (5320.8489)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.95 GB / 503.51 GB
Epoch: [19]  [320/893]  eta: 0:14:25  lr: 0.001683  min_lr: 0.000002  loss: 0.2812 (0.3206)  class_acc: 0.8571 (0.8580)  loss_scale: 4096.0000 (5282.6916)  weight_decay: 0.0500 (0.0500)  time: 1.4663  data: 0.0002  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.89 GB / 503.51 GB
Epoch: [19]  [330/893]  eta: 0:14:10  lr: 0.001682  min_lr: 0.000002  loss: 0.2815 (0.3198)  class_acc: 0.8929 (0.8590)  loss_scale: 4096.0000 (5246.8399)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0002  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.13 GB / 503.51 GB
Epoch: [19]  [340/893]  eta: 0:13:54  lr: 0.001681  min_lr: 0.000002  loss: 0.3035 (0.3203)  class_acc: 0.8571 (0.8590)  loss_scale: 4096.0000 (5213.0909)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0002  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.27 GB / 503.51 GB
Epoch: [19]  [350/893]  eta: 0:13:38  lr: 0.001680  min_lr: 0.000002  loss: 0.3406 (0.3215)  class_acc: 0.8571 (0.8584)  loss_scale: 4096.0000 (5181.2650)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
[2025-03-11 06:01:15,803] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 06:01:15,803] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2025-03-11 06:01:26,174] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 17306
[2025-03-11 06:01:26,174] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 06:01:26,174] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.87 GB / 503.51 GB
Epoch: [19]  [360/893]  eta: 0:13:22  lr: 0.001680  min_lr: 0.000002  loss: 0.3508 (0.3219)  class_acc: 0.8571 (0.8584)  loss_scale: 4096.0000 (5230.6260)  weight_decay: 0.0500 (0.0500)  time: 1.4734  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.99 GB / 503.51 GB
Epoch: [19]  [370/893]  eta: 0:13:07  lr: 0.001679  min_lr: 0.000002  loss: 0.3176 (0.3216)  class_acc: 0.8571 (0.8587)  loss_scale: 4096.0000 (5200.0431)  weight_decay: 0.0500 (0.0500)  time: 1.4746  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.70 GB / 503.51 GB
Epoch: [19]  [380/893]  eta: 0:12:51  lr: 0.001678  min_lr: 0.000002  loss: 0.3071 (0.3229)  class_acc: 0.8571 (0.8585)  loss_scale: 4096.0000 (5171.0656)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.43 GB / 503.51 GB
Epoch: [19]  [390/893]  eta: 0:12:36  lr: 0.001677  min_lr: 0.000002  loss: 0.3169 (0.3228)  class_acc: 0.8393 (0.8583)  loss_scale: 4096.0000 (5143.5703)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0002  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.89 GB / 503.51 GB
Epoch: [19]  [400/893]  eta: 0:12:20  lr: 0.001677  min_lr: 0.000002  loss: 0.2974 (0.3227)  class_acc: 0.8571 (0.8583)  loss_scale: 4096.0000 (5117.4464)  weight_decay: 0.0500 (0.0500)  time: 1.4738  data: 0.0002  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.29 GB / 503.51 GB
Epoch: [19]  [410/893]  eta: 0:12:05  lr: 0.001676  min_lr: 0.000002  loss: 0.3440 (0.3235)  class_acc: 0.8571 (0.8581)  loss_scale: 4096.0000 (5092.5937)  weight_decay: 0.0500 (0.0500)  time: 1.4735  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.86 GB / 503.51 GB
Epoch: [19]  [420/893]  eta: 0:11:50  lr: 0.001675  min_lr: 0.000002  loss: 0.3743 (0.3243)  class_acc: 0.8214 (0.8575)  loss_scale: 4096.0000 (5068.9216)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.22 GB / 503.51 GB
Epoch: [19]  [430/893]  eta: 0:11:34  lr: 0.001675  min_lr: 0.000002  loss: 0.3179 (0.3243)  class_acc: 0.8571 (0.8573)  loss_scale: 4096.0000 (5046.3480)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0003  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.51 GB / 503.51 GB
Epoch: [19]  [440/893]  eta: 0:11:19  lr: 0.001674  min_lr: 0.000002  loss: 0.2974 (0.3242)  class_acc: 0.8750 (0.8578)  loss_scale: 4096.0000 (5024.7982)  weight_decay: 0.0500 (0.0500)  time: 1.4737  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.72 GB / 503.51 GB
Epoch: [19]  [450/893]  eta: 0:11:04  lr: 0.001673  min_lr: 0.000002  loss: 0.2742 (0.3231)  class_acc: 0.8750 (0.8585)  loss_scale: 4096.0000 (5004.2040)  weight_decay: 0.0500 (0.0500)  time: 1.4729  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.76 GB / 503.51 GB
Epoch: [19]  [460/893]  eta: 0:10:48  lr: 0.001672  min_lr: 0.000002  loss: 0.2993 (0.3234)  class_acc: 0.8571 (0.8583)  loss_scale: 4096.0000 (4984.5033)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 98.54 GB / 503.51 GB
Epoch: [19]  [470/893]  eta: 0:10:33  lr: 0.001672  min_lr: 0.000002  loss: 0.3423 (0.3236)  class_acc: 0.8393 (0.8580)  loss_scale: 4096.0000 (4965.6391)  weight_decay: 0.0500 (0.0500)  time: 1.4724  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.32 GB / 503.51 GB
Epoch: [19]  [480/893]  eta: 0:10:18  lr: 0.001671  min_lr: 0.000002  loss: 0.3428 (0.3241)  class_acc: 0.8393 (0.8576)  loss_scale: 4096.0000 (4947.5593)  weight_decay: 0.0500 (0.0500)  time: 1.4775  data: 0.0003  max mem: 31081
[2025-03-11 06:04:35,898] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 06:04:35,898] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.39 GB / 503.51 GB
Epoch: [19]  [490/893]  eta: 0:10:03  lr: 0.001670  min_lr: 0.000002  loss: 0.3167 (0.3236)  class_acc: 0.8571 (0.8579)  loss_scale: 4096.0000 (4963.5845)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0004  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.63 GB / 503.51 GB
Epoch: [19]  [500/893]  eta: 0:09:47  lr: 0.001669  min_lr: 0.000002  loss: 0.3008 (0.3235)  class_acc: 0.8571 (0.8575)  loss_scale: 8192.0000 (5028.0240)  weight_decay: 0.0500 (0.0500)  time: 1.4614  data: 0.0004  max mem: 31081
[2025-03-11 06:04:59,381] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 17451
[2025-03-11 06:04:59,381] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 06:04:59,381] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.07 GB / 503.51 GB
Epoch: [19]  [510/893]  eta: 0:09:32  lr: 0.001669  min_lr: 0.000002  loss: 0.2839 (0.3233)  class_acc: 0.8571 (0.8576)  loss_scale: 8192.0000 (5025.8160)  weight_decay: 0.0500 (0.0500)  time: 1.4619  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.07 GB / 503.51 GB
Epoch: [19]  [520/893]  eta: 0:09:17  lr: 0.001668  min_lr: 0.000002  loss: 0.2839 (0.3231)  class_acc: 0.8750 (0.8577)  loss_scale: 4096.0000 (5007.9693)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.10 GB / 503.51 GB
Epoch: [19]  [530/893]  eta: 0:09:02  lr: 0.001667  min_lr: 0.000002  loss: 0.3123 (0.3230)  class_acc: 0.8571 (0.8577)  loss_scale: 4096.0000 (4990.7947)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.38 GB / 503.51 GB
Epoch: [19]  [540/893]  eta: 0:08:47  lr: 0.001667  min_lr: 0.000002  loss: 0.3062 (0.3228)  class_acc: 0.8571 (0.8578)  loss_scale: 4096.0000 (4974.2551)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.07 GB / 503.51 GB
Epoch: [19]  [550/893]  eta: 0:08:32  lr: 0.001666  min_lr: 0.000002  loss: 0.3342 (0.3232)  class_acc: 0.8571 (0.8576)  loss_scale: 4096.0000 (4958.3158)  weight_decay: 0.0500 (0.0500)  time: 1.4695  data: 0.0002  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.31 GB / 503.51 GB
Epoch: [19]  [560/893]  eta: 0:08:17  lr: 0.001665  min_lr: 0.000002  loss: 0.3169 (0.3231)  class_acc: 0.8393 (0.8575)  loss_scale: 4096.0000 (4942.9447)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.79 GB / 503.51 GB
Epoch: [19]  [570/893]  eta: 0:08:02  lr: 0.001664  min_lr: 0.000002  loss: 0.3142 (0.3231)  class_acc: 0.8393 (0.8572)  loss_scale: 4096.0000 (4928.1121)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.27 GB / 503.51 GB
Epoch: [19]  [580/893]  eta: 0:07:46  lr: 0.001664  min_lr: 0.000002  loss: 0.3318 (0.3237)  class_acc: 0.8393 (0.8570)  loss_scale: 4096.0000 (4913.7900)  weight_decay: 0.0500 (0.0500)  time: 1.4612  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.75 GB / 503.51 GB
Epoch: [19]  [590/893]  eta: 0:07:31  lr: 0.001663  min_lr: 0.000002  loss: 0.3572 (0.3247)  class_acc: 0.8393 (0.8564)  loss_scale: 4096.0000 (4899.9526)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0004  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.73 GB / 503.51 GB
Epoch: [19]  [600/893]  eta: 0:07:16  lr: 0.001662  min_lr: 0.000002  loss: 0.3313 (0.3243)  class_acc: 0.8571 (0.8568)  loss_scale: 4096.0000 (4886.5757)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0004  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.35 GB / 503.51 GB
Epoch: [19]  [610/893]  eta: 0:07:01  lr: 0.001661  min_lr: 0.000002  loss: 0.2991 (0.3245)  class_acc: 0.8571 (0.8566)  loss_scale: 4096.0000 (4873.6367)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.20 GB / 503.51 GB
Epoch: [19]  [620/893]  eta: 0:06:46  lr: 0.001661  min_lr: 0.000002  loss: 0.2842 (0.3240)  class_acc: 0.8571 (0.8569)  loss_scale: 4096.0000 (4861.1143)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.10 GB / 503.51 GB
Epoch: [19]  [630/893]  eta: 0:06:31  lr: 0.001660  min_lr: 0.000002  loss: 0.2986 (0.3244)  class_acc: 0.8571 (0.8568)  loss_scale: 4096.0000 (4848.9889)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0003  max mem: 31081
[2025-03-11 06:08:08,351] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 06:08:08,352] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.66 GB / 503.51 GB
Epoch: [19]  [640/893]  eta: 0:06:16  lr: 0.001659  min_lr: 0.000002  loss: 0.3140 (0.3241)  class_acc: 0.8750 (0.8569)  loss_scale: 4096.0000 (4894.7520)  weight_decay: 0.0500 (0.0500)  time: 1.4754  data: 0.0003  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.93 GB / 503.51 GB
Epoch: [19]  [650/893]  eta: 0:06:01  lr: 0.001659  min_lr: 0.000002  loss: 0.3049 (0.3241)  class_acc: 0.8750 (0.8569)  loss_scale: 8192.0000 (4945.4009)  weight_decay: 0.0500 (0.0500)  time: 1.4715  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.24 GB / 503.51 GB
Epoch: [19]  [660/893]  eta: 0:05:46  lr: 0.001658  min_lr: 0.000002  loss: 0.3086 (0.3238)  class_acc: 0.8571 (0.8572)  loss_scale: 8192.0000 (4994.5174)  weight_decay: 0.0500 (0.0500)  time: 1.4725  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.07 GB / 503.51 GB
Epoch: [19]  [670/893]  eta: 0:05:31  lr: 0.001657  min_lr: 0.000002  loss: 0.2866 (0.3233)  class_acc: 0.8750 (0.8575)  loss_scale: 8192.0000 (5042.1699)  weight_decay: 0.0500 (0.0500)  time: 1.4729  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.57 GB / 503.51 GB
Epoch: [19]  [680/893]  eta: 0:05:17  lr: 0.001656  min_lr: 0.000002  loss: 0.3337 (0.3242)  class_acc: 0.8571 (0.8572)  loss_scale: 8192.0000 (5088.4229)  weight_decay: 0.0500 (0.0500)  time: 1.4699  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.74 GB / 503.51 GB
Epoch: [19]  [690/893]  eta: 0:05:02  lr: 0.001656  min_lr: 0.000002  loss: 0.3613 (0.3245)  class_acc: 0.8393 (0.8571)  loss_scale: 8192.0000 (5133.3372)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0002  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.14 GB / 503.51 GB
Epoch: [19]  [700/893]  eta: 0:04:47  lr: 0.001655  min_lr: 0.000002  loss: 0.3120 (0.3245)  class_acc: 0.8571 (0.8570)  loss_scale: 8192.0000 (5176.9700)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
[2025-03-11 06:09:51,265] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 17650
[2025-03-11 06:09:51,265] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 06:09:51,265] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.36 GB / 503.51 GB
Epoch: [19]  [710/893]  eta: 0:04:32  lr: 0.001654  min_lr: 0.000002  loss: 0.3069 (0.3245)  class_acc: 0.8393 (0.8570)  loss_scale: 8192.0000 (5167.5274)  weight_decay: 0.0500 (0.0500)  time: 1.4733  data: 0.0003  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.22 GB / 503.51 GB
Epoch: [19]  [720/893]  eta: 0:04:17  lr: 0.001653  min_lr: 0.000002  loss: 0.3491 (0.3250)  class_acc: 0.8393 (0.8567)  loss_scale: 4096.0000 (5152.6657)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.35 GB / 503.51 GB
Epoch: [19]  [730/893]  eta: 0:04:02  lr: 0.001653  min_lr: 0.000002  loss: 0.3594 (0.3254)  class_acc: 0.8393 (0.8567)  loss_scale: 4096.0000 (5138.2107)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.87 GB / 503.51 GB
Epoch: [19]  [740/893]  eta: 0:03:47  lr: 0.001652  min_lr: 0.000002  loss: 0.2998 (0.3249)  class_acc: 0.8750 (0.8570)  loss_scale: 4096.0000 (5124.1457)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.51 GB / 503.51 GB
Epoch: [19]  [750/893]  eta: 0:03:32  lr: 0.001651  min_lr: 0.000002  loss: 0.2974 (0.3250)  class_acc: 0.8750 (0.8569)  loss_scale: 4096.0000 (5110.4554)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.55 GB / 503.51 GB
Epoch: [19]  [760/893]  eta: 0:03:17  lr: 0.001650  min_lr: 0.000002  loss: 0.3206 (0.3246)  class_acc: 0.8750 (0.8571)  loss_scale: 4096.0000 (5097.1248)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.75 GB / 503.51 GB
Epoch: [19]  [770/893]  eta: 0:03:02  lr: 0.001650  min_lr: 0.000002  loss: 0.2949 (0.3245)  class_acc: 0.8750 (0.8571)  loss_scale: 4096.0000 (5084.1401)  weight_decay: 0.0500 (0.0500)  time: 1.4812  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.99 GB / 503.51 GB
Epoch: [19]  [780/893]  eta: 0:02:47  lr: 0.001649  min_lr: 0.000002  loss: 0.3042 (0.3247)  class_acc: 0.8393 (0.8570)  loss_scale: 4096.0000 (5071.4878)  weight_decay: 0.0500 (0.0500)  time: 1.4784  data: 0.0004  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.39 GB / 503.51 GB
Epoch: [19]  [790/893]  eta: 0:02:33  lr: 0.001648  min_lr: 0.000002  loss: 0.3145 (0.3248)  class_acc: 0.8571 (0.8570)  loss_scale: 4096.0000 (5059.1555)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.02 GB / 503.51 GB
Epoch: [19]  [800/893]  eta: 0:02:18  lr: 0.001647  min_lr: 0.000002  loss: 0.3132 (0.3247)  class_acc: 0.8571 (0.8570)  loss_scale: 4096.0000 (5047.1311)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.46 GB / 503.51 GB
Epoch: [19]  [810/893]  eta: 0:02:03  lr: 0.001647  min_lr: 0.000002  loss: 0.2942 (0.3246)  class_acc: 0.8750 (0.8572)  loss_scale: 4096.0000 (5035.4032)  weight_decay: 0.0500 (0.0500)  time: 1.4716  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.65 GB / 503.51 GB
Epoch: [19]  [820/893]  eta: 0:01:48  lr: 0.001646  min_lr: 0.000002  loss: 0.2981 (0.3246)  class_acc: 0.8750 (0.8571)  loss_scale: 4096.0000 (5023.9610)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.70 GB / 503.51 GB
Epoch: [19]  [830/893]  eta: 0:01:33  lr: 0.001645  min_lr: 0.000002  loss: 0.3152 (0.3245)  class_acc: 0.8571 (0.8572)  loss_scale: 4096.0000 (5012.7942)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0003  max mem: 31081
[2025-03-11 06:13:00,787] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 06:13:00,787] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2025-03-11 06:13:09,641] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 17785
[2025-03-11 06:13:09,641] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 06:13:09,641] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.90 GB / 503.51 GB
Epoch: [19]  [840/893]  eta: 0:01:18  lr: 0.001645  min_lr: 0.000002  loss: 0.3145 (0.3245)  class_acc: 0.8571 (0.8573)  loss_scale: 4096.0000 (5031.1153)  weight_decay: 0.0500 (0.0500)  time: 1.4685  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.98 GB / 503.51 GB
Epoch: [19]  [850/893]  eta: 0:01:03  lr: 0.001644  min_lr: 0.000002  loss: 0.3215 (0.3245)  class_acc: 0.8571 (0.8573)  loss_scale: 4096.0000 (5020.1269)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.68 GB / 503.51 GB
Epoch: [19]  [860/893]  eta: 0:00:48  lr: 0.001643  min_lr: 0.000002  loss: 0.3281 (0.3248)  class_acc: 0.8393 (0.8572)  loss_scale: 4096.0000 (5009.3937)  weight_decay: 0.0500 (0.0500)  time: 1.4567  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.84 GB / 503.51 GB
Epoch: [19]  [870/893]  eta: 0:00:34  lr: 0.001642  min_lr: 0.000002  loss: 0.3179 (0.3246)  class_acc: 0.8571 (0.8573)  loss_scale: 4096.0000 (4998.9070)  weight_decay: 0.0500 (0.0500)  time: 1.4469  data: 0.0002  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.51 GB / 503.51 GB
Epoch: [19]  [880/893]  eta: 0:00:19  lr: 0.001642  min_lr: 0.000002  loss: 0.3171 (0.3244)  class_acc: 0.8750 (0.8574)  loss_scale: 4096.0000 (4988.6583)  weight_decay: 0.0500 (0.0500)  time: 1.4405  data: 0.0002  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.64 GB / 503.51 GB
Epoch: [19]  [890/893]  eta: 0:00:04  lr: 0.001641  min_lr: 0.000002  loss: 0.3040 (0.3241)  class_acc: 0.8750 (0.8575)  loss_scale: 4096.0000 (4978.6397)  weight_decay: 0.0500 (0.0500)  time: 1.4426  data: 0.0002  max mem: 31081
Epoch: [19]  [892/893]  eta: 0:00:01  lr: 0.001641  min_lr: 0.000002  loss: 0.3062 (0.3242)  class_acc: 0.8571 (0.8575)  loss_scale: 4096.0000 (4977.6502)  weight_decay: 0.0500 (0.0500)  time: 1.3913  data: 0.0002  max mem: 31081
Epoch: [19] Total time: 0:22:03 (1.4817 s / it)
Averaged stats: lr: 0.001641  min_lr: 0.000002  loss: 0.3062 (0.3242)  class_acc: 0.8571 (0.8575)  loss_scale: 4096.0000 (4977.6502)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:16:43  loss: 0.6693 (0.6693)  acc: 75.0000 (75.0000)  time: 11.2692  data: 10.7358  max mem: 31081
Val:  [ 10/728]  eta: 0:18:31  loss: 0.3809 (0.4374)  acc: 83.3333 (82.6840)  time: 1.5487  data: 1.0269  max mem: 31081
Val:  [ 20/728]  eta: 0:14:14  loss: 0.3863 (0.4624)  acc: 83.3333 (80.8390)  time: 0.7038  data: 0.1802  max mem: 31081
Val:  [ 30/728]  eta: 0:12:42  loss: 0.3916 (0.4576)  acc: 79.7619 (79.3395)  time: 0.8422  data: 0.3176  max mem: 31081
Val:  [ 40/728]  eta: 0:11:53  loss: 0.4271 (0.4864)  acc: 79.7619 (78.3972)  time: 0.8585  data: 0.3370  max mem: 31081
Val:  [ 50/728]  eta: 0:11:16  loss: 0.4289 (0.4774)  acc: 76.1905 (78.9216)  time: 0.8507  data: 0.3304  max mem: 31081
Val:  [ 60/728]  eta: 0:10:22  loss: 0.5751 (0.4988)  acc: 75.0000 (78.4153)  time: 0.7150  data: 0.1949  max mem: 31081
Val:  [ 70/728]  eta: 0:09:52  loss: 0.5392 (0.5081)  acc: 76.1905 (77.8672)  time: 0.6503  data: 0.1291  max mem: 31081
Val:  [ 80/728]  eta: 0:09:38  loss: 0.5294 (0.5175)  acc: 76.1905 (77.4691)  time: 0.7745  data: 0.2523  max mem: 31081
Val:  [ 90/728]  eta: 0:09:24  loss: 0.5584 (0.5411)  acc: 73.8095 (76.8184)  time: 0.8284  data: 0.3059  max mem: 31081
Val:  [100/728]  eta: 0:09:12  loss: 0.4508 (0.5265)  acc: 77.3810 (77.4988)  time: 0.8240  data: 0.3034  max mem: 31081
Val:  [110/728]  eta: 0:09:02  loss: 0.3989 (0.5258)  acc: 78.5714 (77.5097)  time: 0.8516  data: 0.3317  max mem: 31081
Val:  [120/728]  eta: 0:08:45  loss: 0.3617 (0.5140)  acc: 82.1429 (78.0500)  time: 0.7875  data: 0.2671  max mem: 31081
Val:  [130/728]  eta: 0:08:26  loss: 0.3555 (0.5214)  acc: 83.3333 (78.0443)  time: 0.6742  data: 0.1537  max mem: 31081
Val:  [140/728]  eta: 0:08:15  loss: 0.4636 (0.5309)  acc: 77.3810 (77.7018)  time: 0.7122  data: 0.1908  max mem: 31081
Val:  [150/728]  eta: 0:08:02  loss: 0.4636 (0.5290)  acc: 76.1905 (77.8382)  time: 0.7542  data: 0.2315  max mem: 31081
Val:  [160/728]  eta: 0:07:54  loss: 0.4435 (0.5321)  acc: 73.8095 (77.7137)  time: 0.7849  data: 0.2561  max mem: 31081
Val:  [170/728]  eta: 0:07:45  loss: 0.4798 (0.5283)  acc: 77.3810 (77.7360)  time: 0.8236  data: 0.2973  max mem: 31081
Val:  [180/728]  eta: 0:07:31  loss: 0.4053 (0.5286)  acc: 79.7619 (77.7624)  time: 0.7404  data: 0.2194  max mem: 31081
Val:  [190/728]  eta: 0:07:20  loss: 0.3942 (0.5243)  acc: 79.7619 (77.8173)  time: 0.6967  data: 0.1755  max mem: 31081
Val:  [200/728]  eta: 0:07:11  loss: 0.4182 (0.5181)  acc: 79.7619 (78.0384)  time: 0.7506  data: 0.2280  max mem: 31081
Val:  [210/728]  eta: 0:07:05  loss: 0.4182 (0.5210)  acc: 79.7619 (78.0580)  time: 0.8520  data: 0.3264  max mem: 31081
Val:  [220/728]  eta: 0:06:59  loss: 0.3994 (0.5183)  acc: 80.9524 (78.1782)  time: 0.9035  data: 0.3806  max mem: 31081
Val:  [230/728]  eta: 0:06:49  loss: 0.3668 (0.5235)  acc: 80.9524 (78.0509)  time: 0.8288  data: 0.3058  max mem: 31081
Val:  [240/728]  eta: 0:06:35  loss: 0.4200 (0.5226)  acc: 80.9524 (78.1861)  time: 0.6542  data: 0.1286  max mem: 31081
Val:  [250/728]  eta: 0:06:27  loss: 0.4355 (0.5228)  acc: 79.7619 (78.2299)  time: 0.6695  data: 0.1440  max mem: 31081
Val:  [260/728]  eta: 0:06:19  loss: 0.3841 (0.5220)  acc: 78.5714 (78.3023)  time: 0.8219  data: 0.2993  max mem: 31081
Val:  [270/728]  eta: 0:06:12  loss: 0.3830 (0.5211)  acc: 80.9524 (78.2507)  time: 0.8354  data: 0.3152  max mem: 31081
Val:  [280/728]  eta: 0:06:06  loss: 0.5037 (0.5226)  acc: 76.1905 (78.2876)  time: 0.9141  data: 0.3923  max mem: 31081
Val:  [290/728]  eta: 0:05:58  loss: 0.4126 (0.5192)  acc: 80.9524 (78.4078)  time: 0.9046  data: 0.3820  max mem: 31081
Val:  [300/728]  eta: 0:05:46  loss: 0.3865 (0.5171)  acc: 80.9524 (78.4607)  time: 0.6685  data: 0.1444  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.4549 (0.5200)  acc: 76.1905 (78.3494)  time: 0.6663  data: 0.1413  max mem: 31081
Val:  [320/728]  eta: 0:05:29  loss: 0.4362 (0.5195)  acc: 79.7619 (78.4045)  time: 0.8008  data: 0.2766  max mem: 31081
Val:  [330/728]  eta: 0:05:22  loss: 0.3405 (0.5163)  acc: 83.3333 (78.4995)  time: 0.8371  data: 0.3145  max mem: 31081
Val:  [340/728]  eta: 0:05:15  loss: 0.3153 (0.5118)  acc: 83.3333 (78.6413)  time: 0.8917  data: 0.3701  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3399 (0.5118)  acc: 83.3333 (78.6596)  time: 0.8483  data: 0.3226  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.3941 (0.5087)  acc: 80.9524 (78.7660)  time: 0.6587  data: 0.1318  max mem: 31081
Val:  [370/728]  eta: 0:04:48  loss: 0.4210 (0.5124)  acc: 80.9524 (78.6324)  time: 0.6965  data: 0.1730  max mem: 31081
Val:  [380/728]  eta: 0:04:40  loss: 0.5177 (0.5141)  acc: 77.3810 (78.6183)  time: 0.8438  data: 0.3190  max mem: 31081
Val:  [390/728]  eta: 0:04:32  loss: 0.3497 (0.5106)  acc: 80.9524 (78.7267)  time: 0.8277  data: 0.3020  max mem: 31081
Val:  [400/728]  eta: 0:04:25  loss: 0.3315 (0.5097)  acc: 84.5238 (78.8416)  time: 0.8667  data: 0.3428  max mem: 31081
Val:  [410/728]  eta: 0:04:17  loss: 0.3622 (0.5091)  acc: 80.9524 (78.8843)  time: 0.8743  data: 0.3528  max mem: 31081
Val:  [420/728]  eta: 0:04:07  loss: 0.3813 (0.5071)  acc: 82.1429 (78.9871)  time: 0.6848  data: 0.1660  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.4557 (0.5067)  acc: 82.1429 (78.9913)  time: 0.6977  data: 0.1743  max mem: 31081
Val:  [440/728]  eta: 0:03:51  loss: 0.4425 (0.5059)  acc: 80.9524 (78.9521)  time: 0.8383  data: 0.3140  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4107 (0.5084)  acc: 80.9524 (78.8486)  time: 0.8222  data: 0.3003  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.6282 (0.5122)  acc: 79.7619 (78.7135)  time: 0.8255  data: 0.3046  max mem: 31081
Val:  [470/728]  eta: 0:03:27  loss: 0.4628 (0.5102)  acc: 79.7619 (78.7837)  time: 0.7927  data: 0.2732  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.4025 (0.5092)  acc: 83.3333 (78.8140)  time: 0.6514  data: 0.1293  max mem: 31081
Val:  [490/728]  eta: 0:03:10  loss: 0.5372 (0.5097)  acc: 76.1905 (78.7484)  time: 0.6942  data: 0.1723  max mem: 31081
Val:  [500/728]  eta: 0:03:02  loss: 0.4697 (0.5098)  acc: 78.5714 (78.7615)  time: 0.8446  data: 0.3234  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3797 (0.5074)  acc: 82.1429 (78.8580)  time: 0.8692  data: 0.3447  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.3988 (0.5096)  acc: 77.3810 (78.7611)  time: 0.8517  data: 0.3262  max mem: 31081
Val:  [530/728]  eta: 0:02:38  loss: 0.5966 (0.5098)  acc: 76.1905 (78.7754)  time: 0.7404  data: 0.2176  max mem: 31081
Val:  [540/728]  eta: 0:02:29  loss: 0.4966 (0.5121)  acc: 77.3810 (78.6683)  time: 0.6073  data: 0.0850  max mem: 31081
Val:  [550/728]  eta: 0:02:21  loss: 0.5170 (0.5126)  acc: 72.6190 (78.6276)  time: 0.6727  data: 0.1497  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.4710 (0.5123)  acc: 77.3810 (78.6712)  time: 0.7600  data: 0.2382  max mem: 31081
Val:  [570/728]  eta: 0:02:05  loss: 0.4348 (0.5164)  acc: 82.1429 (78.5964)  time: 0.7974  data: 0.2754  max mem: 31081
Val:  [580/728]  eta: 0:01:57  loss: 0.4054 (0.5162)  acc: 82.1429 (78.6349)  time: 0.8426  data: 0.3200  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.4092 (0.5168)  acc: 82.1429 (78.5714)  time: 0.7391  data: 0.2174  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4598 (0.5179)  acc: 79.7619 (78.5140)  time: 0.6323  data: 0.1091  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.4643 (0.5199)  acc: 75.0000 (78.4234)  time: 0.6661  data: 0.1428  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.5555 (0.5232)  acc: 75.0000 (78.3951)  time: 0.8102  data: 0.2859  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4682 (0.5213)  acc: 79.7619 (78.4563)  time: 0.8390  data: 0.3094  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.4160 (0.5203)  acc: 80.9524 (78.4990)  time: 0.7954  data: 0.2684  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.3984 (0.5198)  acc: 82.1429 (78.5056)  time: 0.8026  data: 0.2816  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.3984 (0.5187)  acc: 82.1429 (78.5426)  time: 0.7339  data: 0.2119  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4746 (0.5191)  acc: 82.1429 (78.5377)  time: 0.6769  data: 0.1538  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.3706 (0.5176)  acc: 82.1429 (78.6116)  time: 0.7515  data: 0.2302  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3613 (0.5164)  acc: 80.9524 (78.6490)  time: 0.8459  data: 0.3249  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3662 (0.5155)  acc: 80.9524 (78.6682)  time: 0.8443  data: 0.3200  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.3735 (0.5167)  acc: 80.9524 (78.6635)  time: 0.8001  data: 0.2750  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4463 (0.5155)  acc: 82.1429 (78.7068)  time: 0.7424  data: 0.2274  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4463 (0.5170)  acc: 82.1429 (78.6647)  time: 0.7236  data: 0.2333  max mem: 31081
Val: Total time: 0:09:32 (0.7865 s / it)
* Acc@1 78.665 AP 0.7942898869514465 loss 0.517
Accuracy of the network on the 61096 val videos: 78.7%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.85 GB / 503.51 GB
Epoch: [20]  [  0/893]  eta: 3:10:56  lr: 0.001641  min_lr: 0.000002  loss: 0.3237 (0.3237)  class_acc: 0.9107 (0.9107)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 12.8287  data: 11.5459  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.26 GB / 503.51 GB
Epoch: [20]  [ 10/893]  eta: 0:37:02  lr: 0.001640  min_lr: 0.000002  loss: 0.3445 (0.3446)  class_acc: 0.8750 (0.8718)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5168  data: 1.0499  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.50 GB / 503.51 GB
Epoch: [20]  [ 20/893]  eta: 0:29:36  lr: 0.001639  min_lr: 0.000002  loss: 0.3035 (0.3117)  class_acc: 0.8750 (0.8733)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4957  data: 0.0004  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.98 GB / 503.51 GB
Epoch: [20]  [ 30/893]  eta: 0:26:48  lr: 0.001638  min_lr: 0.000002  loss: 0.3035 (0.3179)  class_acc: 0.8750 (0.8652)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5053  data: 0.0004  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.52 GB / 503.51 GB
Epoch: [20]  [ 40/893]  eta: 0:25:16  lr: 0.001638  min_lr: 0.000002  loss: 0.3174 (0.3168)  class_acc: 0.8393 (0.8650)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5067  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.54 GB / 503.51 GB
Epoch: [20]  [ 50/893]  eta: 0:24:11  lr: 0.001637  min_lr: 0.000002  loss: 0.2988 (0.3102)  class_acc: 0.8571 (0.8676)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5018  data: 0.0007  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.99 GB / 503.51 GB
Epoch: [20]  [ 60/893]  eta: 0:23:19  lr: 0.001636  min_lr: 0.000002  loss: 0.2712 (0.3075)  class_acc: 0.8750 (0.8700)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4814  data: 0.0006  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.35 GB / 503.51 GB
Epoch: [20]  [ 70/893]  eta: 0:22:37  lr: 0.001635  min_lr: 0.000002  loss: 0.3196 (0.3122)  class_acc: 0.8571 (0.8662)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
[2025-03-11 06:26:05,309] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 06:26:05,309] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.04 GB / 503.51 GB
Epoch: [20]  [ 80/893]  eta: 0:22:02  lr: 0.001635  min_lr: 0.000002  loss: 0.3083 (0.3125)  class_acc: 0.8571 (0.8666)  loss_scale: 4096.0000 (4449.9753)  weight_decay: 0.0500 (0.0500)  time: 1.4606  data: 0.0002  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.18 GB / 503.51 GB
Epoch: [20]  [ 90/893]  eta: 0:21:32  lr: 0.001634  min_lr: 0.000002  loss: 0.2986 (0.3115)  class_acc: 0.8571 (0.8668)  loss_scale: 8192.0000 (4861.1868)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0002  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.29 GB / 503.51 GB
Epoch: [20]  [100/893]  eta: 0:21:04  lr: 0.001633  min_lr: 0.000002  loss: 0.2998 (0.3112)  class_acc: 0.8750 (0.8678)  loss_scale: 8192.0000 (5190.9703)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0002  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.67 GB / 503.51 GB
Epoch: [20]  [110/893]  eta: 0:20:39  lr: 0.001633  min_lr: 0.000002  loss: 0.3201 (0.3119)  class_acc: 0.8571 (0.8663)  loss_scale: 8192.0000 (5461.3333)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0002  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.63 GB / 503.51 GB
Epoch: [20]  [120/893]  eta: 0:20:15  lr: 0.001632  min_lr: 0.000002  loss: 0.3384 (0.3153)  class_acc: 0.8393 (0.8648)  loss_scale: 8192.0000 (5687.0083)  weight_decay: 0.0500 (0.0500)  time: 1.4614  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.36 GB / 503.51 GB
Epoch: [20]  [130/893]  eta: 0:19:54  lr: 0.001631  min_lr: 0.000002  loss: 0.3220 (0.3143)  class_acc: 0.8571 (0.8652)  loss_scale: 8192.0000 (5878.2290)  weight_decay: 0.0500 (0.0500)  time: 1.4677  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.05 GB / 503.51 GB
Epoch: [20]  [140/893]  eta: 0:19:32  lr: 0.001630  min_lr: 0.000002  loss: 0.2932 (0.3145)  class_acc: 0.8571 (0.8649)  loss_scale: 8192.0000 (6042.3262)  weight_decay: 0.0500 (0.0500)  time: 1.4663  data: 0.0002  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.67 GB / 503.51 GB
Epoch: [20]  [150/893]  eta: 0:19:13  lr: 0.001630  min_lr: 0.000002  loss: 0.2883 (0.3150)  class_acc: 0.8571 (0.8645)  loss_scale: 8192.0000 (6184.6887)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0003  max mem: 31081
[2025-03-11 06:27:58,129] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 17991
[2025-03-11 06:27:58,129] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 06:27:58,129] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2025-03-11 06:28:09,911] [INFO] [logging.py:129:log_dist] [Rank 0] step=18000, skipped=106, lr=[2.127393002557302e-06, 2.127393002557302e-06, 3.5456550042621703e-06, 3.5456550042621703e-06, 5.909425007103618e-06, 5.909425007103618e-06, 9.84904167850603e-06, 9.84904167850603e-06, 1.6415069464176718e-05, 1.6415069464176718e-05, 2.7358449106961192e-05, 2.7358449106961192e-05, 4.559741517826866e-05, 4.559741517826866e-05, 7.599569196378111e-05, 7.599569196378111e-05, 0.00012665948660630182, 0.00012665948660630182, 0.00021109914434383644, 0.00021109914434383644, 0.00035183190723972734, 0.00035183190723972734, 0.0005863865120662123, 0.0005863865120662123, 0.0009773108534436871, 0.0009773108534436871, 0.0016288514224061454, 0.0016288514224061454], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 06:28:09,912] [INFO] [timer.py:264:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=61.00086376447387, CurrSamplesPerSec=59.00375140841334, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.87 GB / 503.51 GB
Epoch: [20]  [160/893]  eta: 0:18:53  lr: 0.001629  min_lr: 0.000002  loss: 0.3113 (0.3149)  class_acc: 0.8750 (0.8648)  loss_scale: 4096.0000 (6054.9565)  weight_decay: 0.0500 (0.0500)  time: 1.4713  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.07 GB / 503.51 GB
Epoch: [20]  [170/893]  eta: 0:18:34  lr: 0.001628  min_lr: 0.000002  loss: 0.3308 (0.3167)  class_acc: 0.8571 (0.8646)  loss_scale: 4096.0000 (5940.3977)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0002  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.01 GB / 503.51 GB
Epoch: [20]  [180/893]  eta: 0:18:16  lr: 0.001627  min_lr: 0.000002  loss: 0.3464 (0.3167)  class_acc: 0.8393 (0.8639)  loss_scale: 4096.0000 (5838.4972)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0002  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.18 GB / 503.51 GB
Epoch: [20]  [190/893]  eta: 0:17:58  lr: 0.001627  min_lr: 0.000002  loss: 0.3286 (0.3169)  class_acc: 0.8571 (0.8642)  loss_scale: 4096.0000 (5747.2670)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0003  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.01 GB / 503.51 GB
Epoch: [20]  [200/893]  eta: 0:17:40  lr: 0.001626  min_lr: 0.000002  loss: 0.3142 (0.3160)  class_acc: 0.8750 (0.8646)  loss_scale: 4096.0000 (5665.1144)  weight_decay: 0.0500 (0.0500)  time: 1.4714  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.50 GB / 503.51 GB
Epoch: [20]  [210/893]  eta: 0:17:23  lr: 0.001625  min_lr: 0.000002  loss: 0.3049 (0.3156)  class_acc: 0.8750 (0.8645)  loss_scale: 4096.0000 (5590.7488)  weight_decay: 0.0500 (0.0500)  time: 1.4673  data: 0.0002  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.40 GB / 503.51 GB
Epoch: [20]  [220/893]  eta: 0:17:06  lr: 0.001624  min_lr: 0.000002  loss: 0.2983 (0.3143)  class_acc: 0.8750 (0.8655)  loss_scale: 4096.0000 (5523.1131)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0002  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.55 GB / 503.51 GB
Epoch: [20]  [230/893]  eta: 0:16:49  lr: 0.001624  min_lr: 0.000002  loss: 0.3308 (0.3170)  class_acc: 0.8571 (0.8648)  loss_scale: 4096.0000 (5461.3333)  weight_decay: 0.0500 (0.0500)  time: 1.4751  data: 0.0002  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.42 GB / 503.51 GB
Epoch: [20]  [240/893]  eta: 0:16:33  lr: 0.001623  min_lr: 0.000002  loss: 0.3379 (0.3171)  class_acc: 0.8393 (0.8637)  loss_scale: 4096.0000 (5404.6805)  weight_decay: 0.0500 (0.0500)  time: 1.4737  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.29 GB / 503.51 GB
Epoch: [20]  [250/893]  eta: 0:16:17  lr: 0.001622  min_lr: 0.000002  loss: 0.3247 (0.3161)  class_acc: 0.8393 (0.8635)  loss_scale: 4096.0000 (5352.5418)  weight_decay: 0.0500 (0.0500)  time: 1.4775  data: 0.0002  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.95 GB / 503.51 GB
Epoch: [20]  [260/893]  eta: 0:16:01  lr: 0.001621  min_lr: 0.000002  loss: 0.3120 (0.3162)  class_acc: 0.8571 (0.8636)  loss_scale: 4096.0000 (5304.3985)  weight_decay: 0.0500 (0.0500)  time: 1.4836  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.90 GB / 503.51 GB
Epoch: [20]  [270/893]  eta: 0:15:45  lr: 0.001621  min_lr: 0.000002  loss: 0.3022 (0.3166)  class_acc: 0.8571 (0.8631)  loss_scale: 4096.0000 (5259.8081)  weight_decay: 0.0500 (0.0500)  time: 1.4859  data: 0.0003  max mem: 31081
[2025-03-11 06:31:08,301] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 06:31:08,301] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.18 GB / 503.51 GB
Epoch: [20]  [280/893]  eta: 0:15:29  lr: 0.001620  min_lr: 0.000002  loss: 0.3208 (0.3168)  class_acc: 0.8750 (0.8636)  loss_scale: 4096.0000 (5232.9680)  weight_decay: 0.0500 (0.0500)  time: 1.4845  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.71 GB / 503.51 GB
Epoch: [20]  [290/893]  eta: 0:15:12  lr: 0.001619  min_lr: 0.000002  loss: 0.3396 (0.3175)  class_acc: 0.8750 (0.8636)  loss_scale: 8192.0000 (5334.6529)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.30 GB / 503.51 GB
Epoch: [20]  [300/893]  eta: 0:14:56  lr: 0.001618  min_lr: 0.000002  loss: 0.3459 (0.3189)  class_acc: 0.8571 (0.8634)  loss_scale: 8192.0000 (5429.5814)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0002  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.57 GB / 503.51 GB
Epoch: [20]  [310/893]  eta: 0:14:40  lr: 0.001618  min_lr: 0.000002  loss: 0.3315 (0.3192)  class_acc: 0.8571 (0.8633)  loss_scale: 8192.0000 (5518.4051)  weight_decay: 0.0500 (0.0500)  time: 1.4712  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.33 GB / 503.51 GB
Epoch: [20]  [320/893]  eta: 0:14:24  lr: 0.001617  min_lr: 0.000002  loss: 0.3274 (0.3202)  class_acc: 0.8571 (0.8625)  loss_scale: 8192.0000 (5601.6947)  weight_decay: 0.0500 (0.0500)  time: 1.4681  data: 0.0003  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.93 GB / 503.51 GB
Epoch: [20]  [330/893]  eta: 0:14:09  lr: 0.001616  min_lr: 0.000002  loss: 0.3162 (0.3200)  class_acc: 0.8571 (0.8625)  loss_scale: 8192.0000 (5679.9517)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0002  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.08 GB / 503.51 GB
Epoch: [20]  [340/893]  eta: 0:13:53  lr: 0.001615  min_lr: 0.000002  loss: 0.2913 (0.3201)  class_acc: 0.8750 (0.8625)  loss_scale: 8192.0000 (5753.6188)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
[2025-03-11 06:32:46,522] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 18187
[2025-03-11 06:32:46,522] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 06:32:46,522] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.57 GB / 503.51 GB
Epoch: [20]  [350/893]  eta: 0:13:37  lr: 0.001615  min_lr: 0.000002  loss: 0.3142 (0.3203)  class_acc: 0.8750 (0.8625)  loss_scale: 8192.0000 (5776.4103)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0004  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.12 GB / 503.51 GB
Epoch: [20]  [360/893]  eta: 0:13:21  lr: 0.001614  min_lr: 0.000002  loss: 0.3035 (0.3202)  class_acc: 0.8571 (0.8621)  loss_scale: 4096.0000 (5729.8615)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0004  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.33 GB / 503.51 GB
Epoch: [20]  [370/893]  eta: 0:13:06  lr: 0.001613  min_lr: 0.000002  loss: 0.2961 (0.3196)  class_acc: 0.8571 (0.8621)  loss_scale: 4096.0000 (5685.8221)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.50 GB / 503.51 GB
Epoch: [20]  [380/893]  eta: 0:12:50  lr: 0.001612  min_lr: 0.000002  loss: 0.3105 (0.3200)  class_acc: 0.8393 (0.8617)  loss_scale: 4096.0000 (5644.0945)  weight_decay: 0.0500 (0.0500)  time: 1.4628  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.86 GB / 503.51 GB
Epoch: [20]  [390/893]  eta: 0:12:35  lr: 0.001612  min_lr: 0.000002  loss: 0.3264 (0.3197)  class_acc: 0.8571 (0.8620)  loss_scale: 4096.0000 (5604.5013)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0002  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.59 GB / 503.51 GB
Epoch: [20]  [400/893]  eta: 0:12:19  lr: 0.001611  min_lr: 0.000002  loss: 0.3105 (0.3197)  class_acc: 0.8571 (0.8620)  loss_scale: 4096.0000 (5566.8828)  weight_decay: 0.0500 (0.0500)  time: 1.4716  data: 0.0002  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.15 GB / 503.51 GB
Epoch: [20]  [410/893]  eta: 0:12:04  lr: 0.001610  min_lr: 0.000002  loss: 0.2915 (0.3191)  class_acc: 0.8571 (0.8621)  loss_scale: 4096.0000 (5531.0949)  weight_decay: 0.0500 (0.0500)  time: 1.4739  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.75 GB / 503.51 GB
Epoch: [20]  [420/893]  eta: 0:11:49  lr: 0.001609  min_lr: 0.000002  loss: 0.2839 (0.3182)  class_acc: 0.8571 (0.8622)  loss_scale: 4096.0000 (5497.0071)  weight_decay: 0.0500 (0.0500)  time: 1.4699  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.03 GB / 503.51 GB
Epoch: [20]  [430/893]  eta: 0:11:34  lr: 0.001609  min_lr: 0.000002  loss: 0.3093 (0.3195)  class_acc: 0.8571 (0.8619)  loss_scale: 4096.0000 (5464.5012)  weight_decay: 0.0500 (0.0500)  time: 1.4744  data: 0.0002  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.07 GB / 503.51 GB
Epoch: [20]  [440/893]  eta: 0:11:18  lr: 0.001608  min_lr: 0.000002  loss: 0.3472 (0.3202)  class_acc: 0.8571 (0.8616)  loss_scale: 4096.0000 (5433.4694)  weight_decay: 0.0500 (0.0500)  time: 1.4772  data: 0.0002  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.17 GB / 503.51 GB
Epoch: [20]  [450/893]  eta: 0:11:03  lr: 0.001607  min_lr: 0.000002  loss: 0.3086 (0.3195)  class_acc: 0.8750 (0.8622)  loss_scale: 4096.0000 (5403.8137)  weight_decay: 0.0500 (0.0500)  time: 1.4703  data: 0.0002  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.38 GB / 503.51 GB
Epoch: [20]  [460/893]  eta: 0:10:48  lr: 0.001606  min_lr: 0.000002  loss: 0.3086 (0.3195)  class_acc: 0.8571 (0.8619)  loss_scale: 4096.0000 (5375.4447)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0002  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.94 GB / 503.51 GB
Epoch: [20]  [470/893]  eta: 0:10:33  lr: 0.001605  min_lr: 0.000002  loss: 0.3149 (0.3194)  class_acc: 0.8571 (0.8620)  loss_scale: 4096.0000 (5348.2803)  weight_decay: 0.0500 (0.0500)  time: 1.4685  data: 0.0003  max mem: 31081
[2025-03-11 06:35:56,057] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 06:35:56,057] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.58 GB / 503.51 GB
Epoch: [20]  [480/893]  eta: 0:10:17  lr: 0.001605  min_lr: 0.000002  loss: 0.2883 (0.3186)  class_acc: 0.8750 (0.8624)  loss_scale: 4096.0000 (5364.8233)  weight_decay: 0.0500 (0.0500)  time: 1.4704  data: 0.0002  max mem: 31081
[2025-03-11 06:36:04,854] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 18322
[2025-03-11 06:36:04,854] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 06:36:04,854] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.19 GB / 503.51 GB
Epoch: [20]  [490/893]  eta: 0:10:02  lr: 0.001604  min_lr: 0.000002  loss: 0.2883 (0.3194)  class_acc: 0.8750 (0.8622)  loss_scale: 4096.0000 (5347.3238)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.90 GB / 503.51 GB
Epoch: [20]  [500/893]  eta: 0:09:47  lr: 0.001603  min_lr: 0.000002  loss: 0.3252 (0.3194)  class_acc: 0.8571 (0.8622)  loss_scale: 4096.0000 (5322.3473)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.78 GB / 503.51 GB
Epoch: [20]  [510/893]  eta: 0:09:32  lr: 0.001602  min_lr: 0.000002  loss: 0.3054 (0.3189)  class_acc: 0.8750 (0.8627)  loss_scale: 4096.0000 (5298.3483)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.05 GB / 503.51 GB
Epoch: [20]  [520/893]  eta: 0:09:17  lr: 0.001602  min_lr: 0.000002  loss: 0.2947 (0.3182)  class_acc: 0.8929 (0.8631)  loss_scale: 4096.0000 (5275.2706)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0004  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.89 GB / 503.51 GB
Epoch: [20]  [530/893]  eta: 0:09:02  lr: 0.001601  min_lr: 0.000002  loss: 0.2881 (0.3177)  class_acc: 0.8750 (0.8631)  loss_scale: 4096.0000 (5253.0621)  weight_decay: 0.0500 (0.0500)  time: 1.4710  data: 0.0004  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.28 GB / 503.51 GB
Epoch: [20]  [540/893]  eta: 0:08:46  lr: 0.001600  min_lr: 0.000002  loss: 0.3040 (0.3178)  class_acc: 0.8571 (0.8631)  loss_scale: 4096.0000 (5231.6747)  weight_decay: 0.0500 (0.0500)  time: 1.4695  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.47 GB / 503.51 GB
Epoch: [20]  [550/893]  eta: 0:08:32  lr: 0.001599  min_lr: 0.000002  loss: 0.3008 (0.3174)  class_acc: 0.8571 (0.8636)  loss_scale: 4096.0000 (5211.0635)  weight_decay: 0.0500 (0.0500)  time: 1.4760  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.08 GB / 503.51 GB
Epoch: [20]  [560/893]  eta: 0:08:16  lr: 0.001599  min_lr: 0.000002  loss: 0.2761 (0.3172)  class_acc: 0.8929 (0.8636)  loss_scale: 4096.0000 (5191.1872)  weight_decay: 0.0500 (0.0500)  time: 1.4763  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 92.82 GB / 503.51 GB
Epoch: [20]  [570/893]  eta: 0:08:01  lr: 0.001598  min_lr: 0.000002  loss: 0.3123 (0.3172)  class_acc: 0.8750 (0.8637)  loss_scale: 4096.0000 (5172.0070)  weight_decay: 0.0500 (0.0500)  time: 1.4669  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.99 GB / 503.51 GB
Epoch: [20]  [580/893]  eta: 0:07:46  lr: 0.001597  min_lr: 0.000002  loss: 0.3115 (0.3171)  class_acc: 0.8571 (0.8637)  loss_scale: 4096.0000 (5153.4871)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.20 GB / 503.51 GB
Epoch: [20]  [590/893]  eta: 0:07:31  lr: 0.001596  min_lr: 0.000002  loss: 0.3115 (0.3176)  class_acc: 0.8393 (0.8633)  loss_scale: 4096.0000 (5135.5939)  weight_decay: 0.0500 (0.0500)  time: 1.4592  data: 0.0003  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.21 GB / 503.51 GB
Epoch: [20]  [600/893]  eta: 0:07:16  lr: 0.001596  min_lr: 0.000002  loss: 0.3103 (0.3172)  class_acc: 0.8571 (0.8636)  loss_scale: 4096.0000 (5118.2962)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0004  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.00 GB / 503.51 GB
Epoch: [20]  [610/893]  eta: 0:07:01  lr: 0.001595  min_lr: 0.000002  loss: 0.3027 (0.3171)  class_acc: 0.8750 (0.8636)  loss_scale: 4096.0000 (5101.5646)  weight_decay: 0.0500 (0.0500)  time: 1.4720  data: 0.0003  max mem: 31081
[2025-03-11 06:39:14,301] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 06:39:14,301] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.71 GB / 503.51 GB
Epoch: [20]  [620/893]  eta: 0:06:46  lr: 0.001594  min_lr: 0.000002  loss: 0.3210 (0.3175)  class_acc: 0.8571 (0.8631)  loss_scale: 4096.0000 (5151.3301)  weight_decay: 0.0500 (0.0500)  time: 1.4691  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.66 GB / 503.51 GB
Epoch: [20]  [630/893]  eta: 0:06:31  lr: 0.001593  min_lr: 0.000002  loss: 0.2876 (0.3169)  class_acc: 0.8571 (0.8634)  loss_scale: 8192.0000 (5199.5182)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0002  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.20 GB / 503.51 GB
Epoch: [20]  [640/893]  eta: 0:06:16  lr: 0.001593  min_lr: 0.000002  loss: 0.2754 (0.3165)  class_acc: 0.8750 (0.8634)  loss_scale: 8192.0000 (5246.2028)  weight_decay: 0.0500 (0.0500)  time: 1.4682  data: 0.0002  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.54 GB / 503.51 GB
Epoch: [20]  [650/893]  eta: 0:06:01  lr: 0.001592  min_lr: 0.000002  loss: 0.2910 (0.3160)  class_acc: 0.8571 (0.8636)  loss_scale: 8192.0000 (5291.4531)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0003  max mem: 31081
[2025-03-11 06:40:18,864] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 18495
[2025-03-11 06:40:18,864] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 06:40:18,864] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.03 GB / 503.51 GB
Epoch: [20]  [660/893]  eta: 0:05:46  lr: 0.001591  min_lr: 0.000002  loss: 0.2983 (0.3157)  class_acc: 0.8571 (0.8637)  loss_scale: 8192.0000 (5298.1543)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 94.10 GB / 503.51 GB
Epoch: [20]  [670/893]  eta: 0:05:31  lr: 0.001590  min_lr: 0.000002  loss: 0.2971 (0.3155)  class_acc: 0.8750 (0.8640)  loss_scale: 4096.0000 (5280.2385)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 93.91 GB / 503.51 GB
Epoch: [20]  [680/893]  eta: 0:05:16  lr: 0.001590  min_lr: 0.000002  loss: 0.2715 (0.3148)  class_acc: 0.8929 (0.8644)  loss_scale: 4096.0000 (5262.8488)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.76 GB / 503.51 GB
Epoch: [20]  [690/893]  eta: 0:05:01  lr: 0.001589  min_lr: 0.000002  loss: 0.2747 (0.3147)  class_acc: 0.8750 (0.8645)  loss_scale: 4096.0000 (5245.9624)  weight_decay: 0.0500 (0.0500)  time: 1.4714  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.85 GB / 503.51 GB
Epoch: [20]  [700/893]  eta: 0:04:47  lr: 0.001588  min_lr: 0.000002  loss: 0.3025 (0.3151)  class_acc: 0.8571 (0.8644)  loss_scale: 4096.0000 (5229.5578)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.26 GB / 503.51 GB
Epoch: [20]  [710/893]  eta: 0:04:32  lr: 0.001587  min_lr: 0.000002  loss: 0.3384 (0.3155)  class_acc: 0.8571 (0.8642)  loss_scale: 4096.0000 (5213.6146)  weight_decay: 0.0500 (0.0500)  time: 1.4677  data: 0.0003  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 95.94 GB / 503.51 GB
Epoch: [20]  [720/893]  eta: 0:04:17  lr: 0.001586  min_lr: 0.000002  loss: 0.3162 (0.3157)  class_acc: 0.8393 (0.8640)  loss_scale: 4096.0000 (5198.1137)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0004  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.02 GB / 503.51 GB
Epoch: [20]  [730/893]  eta: 0:04:02  lr: 0.001586  min_lr: 0.000002  loss: 0.3042 (0.3156)  class_acc: 0.8750 (0.8641)  loss_scale: 4096.0000 (5183.0369)  weight_decay: 0.0500 (0.0500)  time: 1.4699  data: 0.0005  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 96.32 GB / 503.51 GB
Epoch: [20]  [740/893]  eta: 0:03:47  lr: 0.001585  min_lr: 0.000002  loss: 0.2751 (0.3154)  class_acc: 0.8750 (0.8642)  loss_scale: 4096.0000 (5168.3671)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0004  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.41 GB / 503.51 GB
Epoch: [20]  [750/893]  eta: 0:03:32  lr: 0.001584  min_lr: 0.000002  loss: 0.2827 (0.3157)  class_acc: 0.8750 (0.8641)  loss_scale: 4096.0000 (5154.0879)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.12 GB / 503.51 GB
Epoch: [20]  [760/893]  eta: 0:03:17  lr: 0.001583  min_lr: 0.000002  loss: 0.3235 (0.3159)  class_acc: 0.8571 (0.8640)  loss_scale: 4096.0000 (5140.1840)  weight_decay: 0.0500 (0.0500)  time: 1.4758  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.04 GB / 503.51 GB
Epoch: [20]  [770/893]  eta: 0:03:02  lr: 0.001583  min_lr: 0.000002  loss: 0.3179 (0.3162)  class_acc: 0.8571 (0.8640)  loss_scale: 4096.0000 (5126.6407)  weight_decay: 0.0500 (0.0500)  time: 1.4749  data: 0.0004  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.43 GB / 503.51 GB
Epoch: [20]  [780/893]  eta: 0:02:47  lr: 0.001582  min_lr: 0.000002  loss: 0.3254 (0.3164)  class_acc: 0.8571 (0.8639)  loss_scale: 4096.0000 (5113.4443)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
[2025-03-11 06:43:28,573] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 06:43:28,573] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.43 GB / 503.51 GB
Epoch: [20]  [790/893]  eta: 0:02:33  lr: 0.001581  min_lr: 0.000002  loss: 0.3186 (0.3163)  class_acc: 0.8571 (0.8640)  loss_scale: 4096.0000 (5136.8293)  weight_decay: 0.0500 (0.0500)  time: 1.4946  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.77 GB / 503.51 GB
Epoch: [20]  [800/893]  eta: 0:02:18  lr: 0.001580  min_lr: 0.000002  loss: 0.2737 (0.3158)  class_acc: 0.8571 (0.8643)  loss_scale: 8192.0000 (5174.9713)  weight_decay: 0.0500 (0.0500)  time: 1.4935  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.80 GB / 503.51 GB
Epoch: [20]  [810/893]  eta: 0:02:03  lr: 0.001580  min_lr: 0.000002  loss: 0.2512 (0.3157)  class_acc: 0.8750 (0.8643)  loss_scale: 8192.0000 (5212.1726)  weight_decay: 0.0500 (0.0500)  time: 1.4771  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.79 GB / 503.51 GB
Epoch: [20]  [820/893]  eta: 0:01:48  lr: 0.001579  min_lr: 0.000002  loss: 0.3071 (0.3154)  class_acc: 0.8750 (0.8645)  loss_scale: 8192.0000 (5248.4677)  weight_decay: 0.0500 (0.0500)  time: 1.4780  data: 0.0004  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 90.80 GB / 503.51 GB
Epoch: [20]  [830/893]  eta: 0:01:33  lr: 0.001578  min_lr: 0.000002  loss: 0.3274 (0.3157)  class_acc: 0.8571 (0.8641)  loss_scale: 8192.0000 (5283.8893)  weight_decay: 0.0500 (0.0500)  time: 1.4712  data: 0.0004  max mem: 31081
[2025-03-11 06:44:44,110] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 18675
[2025-03-11 06:44:44,110] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 06:44:44,110] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 91.65 GB / 503.51 GB
Epoch: [20]  [840/893]  eta: 0:01:18  lr: 0.001577  min_lr: 0.000002  loss: 0.3425 (0.3159)  class_acc: 0.8393 (0.8640)  loss_scale: 8192.0000 (5289.2461)  weight_decay: 0.0500 (0.0500)  time: 1.4699  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.26 GB / 503.51 GB
Epoch: [20]  [850/893]  eta: 0:01:03  lr: 0.001577  min_lr: 0.000002  loss: 0.3003 (0.3159)  class_acc: 0.8393 (0.8640)  loss_scale: 4096.0000 (5275.2244)  weight_decay: 0.0500 (0.0500)  time: 1.4602  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.50 GB / 503.51 GB
Epoch: [20]  [860/893]  eta: 0:00:48  lr: 0.001576  min_lr: 0.000002  loss: 0.3079 (0.3160)  class_acc: 0.8571 (0.8639)  loss_scale: 4096.0000 (5261.5285)  weight_decay: 0.0500 (0.0500)  time: 1.4462  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.38 GB / 503.51 GB
Epoch: [20]  [870/893]  eta: 0:00:34  lr: 0.001575  min_lr: 0.000002  loss: 0.3079 (0.3160)  class_acc: 0.8571 (0.8640)  loss_scale: 4096.0000 (5248.1470)  weight_decay: 0.0500 (0.0500)  time: 1.4411  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.38 GB / 503.51 GB
Epoch: [20]  [880/893]  eta: 0:00:19  lr: 0.001574  min_lr: 0.000002  loss: 0.2954 (0.3159)  class_acc: 0.8571 (0.8638)  loss_scale: 4096.0000 (5235.0692)  weight_decay: 0.0500 (0.0500)  time: 1.4391  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.38 GB / 503.51 GB
Epoch: [20]  [890/893]  eta: 0:00:04  lr: 0.001573  min_lr: 0.000002  loss: 0.2954 (0.3159)  class_acc: 0.8571 (0.8639)  loss_scale: 4096.0000 (5222.2851)  weight_decay: 0.0500 (0.0500)  time: 1.4401  data: 0.0001  max mem: 31081
Epoch: [20]  [892/893]  eta: 0:00:01  lr: 0.001573  min_lr: 0.000002  loss: 0.2954 (0.3158)  class_acc: 0.8571 (0.8639)  loss_scale: 4096.0000 (5221.0224)  weight_decay: 0.0500 (0.0500)  time: 1.3888  data: 0.0001  max mem: 31081
Epoch: [20] Total time: 0:22:03 (1.4818 s / it)
Averaged stats: lr: 0.001573  min_lr: 0.000002  loss: 0.2954 (0.3158)  class_acc: 0.8571 (0.8639)  loss_scale: 4096.0000 (5221.0224)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:12:22  loss: 0.7306 (0.7306)  acc: 78.5714 (78.5714)  time: 10.9095  data: 10.3662  max mem: 31081
Val:  [ 10/728]  eta: 0:18:23  loss: 0.4819 (0.4746)  acc: 80.9524 (80.7359)  time: 1.5369  data: 1.0143  max mem: 31081
Val:  [ 20/728]  eta: 0:14:06  loss: 0.4343 (0.4929)  acc: 80.9524 (79.3084)  time: 0.7094  data: 0.1894  max mem: 31081
Val:  [ 30/728]  eta: 0:12:37  loss: 0.3909 (0.4711)  acc: 82.1429 (79.4931)  time: 0.8358  data: 0.3135  max mem: 31081
Val:  [ 40/728]  eta: 0:11:49  loss: 0.3831 (0.4784)  acc: 80.9524 (78.9199)  time: 0.8603  data: 0.3360  max mem: 31081
Val:  [ 50/728]  eta: 0:11:10  loss: 0.4447 (0.4648)  acc: 80.9524 (79.4818)  time: 0.8394  data: 0.3188  max mem: 31081
Val:  [ 60/728]  eta: 0:10:18  loss: 0.4901 (0.4869)  acc: 78.5714 (78.5324)  time: 0.7067  data: 0.1873  max mem: 31081
Val:  [ 70/728]  eta: 0:09:49  loss: 0.6100 (0.4967)  acc: 78.5714 (77.7666)  time: 0.6594  data: 0.1376  max mem: 31081
Val:  [ 80/728]  eta: 0:09:34  loss: 0.4421 (0.5080)  acc: 78.5714 (77.6014)  time: 0.7692  data: 0.2472  max mem: 31081
Val:  [ 90/728]  eta: 0:09:22  loss: 0.5293 (0.5227)  acc: 78.5714 (77.0801)  time: 0.8294  data: 0.3070  max mem: 31081
Val:  [100/728]  eta: 0:09:09  loss: 0.4597 (0.5070)  acc: 77.3810 (77.7463)  time: 0.8284  data: 0.3039  max mem: 31081
Val:  [110/728]  eta: 0:09:01  loss: 0.3758 (0.5065)  acc: 78.5714 (77.7671)  time: 0.8493  data: 0.3236  max mem: 31081
Val:  [120/728]  eta: 0:08:42  loss: 0.3528 (0.4927)  acc: 83.3333 (78.4337)  time: 0.7775  data: 0.2559  max mem: 31081
Val:  [130/728]  eta: 0:08:25  loss: 0.3605 (0.5021)  acc: 83.3333 (78.3442)  time: 0.6744  data: 0.1540  max mem: 31081
Val:  [140/728]  eta: 0:08:14  loss: 0.4952 (0.5049)  acc: 78.5714 (78.1746)  time: 0.7318  data: 0.2102  max mem: 31081
Val:  [150/728]  eta: 0:08:01  loss: 0.4612 (0.5048)  acc: 80.9524 (78.2640)  time: 0.7557  data: 0.2343  max mem: 31081
Val:  [160/728]  eta: 0:07:53  loss: 0.4730 (0.5121)  acc: 73.8095 (78.0021)  time: 0.7820  data: 0.2582  max mem: 31081
Val:  [170/728]  eta: 0:07:47  loss: 0.4730 (0.5095)  acc: 75.0000 (77.9518)  time: 0.8683  data: 0.3445  max mem: 31081
Val:  [180/728]  eta: 0:07:31  loss: 0.3932 (0.5069)  acc: 78.5714 (78.0321)  time: 0.7385  data: 0.2175  max mem: 31081
Val:  [190/728]  eta: 0:07:19  loss: 0.4226 (0.5034)  acc: 80.9524 (78.1476)  time: 0.6492  data: 0.1279  max mem: 31081
Val:  [200/728]  eta: 0:07:12  loss: 0.4001 (0.4967)  acc: 83.3333 (78.4115)  time: 0.7750  data: 0.2500  max mem: 31081
Val:  [210/728]  eta: 0:07:06  loss: 0.4001 (0.4991)  acc: 78.5714 (78.3683)  time: 0.8902  data: 0.3643  max mem: 31081
Val:  [220/728]  eta: 0:06:58  loss: 0.4147 (0.4980)  acc: 76.1905 (78.4906)  time: 0.8795  data: 0.3537  max mem: 31081
Val:  [230/728]  eta: 0:06:48  loss: 0.4001 (0.5002)  acc: 82.1429 (78.4168)  time: 0.7888  data: 0.2634  max mem: 31081
Val:  [240/728]  eta: 0:06:36  loss: 0.4385 (0.5009)  acc: 80.9524 (78.4726)  time: 0.6965  data: 0.1728  max mem: 31081
Val:  [250/728]  eta: 0:06:26  loss: 0.4673 (0.5008)  acc: 77.3810 (78.4861)  time: 0.6626  data: 0.1424  max mem: 31081
Val:  [260/728]  eta: 0:06:19  loss: 0.3417 (0.4993)  acc: 78.5714 (78.5851)  time: 0.7814  data: 0.2616  max mem: 31081
Val:  [270/728]  eta: 0:06:11  loss: 0.3417 (0.5005)  acc: 79.7619 (78.5583)  time: 0.8530  data: 0.3321  max mem: 31081
Val:  [280/728]  eta: 0:06:07  loss: 0.5222 (0.5027)  acc: 77.3810 (78.5333)  time: 0.9292  data: 0.4068  max mem: 31081
Val:  [290/728]  eta: 0:05:58  loss: 0.4055 (0.4995)  acc: 80.9524 (78.7473)  time: 0.9124  data: 0.3881  max mem: 31081
Val:  [300/728]  eta: 0:05:46  loss: 0.4055 (0.4983)  acc: 80.9524 (78.7652)  time: 0.6630  data: 0.1407  max mem: 31081
Val:  [310/728]  eta: 0:05:38  loss: 0.5132 (0.5012)  acc: 77.3810 (78.6174)  time: 0.6727  data: 0.1514  max mem: 31081
Val:  [320/728]  eta: 0:05:30  loss: 0.4503 (0.4984)  acc: 80.9524 (78.6975)  time: 0.8096  data: 0.2875  max mem: 31081
Val:  [330/728]  eta: 0:05:22  loss: 0.3806 (0.4967)  acc: 80.9524 (78.7549)  time: 0.8390  data: 0.3167  max mem: 31081
Val:  [340/728]  eta: 0:05:15  loss: 0.3539 (0.4918)  acc: 80.9524 (78.9380)  time: 0.8914  data: 0.3689  max mem: 31081
Val:  [350/728]  eta: 0:05:07  loss: 0.3556 (0.4951)  acc: 82.1429 (79.0022)  time: 0.8531  data: 0.3298  max mem: 31081
Val:  [360/728]  eta: 0:04:56  loss: 0.3829 (0.4923)  acc: 82.1429 (79.1189)  time: 0.6622  data: 0.1414  max mem: 31081
Val:  [370/728]  eta: 0:04:48  loss: 0.3829 (0.4968)  acc: 80.9524 (78.9148)  time: 0.6903  data: 0.1684  max mem: 31081
Val:  [380/728]  eta: 0:04:40  loss: 0.4879 (0.4989)  acc: 76.1905 (78.8495)  time: 0.8422  data: 0.3182  max mem: 31081
Val:  [390/728]  eta: 0:04:33  loss: 0.3743 (0.4941)  acc: 78.5714 (79.0068)  time: 0.8244  data: 0.3007  max mem: 31081
Val:  [400/728]  eta: 0:04:25  loss: 0.3332 (0.4938)  acc: 85.7143 (79.1177)  time: 0.8664  data: 0.3446  max mem: 31081
Val:  [410/728]  eta: 0:04:18  loss: 0.4068 (0.4933)  acc: 83.3333 (79.1420)  time: 0.8855  data: 0.3668  max mem: 31081
Val:  [420/728]  eta: 0:04:07  loss: 0.4577 (0.4918)  acc: 80.9524 (79.2275)  time: 0.6941  data: 0.1731  max mem: 31081
Val:  [430/728]  eta: 0:04:00  loss: 0.3919 (0.4907)  acc: 80.9524 (79.2482)  time: 0.6981  data: 0.1741  max mem: 31081
Val:  [440/728]  eta: 0:03:52  loss: 0.3788 (0.4906)  acc: 80.9524 (79.2598)  time: 0.8294  data: 0.3074  max mem: 31081
Val:  [450/728]  eta: 0:03:44  loss: 0.4167 (0.4927)  acc: 82.1429 (79.2366)  time: 0.8237  data: 0.3027  max mem: 31081
Val:  [460/728]  eta: 0:03:36  loss: 0.5367 (0.4945)  acc: 80.9524 (79.1318)  time: 0.8415  data: 0.3213  max mem: 31081
Val:  [470/728]  eta: 0:03:28  loss: 0.4001 (0.4932)  acc: 80.9524 (79.1957)  time: 0.7931  data: 0.2696  max mem: 31081
Val:  [480/728]  eta: 0:03:18  loss: 0.3745 (0.4917)  acc: 79.7619 (79.2521)  time: 0.6426  data: 0.1178  max mem: 31081
Val:  [490/728]  eta: 0:03:10  loss: 0.4871 (0.4914)  acc: 79.7619 (79.2358)  time: 0.6840  data: 0.1640  max mem: 31081
Val:  [500/728]  eta: 0:03:02  loss: 0.4130 (0.4910)  acc: 80.9524 (79.2795)  time: 0.8424  data: 0.3190  max mem: 31081
Val:  [510/728]  eta: 0:02:55  loss: 0.3732 (0.4884)  acc: 83.3333 (79.3565)  time: 0.8646  data: 0.3391  max mem: 31081
Val:  [520/728]  eta: 0:02:47  loss: 0.3938 (0.4905)  acc: 80.9524 (79.2729)  time: 0.8428  data: 0.3182  max mem: 31081
Val:  [530/728]  eta: 0:02:38  loss: 0.4543 (0.4902)  acc: 79.7619 (79.2889)  time: 0.7437  data: 0.2201  max mem: 31081
Val:  [540/728]  eta: 0:02:29  loss: 0.4543 (0.4924)  acc: 79.7619 (79.1986)  time: 0.6075  data: 0.0859  max mem: 31081
Val:  [550/728]  eta: 0:02:21  loss: 0.5882 (0.4953)  acc: 75.0000 (79.1159)  time: 0.6663  data: 0.1408  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.4716 (0.4943)  acc: 77.3810 (79.1614)  time: 0.7479  data: 0.2225  max mem: 31081
Val:  [570/728]  eta: 0:02:05  loss: 0.4059 (0.5004)  acc: 77.3810 (79.0614)  time: 0.7798  data: 0.2584  max mem: 31081
Val:  [580/728]  eta: 0:01:57  loss: 0.4716 (0.4997)  acc: 77.3810 (79.1124)  time: 0.8247  data: 0.3009  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.4648 (0.5010)  acc: 80.9524 (79.0428)  time: 0.7349  data: 0.2067  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.4570 (0.5010)  acc: 80.9524 (79.0330)  time: 0.6815  data: 0.1552  max mem: 31081
Val:  [610/728]  eta: 0:01:33  loss: 0.5259 (0.5028)  acc: 76.1905 (78.9163)  time: 0.6593  data: 0.1374  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4753 (0.5066)  acc: 77.3810 (78.9012)  time: 0.7692  data: 0.2482  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4226 (0.5044)  acc: 80.9524 (78.9997)  time: 0.8580  data: 0.3365  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3735 (0.5035)  acc: 84.5238 (79.0264)  time: 0.7923  data: 0.2699  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4265 (0.5033)  acc: 80.9524 (79.0213)  time: 0.7911  data: 0.2674  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4265 (0.5029)  acc: 82.1429 (79.0271)  time: 0.7965  data: 0.2713  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4465 (0.5032)  acc: 79.7619 (79.0221)  time: 0.6909  data: 0.1681  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4465 (0.5018)  acc: 79.7619 (79.0574)  time: 0.6613  data: 0.1403  max mem: 31081
Val:  [690/728]  eta: 0:00:30  loss: 0.3422 (0.5010)  acc: 84.5238 (79.0900)  time: 0.8250  data: 0.3018  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3857 (0.5004)  acc: 84.5238 (79.0741)  time: 0.8635  data: 0.3390  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4028 (0.5010)  acc: 79.7619 (79.0553)  time: 0.8362  data: 0.3144  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.4390 (0.5012)  acc: 80.9524 (79.0767)  time: 0.8101  data: 0.2968  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.4390 (0.5031)  acc: 82.1429 (79.0576)  time: 0.7867  data: 0.2967  max mem: 31081
Val: Total time: 0:09:33 (0.7874 s / it)
* Acc@1 79.058 AP 0.8020454049110413 loss 0.503
Accuracy of the network on the 61096 val videos: 79.1%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.69 GB / 503.51 GB
Epoch: [21]  [  0/893]  eta: 3:04:10  lr: 0.001573  min_lr: 0.000002  loss: 0.3303 (0.3303)  class_acc: 0.8571 (0.8571)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 12.3748  data: 11.0654  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.64 GB / 503.51 GB
Epoch: [21]  [ 10/893]  eta: 0:38:04  lr: 0.001573  min_lr: 0.000002  loss: 0.3323 (0.3312)  class_acc: 0.8571 (0.8555)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5872  data: 1.0961  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.62 GB / 503.51 GB
Epoch: [21]  [ 20/893]  eta: 0:30:09  lr: 0.001572  min_lr: 0.000002  loss: 0.3267 (0.3123)  class_acc: 0.8571 (0.8656)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5573  data: 0.0498  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.35 GB / 503.51 GB
Epoch: [21]  [ 30/893]  eta: 0:27:12  lr: 0.001571  min_lr: 0.000002  loss: 0.3091 (0.3121)  class_acc: 0.8571 (0.8618)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5097  data: 0.0005  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.20 GB / 503.51 GB
Epoch: [21]  [ 40/893]  eta: 0:25:34  lr: 0.001570  min_lr: 0.000002  loss: 0.3091 (0.3163)  class_acc: 0.8571 (0.8619)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5124  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.66 GB / 503.51 GB
Epoch: [21]  [ 50/893]  eta: 0:24:26  lr: 0.001569  min_lr: 0.000002  loss: 0.3074 (0.3115)  class_acc: 0.8393 (0.8641)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5044  data: 0.0004  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.62 GB / 503.51 GB
Epoch: [21]  [ 60/893]  eta: 0:23:31  lr: 0.001569  min_lr: 0.000002  loss: 0.3188 (0.3152)  class_acc: 0.8393 (0.8621)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4788  data: 0.0003  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.95 GB / 503.51 GB
Epoch: [21]  [ 70/893]  eta: 0:22:48  lr: 0.001568  min_lr: 0.000002  loss: 0.3262 (0.3152)  class_acc: 0.8571 (0.8627)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
[2025-03-11 06:57:41,124] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 06:57:41,124] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2025-03-11 06:57:52,885] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 18812
[2025-03-11 06:57:52,885] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 06:57:52,885] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.58 GB / 503.51 GB
Epoch: [21]  [ 80/893]  eta: 0:22:12  lr: 0.001567  min_lr: 0.000002  loss: 0.3267 (0.3192)  class_acc: 0.8393 (0.8593)  loss_scale: 4096.0000 (4500.5432)  weight_decay: 0.0500 (0.0500)  time: 1.4724  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.13 GB / 503.51 GB
Epoch: [21]  [ 90/893]  eta: 0:21:41  lr: 0.001566  min_lr: 0.000002  loss: 0.3123 (0.3179)  class_acc: 0.8571 (0.8597)  loss_scale: 4096.0000 (4456.0879)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0003  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.23 GB / 503.51 GB
Epoch: [21]  [100/893]  eta: 0:21:12  lr: 0.001566  min_lr: 0.000002  loss: 0.2771 (0.3132)  class_acc: 0.8750 (0.8630)  loss_scale: 4096.0000 (4420.4356)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.76 GB / 503.51 GB
Epoch: [21]  [110/893]  eta: 0:20:46  lr: 0.001565  min_lr: 0.000002  loss: 0.2617 (0.3138)  class_acc: 0.8750 (0.8621)  loss_scale: 4096.0000 (4391.2072)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.76 GB / 503.51 GB
Epoch: [21]  [120/893]  eta: 0:20:22  lr: 0.001564  min_lr: 0.000002  loss: 0.3101 (0.3163)  class_acc: 0.8393 (0.8597)  loss_scale: 4096.0000 (4366.8099)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.37 GB / 503.51 GB
Epoch: [21]  [130/893]  eta: 0:19:59  lr: 0.001563  min_lr: 0.000002  loss: 0.2852 (0.3148)  class_acc: 0.8393 (0.8601)  loss_scale: 4096.0000 (4346.1374)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0002  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.90 GB / 503.51 GB
Epoch: [21]  [140/893]  eta: 0:19:38  lr: 0.001562  min_lr: 0.000002  loss: 0.2808 (0.3118)  class_acc: 0.8571 (0.8612)  loss_scale: 4096.0000 (4328.3972)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0002  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.07 GB / 503.51 GB
Epoch: [21]  [150/893]  eta: 0:19:17  lr: 0.001562  min_lr: 0.000002  loss: 0.2771 (0.3106)  class_acc: 0.8750 (0.8620)  loss_scale: 4096.0000 (4313.0066)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.38 GB / 503.51 GB
Epoch: [21]  [160/893]  eta: 0:18:58  lr: 0.001561  min_lr: 0.000002  loss: 0.2896 (0.3108)  class_acc: 0.8571 (0.8611)  loss_scale: 4096.0000 (4299.5280)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.00 GB / 503.51 GB
Epoch: [21]  [170/893]  eta: 0:18:38  lr: 0.001560  min_lr: 0.000002  loss: 0.2722 (0.3070)  class_acc: 0.8750 (0.8636)  loss_scale: 4096.0000 (4287.6257)  weight_decay: 0.0500 (0.0500)  time: 1.4669  data: 0.0004  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.97 GB / 503.51 GB
Epoch: [21]  [180/893]  eta: 0:18:20  lr: 0.001559  min_lr: 0.000002  loss: 0.2874 (0.3088)  class_acc: 0.8750 (0.8628)  loss_scale: 4096.0000 (4277.0387)  weight_decay: 0.0500 (0.0500)  time: 1.4615  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.65 GB / 503.51 GB
Epoch: [21]  [190/893]  eta: 0:18:01  lr: 0.001559  min_lr: 0.000002  loss: 0.3215 (0.3100)  class_acc: 0.8571 (0.8627)  loss_scale: 4096.0000 (4267.5602)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0003  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.97 GB / 503.51 GB
Epoch: [21]  [200/893]  eta: 0:17:44  lr: 0.001558  min_lr: 0.000002  loss: 0.2944 (0.3092)  class_acc: 0.8571 (0.8628)  loss_scale: 4096.0000 (4259.0249)  weight_decay: 0.0500 (0.0500)  time: 1.4704  data: 0.0004  max mem: 31081
[2025-03-11 07:01:01,853] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 07:01:01,853] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.46 GB / 503.51 GB
Epoch: [21]  [210/893]  eta: 0:17:26  lr: 0.001557  min_lr: 0.000002  loss: 0.2798 (0.3077)  class_acc: 0.8750 (0.8640)  loss_scale: 4096.0000 (4290.1232)  weight_decay: 0.0500 (0.0500)  time: 1.4649  data: 0.0004  max mem: 31081
[2025-03-11 07:01:17,935] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 18952
[2025-03-11 07:01:17,935] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 07:01:17,935] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.67 GB / 503.51 GB
Epoch: [21]  [220/893]  eta: 0:17:08  lr: 0.001556  min_lr: 0.000002  loss: 0.2764 (0.3058)  class_acc: 0.8929 (0.8655)  loss_scale: 8192.0000 (4448.1448)  weight_decay: 0.0500 (0.0500)  time: 1.4575  data: 0.0004  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.54 GB / 503.51 GB
Epoch: [21]  [230/893]  eta: 0:16:51  lr: 0.001556  min_lr: 0.000002  loss: 0.2947 (0.3063)  class_acc: 0.8750 (0.8655)  loss_scale: 4096.0000 (4432.9004)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.70 GB / 503.51 GB
Epoch: [21]  [240/893]  eta: 0:16:34  lr: 0.001555  min_lr: 0.000002  loss: 0.3005 (0.3075)  class_acc: 0.8571 (0.8656)  loss_scale: 4096.0000 (4418.9212)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.26 GB / 503.51 GB
Epoch: [21]  [250/893]  eta: 0:16:18  lr: 0.001554  min_lr: 0.000002  loss: 0.3115 (0.3069)  class_acc: 0.8750 (0.8662)  loss_scale: 4096.0000 (4406.0558)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0002  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.87 GB / 503.51 GB
Epoch: [21]  [260/893]  eta: 0:16:01  lr: 0.001553  min_lr: 0.000002  loss: 0.2776 (0.3069)  class_acc: 0.8929 (0.8665)  loss_scale: 4096.0000 (4394.1762)  weight_decay: 0.0500 (0.0500)  time: 1.4704  data: 0.0002  max mem: 31081
[2025-03-11 07:02:26,873] [INFO] [logging.py:129:log_dist] [Rank 0] step=19000, skipped=112, lr=[2.0278501529056753e-06, 2.0278501529056753e-06, 3.3797502548427923e-06, 3.3797502548427923e-06, 5.632917091404654e-06, 5.632917091404654e-06, 9.388195152341089e-06, 9.388195152341089e-06, 1.5646991920568485e-05, 1.5646991920568485e-05, 2.607831986761414e-05, 2.607831986761414e-05, 4.346386644602357e-05, 4.346386644602357e-05, 7.243977741003929e-05, 7.243977741003929e-05, 0.00012073296235006548, 0.00012073296235006548, 0.00020122160391677582, 0.00020122160391677582, 0.000335369339861293, 0.000335369339861293, 0.0005589488997688217, 0.0005589488997688217, 0.0009315814996147029, 0.0009315814996147029, 0.0015526358326911716, 0.0015526358326911716], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 07:02:26,874] [INFO] [timer.py:264:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=60.99772181324069, CurrSamplesPerSec=58.99038485432724, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.68 GB / 503.51 GB
Epoch: [21]  [270/893]  eta: 0:15:45  lr: 0.001552  min_lr: 0.000002  loss: 0.2776 (0.3061)  class_acc: 0.8750 (0.8669)  loss_scale: 4096.0000 (4383.1734)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.08 GB / 503.51 GB
Epoch: [21]  [280/893]  eta: 0:15:28  lr: 0.001552  min_lr: 0.000002  loss: 0.3098 (0.3067)  class_acc: 0.8750 (0.8666)  loss_scale: 4096.0000 (4372.9537)  weight_decay: 0.0500 (0.0500)  time: 1.4637  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.65 GB / 503.51 GB
Epoch: [21]  [290/893]  eta: 0:15:12  lr: 0.001551  min_lr: 0.000002  loss: 0.3223 (0.3068)  class_acc: 0.8571 (0.8665)  loss_scale: 4096.0000 (4363.4364)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0004  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.27 GB / 503.51 GB
Epoch: [21]  [300/893]  eta: 0:14:56  lr: 0.001550  min_lr: 0.000002  loss: 0.3232 (0.3072)  class_acc: 0.8571 (0.8666)  loss_scale: 4096.0000 (4354.5515)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0004  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.36 GB / 503.51 GB
Epoch: [21]  [310/893]  eta: 0:14:40  lr: 0.001549  min_lr: 0.000002  loss: 0.3279 (0.3080)  class_acc: 0.8571 (0.8660)  loss_scale: 4096.0000 (4346.2379)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.23 GB / 503.51 GB
Epoch: [21]  [320/893]  eta: 0:14:24  lr: 0.001549  min_lr: 0.000002  loss: 0.3389 (0.3086)  class_acc: 0.8393 (0.8653)  loss_scale: 4096.0000 (4338.4424)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0003  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.35 GB / 503.51 GB
Epoch: [21]  [330/893]  eta: 0:14:08  lr: 0.001548  min_lr: 0.000002  loss: 0.3154 (0.3089)  class_acc: 0.8571 (0.8650)  loss_scale: 4096.0000 (4331.1178)  weight_decay: 0.0500 (0.0500)  time: 1.4615  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.60 GB / 503.51 GB
Epoch: [21]  [340/893]  eta: 0:13:53  lr: 0.001547  min_lr: 0.000002  loss: 0.3147 (0.3094)  class_acc: 0.8571 (0.8648)  loss_scale: 4096.0000 (4324.2229)  weight_decay: 0.0500 (0.0500)  time: 1.4724  data: 0.0003  max mem: 31081
[2025-03-11 07:04:27,080] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 07:04:27,080] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.47 GB / 503.51 GB
Epoch: [21]  [350/893]  eta: 0:13:37  lr: 0.001546  min_lr: 0.000002  loss: 0.3140 (0.3100)  class_acc: 0.8750 (0.8644)  loss_scale: 4096.0000 (4341.0598)  weight_decay: 0.0500 (0.0500)  time: 1.4714  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.07 GB / 503.51 GB
Epoch: [21]  [360/893]  eta: 0:13:21  lr: 0.001545  min_lr: 0.000002  loss: 0.3140 (0.3101)  class_acc: 0.8571 (0.8642)  loss_scale: 8192.0000 (4447.7341)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0002  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.22 GB / 503.51 GB
Epoch: [21]  [370/893]  eta: 0:13:06  lr: 0.001545  min_lr: 0.000002  loss: 0.3022 (0.3099)  class_acc: 0.8571 (0.8642)  loss_scale: 8192.0000 (4548.6577)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0002  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.59 GB / 503.51 GB
Epoch: [21]  [380/893]  eta: 0:12:50  lr: 0.001544  min_lr: 0.000002  loss: 0.2908 (0.3097)  class_acc: 0.8393 (0.8641)  loss_scale: 8192.0000 (4644.2835)  weight_decay: 0.0500 (0.0500)  time: 1.4669  data: 0.0002  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.16 GB / 503.51 GB
Epoch: [21]  [390/893]  eta: 0:12:35  lr: 0.001543  min_lr: 0.000002  loss: 0.2737 (0.3089)  class_acc: 0.8750 (0.8648)  loss_scale: 8192.0000 (4735.0179)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0002  max mem: 31081
[2025-03-11 07:05:39,051] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 19130
[2025-03-11 07:05:39,051] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 07:05:39,051] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.39 GB / 503.51 GB
Epoch: [21]  [400/893]  eta: 0:12:19  lr: 0.001542  min_lr: 0.000002  loss: 0.2632 (0.3079)  class_acc: 0.8929 (0.8652)  loss_scale: 8192.0000 (4790.5835)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0002  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.90 GB / 503.51 GB
Epoch: [21]  [410/893]  eta: 0:12:04  lr: 0.001541  min_lr: 0.000002  loss: 0.2325 (0.3074)  class_acc: 0.8750 (0.8656)  loss_scale: 4096.0000 (4773.6837)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0002  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.34 GB / 503.51 GB
Epoch: [21]  [420/893]  eta: 0:11:49  lr: 0.001541  min_lr: 0.000002  loss: 0.2920 (0.3070)  class_acc: 0.8750 (0.8656)  loss_scale: 4096.0000 (4757.5867)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.25 GB / 503.51 GB
Epoch: [21]  [430/893]  eta: 0:11:33  lr: 0.001540  min_lr: 0.000002  loss: 0.2893 (0.3075)  class_acc: 0.8750 (0.8654)  loss_scale: 4096.0000 (4742.2367)  weight_decay: 0.0500 (0.0500)  time: 1.4617  data: 0.0003  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.17 GB / 503.51 GB
Epoch: [21]  [440/893]  eta: 0:11:18  lr: 0.001539  min_lr: 0.000002  loss: 0.2893 (0.3072)  class_acc: 0.8750 (0.8653)  loss_scale: 4096.0000 (4727.5828)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.18 GB / 503.51 GB
Epoch: [21]  [450/893]  eta: 0:11:02  lr: 0.001538  min_lr: 0.000002  loss: 0.2788 (0.3067)  class_acc: 0.8929 (0.8661)  loss_scale: 4096.0000 (4713.5787)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0002  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.48 GB / 503.51 GB
Epoch: [21]  [460/893]  eta: 0:10:47  lr: 0.001538  min_lr: 0.000002  loss: 0.2788 (0.3070)  class_acc: 0.8571 (0.8655)  loss_scale: 4096.0000 (4700.1822)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.28 GB / 503.51 GB
Epoch: [21]  [470/893]  eta: 0:10:32  lr: 0.001537  min_lr: 0.000002  loss: 0.3193 (0.3077)  class_acc: 0.8571 (0.8657)  loss_scale: 4096.0000 (4687.3546)  weight_decay: 0.0500 (0.0500)  time: 1.4632  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.84 GB / 503.51 GB
Epoch: [21]  [480/893]  eta: 0:10:17  lr: 0.001536  min_lr: 0.000002  loss: 0.3225 (0.3083)  class_acc: 0.8571 (0.8652)  loss_scale: 4096.0000 (4675.0603)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.72 GB / 503.51 GB
Epoch: [21]  [490/893]  eta: 0:10:02  lr: 0.001535  min_lr: 0.000002  loss: 0.3684 (0.3096)  class_acc: 0.8393 (0.8645)  loss_scale: 4096.0000 (4663.2668)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.24 GB / 503.51 GB
Epoch: [21]  [500/893]  eta: 0:09:46  lr: 0.001534  min_lr: 0.000002  loss: 0.3420 (0.3095)  class_acc: 0.8393 (0.8645)  loss_scale: 4096.0000 (4651.9441)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.33 GB / 503.51 GB
Epoch: [21]  [510/893]  eta: 0:09:31  lr: 0.001534  min_lr: 0.000002  loss: 0.3120 (0.3092)  class_acc: 0.8750 (0.8646)  loss_scale: 4096.0000 (4641.0646)  weight_decay: 0.0500 (0.0500)  time: 1.4727  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.68 GB / 503.51 GB
Epoch: [21]  [520/893]  eta: 0:09:16  lr: 0.001533  min_lr: 0.000002  loss: 0.3137 (0.3090)  class_acc: 0.8571 (0.8649)  loss_scale: 4096.0000 (4630.6027)  weight_decay: 0.0500 (0.0500)  time: 1.4703  data: 0.0002  max mem: 31081
[2025-03-11 07:08:48,064] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 07:08:48,065] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.31 GB / 503.51 GB
Epoch: [21]  [530/893]  eta: 0:09:01  lr: 0.001532  min_lr: 0.000002  loss: 0.3142 (0.3089)  class_acc: 0.8571 (0.8651)  loss_scale: 4096.0000 (4651.3898)  weight_decay: 0.0500 (0.0500)  time: 1.4663  data: 0.0002  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.68 GB / 503.51 GB
Epoch: [21]  [540/893]  eta: 0:08:46  lr: 0.001531  min_lr: 0.000002  loss: 0.2710 (0.3083)  class_acc: 0.8750 (0.8655)  loss_scale: 8192.0000 (4716.8355)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0002  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.55 GB / 503.51 GB
Epoch: [21]  [550/893]  eta: 0:08:31  lr: 0.001531  min_lr: 0.000002  loss: 0.2761 (0.3085)  class_acc: 0.8571 (0.8654)  loss_scale: 8192.0000 (4779.9056)  weight_decay: 0.0500 (0.0500)  time: 1.4644  data: 0.0002  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.56 GB / 503.51 GB
Epoch: [21]  [560/893]  eta: 0:08:16  lr: 0.001530  min_lr: 0.000002  loss: 0.2993 (0.3082)  class_acc: 0.8571 (0.8656)  loss_scale: 8192.0000 (4840.7273)  weight_decay: 0.0500 (0.0500)  time: 1.4713  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.29 GB / 503.51 GB
Epoch: [21]  [570/893]  eta: 0:08:01  lr: 0.001529  min_lr: 0.000002  loss: 0.3135 (0.3082)  class_acc: 0.8571 (0.8652)  loss_scale: 8192.0000 (4899.4186)  weight_decay: 0.0500 (0.0500)  time: 1.4770  data: 0.0003  max mem: 31081
[2025-03-11 07:09:57,045] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 19306
[2025-03-11 07:09:57,045] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 07:09:57,046] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.82 GB / 503.51 GB
Epoch: [21]  [580/893]  eta: 0:07:46  lr: 0.001528  min_lr: 0.000002  loss: 0.3157 (0.3088)  class_acc: 0.8571 (0.8651)  loss_scale: 8192.0000 (4906.7401)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.82 GB / 503.51 GB
Epoch: [21]  [590/893]  eta: 0:07:31  lr: 0.001527  min_lr: 0.000002  loss: 0.3271 (0.3092)  class_acc: 0.8214 (0.8648)  loss_scale: 4096.0000 (4893.0220)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0004  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.97 GB / 503.51 GB
Epoch: [21]  [600/893]  eta: 0:07:16  lr: 0.001527  min_lr: 0.000002  loss: 0.3271 (0.3095)  class_acc: 0.8393 (0.8646)  loss_scale: 4096.0000 (4879.7604)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0004  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.59 GB / 503.51 GB
Epoch: [21]  [610/893]  eta: 0:07:01  lr: 0.001526  min_lr: 0.000002  loss: 0.3010 (0.3091)  class_acc: 0.8571 (0.8648)  loss_scale: 4096.0000 (4866.9329)  weight_decay: 0.0500 (0.0500)  time: 1.4669  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.60 GB / 503.51 GB
Epoch: [21]  [620/893]  eta: 0:06:46  lr: 0.001525  min_lr: 0.000002  loss: 0.2688 (0.3089)  class_acc: 0.8750 (0.8650)  loss_scale: 4096.0000 (4854.5185)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0002  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.70 GB / 503.51 GB
Epoch: [21]  [630/893]  eta: 0:06:31  lr: 0.001524  min_lr: 0.000002  loss: 0.2817 (0.3089)  class_acc: 0.8750 (0.8649)  loss_scale: 4096.0000 (4842.4976)  weight_decay: 0.0500 (0.0500)  time: 1.4800  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.30 GB / 503.51 GB
Epoch: [21]  [640/893]  eta: 0:06:16  lr: 0.001523  min_lr: 0.000002  loss: 0.3142 (0.3089)  class_acc: 0.8750 (0.8649)  loss_scale: 4096.0000 (4830.8518)  weight_decay: 0.0500 (0.0500)  time: 1.4811  data: 0.0003  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.03 GB / 503.51 GB
Epoch: [21]  [650/893]  eta: 0:06:01  lr: 0.001523  min_lr: 0.000002  loss: 0.3416 (0.3097)  class_acc: 0.8571 (0.8647)  loss_scale: 4096.0000 (4819.5637)  weight_decay: 0.0500 (0.0500)  time: 1.4696  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.66 GB / 503.51 GB
Epoch: [21]  [660/893]  eta: 0:05:46  lr: 0.001522  min_lr: 0.000002  loss: 0.3459 (0.3101)  class_acc: 0.8571 (0.8646)  loss_scale: 4096.0000 (4808.6172)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 89.88 GB / 503.51 GB
Epoch: [21]  [670/893]  eta: 0:05:31  lr: 0.001521  min_lr: 0.000002  loss: 0.3337 (0.3103)  class_acc: 0.8571 (0.8644)  loss_scale: 4096.0000 (4797.9970)  weight_decay: 0.0500 (0.0500)  time: 1.4577  data: 0.0002  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.62 GB / 503.51 GB
Epoch: [21]  [680/893]  eta: 0:05:16  lr: 0.001520  min_lr: 0.000002  loss: 0.3101 (0.3106)  class_acc: 0.8571 (0.8644)  loss_scale: 4096.0000 (4787.6887)  weight_decay: 0.0500 (0.0500)  time: 1.4558  data: 0.0002  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.67 GB / 503.51 GB
Epoch: [21]  [690/893]  eta: 0:05:01  lr: 0.001520  min_lr: 0.000002  loss: 0.2754 (0.3099)  class_acc: 0.8750 (0.8646)  loss_scale: 4096.0000 (4777.6787)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.64 GB / 503.51 GB
Epoch: [21]  [700/893]  eta: 0:04:46  lr: 0.001519  min_lr: 0.000002  loss: 0.2712 (0.3099)  class_acc: 0.8750 (0.8646)  loss_scale: 4096.0000 (4767.9544)  weight_decay: 0.0500 (0.0500)  time: 1.4664  data: 0.0003  max mem: 31081
[2025-03-11 07:13:06,372] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 07:13:06,372] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.59 GB / 503.51 GB
Epoch: [21]  [710/893]  eta: 0:04:31  lr: 0.001518  min_lr: 0.000002  loss: 0.2705 (0.3092)  class_acc: 0.8750 (0.8651)  loss_scale: 4096.0000 (4804.5907)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.75 GB / 503.51 GB
Epoch: [21]  [720/893]  eta: 0:04:16  lr: 0.001517  min_lr: 0.000002  loss: 0.2705 (0.3087)  class_acc: 0.8929 (0.8654)  loss_scale: 8192.0000 (4851.5728)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.65 GB / 503.51 GB
Epoch: [21]  [730/893]  eta: 0:04:02  lr: 0.001516  min_lr: 0.000002  loss: 0.3032 (0.3090)  class_acc: 0.8750 (0.8653)  loss_scale: 8192.0000 (4897.2695)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.56 GB / 503.51 GB
Epoch: [21]  [740/893]  eta: 0:03:47  lr: 0.001516  min_lr: 0.000002  loss: 0.3364 (0.3093)  class_acc: 0.8393 (0.8651)  loss_scale: 8192.0000 (4941.7328)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.52 GB / 503.51 GB
Epoch: [21]  [750/893]  eta: 0:03:32  lr: 0.001515  min_lr: 0.000002  loss: 0.3364 (0.3098)  class_acc: 0.8393 (0.8646)  loss_scale: 8192.0000 (4985.0120)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0002  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.77 GB / 503.51 GB
Epoch: [21]  [760/893]  eta: 0:03:17  lr: 0.001514  min_lr: 0.000002  loss: 0.3203 (0.3097)  class_acc: 0.8750 (0.8648)  loss_scale: 8192.0000 (5027.1537)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.77 GB / 503.51 GB
Epoch: [21]  [770/893]  eta: 0:03:02  lr: 0.001513  min_lr: 0.000002  loss: 0.3032 (0.3093)  class_acc: 0.8750 (0.8650)  loss_scale: 8192.0000 (5068.2023)  weight_decay: 0.0500 (0.0500)  time: 1.5005  data: 0.0003  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.73 GB / 503.51 GB
Epoch: [21]  [780/893]  eta: 0:02:47  lr: 0.001512  min_lr: 0.000002  loss: 0.2781 (0.3093)  class_acc: 0.8750 (0.8651)  loss_scale: 8192.0000 (5108.1997)  weight_decay: 0.0500 (0.0500)  time: 1.4977  data: 0.0002  max mem: 31081
[2025-03-11 07:15:02,670] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 19514
[2025-03-11 07:15:02,670] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 07:15:02,670] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.08 GB / 503.51 GB
Epoch: [21]  [790/893]  eta: 0:02:32  lr: 0.001512  min_lr: 0.000002  loss: 0.2761 (0.3088)  class_acc: 0.8750 (0.8653)  loss_scale: 8192.0000 (5100.5815)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0003  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.12 GB / 503.51 GB
Epoch: [21]  [800/893]  eta: 0:02:18  lr: 0.001511  min_lr: 0.000002  loss: 0.2651 (0.3087)  class_acc: 0.8750 (0.8653)  loss_scale: 4096.0000 (5088.0400)  weight_decay: 0.0500 (0.0500)  time: 1.4603  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.60 GB / 503.51 GB
Epoch: [21]  [810/893]  eta: 0:02:03  lr: 0.001510  min_lr: 0.000002  loss: 0.2993 (0.3090)  class_acc: 0.8750 (0.8653)  loss_scale: 4096.0000 (5075.8076)  weight_decay: 0.0500 (0.0500)  time: 1.4618  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.27 GB / 503.51 GB
Epoch: [21]  [820/893]  eta: 0:01:48  lr: 0.001509  min_lr: 0.000002  loss: 0.3000 (0.3090)  class_acc: 0.8750 (0.8655)  loss_scale: 4096.0000 (5063.8733)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.62 GB / 503.51 GB
Epoch: [21]  [830/893]  eta: 0:01:33  lr: 0.001508  min_lr: 0.000002  loss: 0.2642 (0.3087)  class_acc: 0.8929 (0.8656)  loss_scale: 4096.0000 (5052.2262)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.79 GB / 503.51 GB
Epoch: [21]  [840/893]  eta: 0:01:18  lr: 0.001508  min_lr: 0.000002  loss: 0.2961 (0.3089)  class_acc: 0.8750 (0.8655)  loss_scale: 4096.0000 (5040.8561)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.17 GB / 503.51 GB
Epoch: [21]  [850/893]  eta: 0:01:03  lr: 0.001507  min_lr: 0.000002  loss: 0.3262 (0.3097)  class_acc: 0.8393 (0.8651)  loss_scale: 4096.0000 (5029.7532)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.20 GB / 503.51 GB
Epoch: [21]  [860/893]  eta: 0:00:48  lr: 0.001506  min_lr: 0.000002  loss: 0.3088 (0.3095)  class_acc: 0.8393 (0.8651)  loss_scale: 4096.0000 (5018.9082)  weight_decay: 0.0500 (0.0500)  time: 1.4521  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.21 GB / 503.51 GB
Epoch: [21]  [870/893]  eta: 0:00:34  lr: 0.001505  min_lr: 0.000002  loss: 0.2969 (0.3095)  class_acc: 0.8750 (0.8651)  loss_scale: 4096.0000 (5008.3123)  weight_decay: 0.0500 (0.0500)  time: 1.4413  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.18 GB / 503.51 GB
Epoch: [21]  [880/893]  eta: 0:00:19  lr: 0.001504  min_lr: 0.000002  loss: 0.3079 (0.3094)  class_acc: 0.8750 (0.8653)  loss_scale: 4096.0000 (4997.9569)  weight_decay: 0.0500 (0.0500)  time: 1.4398  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.23 GB / 503.51 GB
Epoch: [21]  [890/893]  eta: 0:00:04  lr: 0.001504  min_lr: 0.000002  loss: 0.3198 (0.3097)  class_acc: 0.8750 (0.8651)  loss_scale: 4096.0000 (4987.8339)  weight_decay: 0.0500 (0.0500)  time: 1.4383  data: 0.0001  max mem: 31081
Epoch: [21]  [892/893]  eta: 0:00:01  lr: 0.001504  min_lr: 0.000002  loss: 0.3198 (0.3098)  class_acc: 0.8750 (0.8650)  loss_scale: 4096.0000 (4986.8341)  weight_decay: 0.0500 (0.0500)  time: 1.3880  data: 0.0001  max mem: 31081
Epoch: [21] Total time: 0:22:01 (1.4800 s / it)
Averaged stats: lr: 0.001504  min_lr: 0.000002  loss: 0.3198 (0.3098)  class_acc: 0.8750 (0.8650)  loss_scale: 4096.0000 (4986.8341)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:15:13  loss: 0.4461 (0.4461)  acc: 85.7143 (85.7143)  time: 11.1445  data: 10.5988  max mem: 31081
Val:  [ 10/728]  eta: 0:18:21  loss: 0.4236 (0.4714)  acc: 80.9524 (80.6277)  time: 1.5346  data: 1.0123  max mem: 31081
Val:  [ 20/728]  eta: 0:14:04  loss: 0.4181 (0.4762)  acc: 79.7619 (79.7619)  time: 0.6959  data: 0.1734  max mem: 31081
Val:  [ 30/728]  eta: 0:12:35  loss: 0.4211 (0.4742)  acc: 80.9524 (79.1859)  time: 0.8329  data: 0.3092  max mem: 31081
Val:  [ 40/728]  eta: 0:11:45  loss: 0.4400 (0.4857)  acc: 80.9524 (78.7456)  time: 0.8488  data: 0.3258  max mem: 31081
Val:  [ 50/728]  eta: 0:11:09  loss: 0.4234 (0.4750)  acc: 79.7619 (79.2717)  time: 0.8429  data: 0.3204  max mem: 31081
Val:  [ 60/728]  eta: 0:10:14  loss: 0.4888 (0.4903)  acc: 77.3810 (78.5129)  time: 0.7022  data: 0.1809  max mem: 31081
Val:  [ 70/728]  eta: 0:09:46  loss: 0.5464 (0.5042)  acc: 76.1905 (78.0181)  time: 0.6462  data: 0.1247  max mem: 31081
Val:  [ 80/728]  eta: 0:09:35  loss: 0.5665 (0.5207)  acc: 78.5714 (77.9394)  time: 0.7900  data: 0.2692  max mem: 31081
Val:  [ 90/728]  eta: 0:09:19  loss: 0.6236 (0.5558)  acc: 76.1905 (77.1586)  time: 0.8210  data: 0.2993  max mem: 31081
Val:  [100/728]  eta: 0:09:08  loss: 0.4853 (0.5395)  acc: 77.3810 (77.7110)  time: 0.8127  data: 0.2911  max mem: 31081
Val:  [110/728]  eta: 0:08:58  loss: 0.3879 (0.5428)  acc: 79.7619 (77.4024)  time: 0.8490  data: 0.3289  max mem: 31081
Val:  [120/728]  eta: 0:08:42  loss: 0.3467 (0.5299)  acc: 82.1429 (77.9122)  time: 0.7939  data: 0.2744  max mem: 31081
Val:  [130/728]  eta: 0:08:23  loss: 0.3683 (0.5349)  acc: 83.3333 (77.8717)  time: 0.6782  data: 0.1581  max mem: 31081
Val:  [140/728]  eta: 0:08:12  loss: 0.4344 (0.5395)  acc: 78.5714 (77.6933)  time: 0.6984  data: 0.1776  max mem: 31081
Val:  [150/728]  eta: 0:07:58  loss: 0.4557 (0.5427)  acc: 78.5714 (77.6963)  time: 0.7333  data: 0.2115  max mem: 31081
Val:  [160/728]  eta: 0:07:51  loss: 0.4583 (0.5456)  acc: 78.5714 (77.6471)  time: 0.7775  data: 0.2556  max mem: 31081
Val:  [170/728]  eta: 0:07:41  loss: 0.4098 (0.5434)  acc: 80.9524 (77.8126)  time: 0.8227  data: 0.2994  max mem: 31081
Val:  [180/728]  eta: 0:07:29  loss: 0.4001 (0.5407)  acc: 80.9524 (77.8216)  time: 0.7442  data: 0.2211  max mem: 31081
Val:  [190/728]  eta: 0:07:16  loss: 0.4249 (0.5354)  acc: 79.7619 (78.0042)  time: 0.6772  data: 0.1530  max mem: 31081
Val:  [200/728]  eta: 0:07:07  loss: 0.4165 (0.5296)  acc: 80.9524 (78.1568)  time: 0.7232  data: 0.1964  max mem: 31081
Val:  [210/728]  eta: 0:07:01  loss: 0.4165 (0.5351)  acc: 80.9524 (78.2047)  time: 0.8469  data: 0.3245  max mem: 31081
Val:  [220/728]  eta: 0:06:55  loss: 0.4026 (0.5310)  acc: 80.9524 (78.3775)  time: 0.8989  data: 0.3767  max mem: 31081
Val:  [230/728]  eta: 0:06:46  loss: 0.4699 (0.5353)  acc: 79.7619 (78.2107)  time: 0.8314  data: 0.3102  max mem: 31081
Val:  [240/728]  eta: 0:06:34  loss: 0.4794 (0.5339)  acc: 79.7619 (78.3343)  time: 0.7046  data: 0.1832  max mem: 31081
Val:  [250/728]  eta: 0:06:23  loss: 0.3920 (0.5342)  acc: 78.5714 (78.3770)  time: 0.6439  data: 0.1198  max mem: 31081
Val:  [260/728]  eta: 0:06:16  loss: 0.3326 (0.5330)  acc: 79.7619 (78.4939)  time: 0.7496  data: 0.2268  max mem: 31081
Val:  [270/728]  eta: 0:06:08  loss: 0.4304 (0.5344)  acc: 79.7619 (78.4484)  time: 0.8402  data: 0.3163  max mem: 31081
Val:  [280/728]  eta: 0:06:03  loss: 0.5384 (0.5342)  acc: 79.7619 (78.5375)  time: 0.9150  data: 0.3906  max mem: 31081
Val:  [290/728]  eta: 0:05:55  loss: 0.3160 (0.5292)  acc: 82.1429 (78.6901)  time: 0.9045  data: 0.3806  max mem: 31081
Val:  [300/728]  eta: 0:05:43  loss: 0.4029 (0.5295)  acc: 82.1429 (78.7138)  time: 0.6640  data: 0.1402  max mem: 31081
Val:  [310/728]  eta: 0:05:35  loss: 0.4577 (0.5308)  acc: 78.5714 (78.6710)  time: 0.6701  data: 0.1468  max mem: 31081
Val:  [320/728]  eta: 0:05:27  loss: 0.3769 (0.5287)  acc: 82.1429 (78.7309)  time: 0.7973  data: 0.2741  max mem: 31081
Val:  [330/728]  eta: 0:05:20  loss: 0.3460 (0.5242)  acc: 83.3333 (78.8664)  time: 0.8248  data: 0.3019  max mem: 31081
Val:  [340/728]  eta: 0:05:13  loss: 0.3186 (0.5192)  acc: 85.7143 (79.0427)  time: 0.8874  data: 0.3659  max mem: 31081
Val:  [350/728]  eta: 0:05:04  loss: 0.3186 (0.5191)  acc: 83.3333 (79.1107)  time: 0.8403  data: 0.3204  max mem: 31081
Val:  [360/728]  eta: 0:04:53  loss: 0.3706 (0.5154)  acc: 83.3333 (79.2475)  time: 0.6483  data: 0.1291  max mem: 31081
Val:  [370/728]  eta: 0:04:46  loss: 0.3989 (0.5192)  acc: 82.1429 (79.0945)  time: 0.6941  data: 0.1690  max mem: 31081
Val:  [380/728]  eta: 0:04:38  loss: 0.4613 (0.5208)  acc: 79.7619 (79.1026)  time: 0.8522  data: 0.3254  max mem: 31081
Val:  [390/728]  eta: 0:04:31  loss: 0.3243 (0.5156)  acc: 84.5238 (79.2656)  time: 0.8325  data: 0.3089  max mem: 31081
Val:  [400/728]  eta: 0:04:23  loss: 0.3177 (0.5146)  acc: 85.7143 (79.3700)  time: 0.8666  data: 0.3409  max mem: 31081
Val:  [410/728]  eta: 0:04:16  loss: 0.3590 (0.5145)  acc: 80.9524 (79.3303)  time: 0.8771  data: 0.3545  max mem: 31081
Val:  [420/728]  eta: 0:04:06  loss: 0.4725 (0.5133)  acc: 79.7619 (79.3886)  time: 0.6864  data: 0.1661  max mem: 31081
Val:  [430/728]  eta: 0:03:58  loss: 0.4789 (0.5141)  acc: 79.7619 (79.3642)  time: 0.6946  data: 0.1710  max mem: 31081
Val:  [440/728]  eta: 0:03:50  loss: 0.4476 (0.5127)  acc: 80.9524 (79.3705)  time: 0.8315  data: 0.3077  max mem: 31081
Val:  [450/728]  eta: 0:03:42  loss: 0.4314 (0.5153)  acc: 83.3333 (79.3501)  time: 0.8275  data: 0.3033  max mem: 31081
Val:  [460/728]  eta: 0:03:34  loss: 0.6410 (0.5192)  acc: 79.7619 (79.2067)  time: 0.8329  data: 0.3091  max mem: 31081
Val:  [470/728]  eta: 0:03:26  loss: 0.4264 (0.5172)  acc: 80.9524 (79.2842)  time: 0.7886  data: 0.2687  max mem: 31081
Val:  [480/728]  eta: 0:03:17  loss: 0.4264 (0.5169)  acc: 80.9524 (79.2768)  time: 0.6452  data: 0.1261  max mem: 31081
Val:  [490/728]  eta: 0:03:09  loss: 0.5535 (0.5176)  acc: 75.0000 (79.2236)  time: 0.6863  data: 0.1670  max mem: 31081
Val:  [500/728]  eta: 0:03:01  loss: 0.4144 (0.5186)  acc: 80.9524 (79.2558)  time: 0.8487  data: 0.3262  max mem: 31081
Val:  [510/728]  eta: 0:02:54  loss: 0.3531 (0.5153)  acc: 83.3333 (79.3589)  time: 0.8856  data: 0.3634  max mem: 31081
Val:  [520/728]  eta: 0:02:46  loss: 0.3531 (0.5181)  acc: 82.1429 (79.2546)  time: 0.8487  data: 0.3281  max mem: 31081
Val:  [530/728]  eta: 0:02:37  loss: 0.4232 (0.5173)  acc: 78.5714 (79.2956)  time: 0.7263  data: 0.2045  max mem: 31081
Val:  [540/728]  eta: 0:02:28  loss: 0.4711 (0.5192)  acc: 78.5714 (79.2096)  time: 0.6032  data: 0.0805  max mem: 31081
Val:  [550/728]  eta: 0:02:21  loss: 0.5067 (0.5216)  acc: 77.3810 (79.1656)  time: 0.6655  data: 0.1390  max mem: 31081
Val:  [560/728]  eta: 0:02:12  loss: 0.4832 (0.5216)  acc: 78.5714 (79.1762)  time: 0.7448  data: 0.2171  max mem: 31081
Val:  [570/728]  eta: 0:02:05  loss: 0.4825 (0.5280)  acc: 78.5714 (79.0968)  time: 0.7822  data: 0.2558  max mem: 31081
Val:  [580/728]  eta: 0:01:57  loss: 0.4334 (0.5274)  acc: 78.5714 (79.1349)  time: 0.8235  data: 0.2994  max mem: 31081
Val:  [590/728]  eta: 0:01:48  loss: 0.4160 (0.5291)  acc: 83.3333 (79.1012)  time: 0.7239  data: 0.2012  max mem: 31081
Val:  [600/728]  eta: 0:01:40  loss: 0.5351 (0.5307)  acc: 78.5714 (79.0330)  time: 0.6437  data: 0.1197  max mem: 31081
Val:  [610/728]  eta: 0:01:32  loss: 0.5474 (0.5327)  acc: 78.5714 (78.9533)  time: 0.6526  data: 0.1291  max mem: 31081
Val:  [620/728]  eta: 0:01:24  loss: 0.4359 (0.5358)  acc: 78.5714 (78.9069)  time: 0.7916  data: 0.2697  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.3959 (0.5344)  acc: 82.1429 (78.9582)  time: 0.8659  data: 0.3428  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3800 (0.5329)  acc: 83.3333 (78.9744)  time: 0.8010  data: 0.2755  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4182 (0.5333)  acc: 84.5238 (78.9628)  time: 0.7843  data: 0.2593  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4290 (0.5335)  acc: 78.5714 (78.9442)  time: 0.7375  data: 0.2159  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.5008 (0.5344)  acc: 76.1905 (78.9192)  time: 0.6753  data: 0.1534  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.5008 (0.5332)  acc: 77.3810 (78.9543)  time: 0.7057  data: 0.1829  max mem: 31081
Val:  [690/728]  eta: 0:00:29  loss: 0.3511 (0.5315)  acc: 83.3333 (79.0039)  time: 0.8247  data: 0.3004  max mem: 31081
Val:  [700/728]  eta: 0:00:21  loss: 0.3478 (0.5315)  acc: 83.3333 (79.0011)  time: 0.8538  data: 0.3288  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.3763 (0.5326)  acc: 76.1905 (78.9699)  time: 0.7980  data: 0.2766  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.5084 (0.5328)  acc: 77.3810 (78.9892)  time: 0.7521  data: 0.2375  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.5028 (0.5348)  acc: 79.7619 (78.9495)  time: 0.7289  data: 0.2373  max mem: 31081
Val: Total time: 0:09:28 (0.7815 s / it)
* Acc@1 78.950 AP 0.8003063201904297 loss 0.535
Accuracy of the network on the 61096 val videos: 78.9%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.58 GB / 503.51 GB
Epoch: [22]  [  0/893]  eta: 3:20:43  lr: 0.001504  min_lr: 0.000002  loss: 0.2019 (0.2019)  class_acc: 0.8929 (0.8929)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 13.4868  data: 12.1101  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.77 GB / 503.51 GB
Epoch: [22]  [ 10/893]  eta: 0:37:56  lr: 0.001503  min_lr: 0.000002  loss: 0.3674 (0.3391)  class_acc: 0.8393 (0.8442)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5787  data: 1.1014  max mem: 31081
[2025-03-11 07:27:53,708] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 07:27:53,708] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.51 GB / 503.51 GB
Epoch: [22]  [ 20/893]  eta: 0:29:59  lr: 0.001502  min_lr: 0.000002  loss: 0.3569 (0.3215)  class_acc: 0.8393 (0.8605)  loss_scale: 4096.0000 (4486.0952)  weight_decay: 0.0500 (0.0500)  time: 1.4901  data: 0.0005  max mem: 31081
[2025-03-11 07:28:04,239] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 19650
[2025-03-11 07:28:04,239] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 07:28:04,239] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.59 GB / 503.51 GB
Epoch: [22]  [ 30/893]  eta: 0:27:03  lr: 0.001501  min_lr: 0.000002  loss: 0.3015 (0.3174)  class_acc: 0.8750 (0.8664)  loss_scale: 4096.0000 (5020.9032)  weight_decay: 0.0500 (0.0500)  time: 1.4981  data: 0.0004  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.26 GB / 503.51 GB
Epoch: [22]  [ 40/893]  eta: 0:25:26  lr: 0.001500  min_lr: 0.000002  loss: 0.2849 (0.3125)  class_acc: 0.8750 (0.8650)  loss_scale: 4096.0000 (4795.3171)  weight_decay: 0.0500 (0.0500)  time: 1.5045  data: 0.0005  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.50 GB / 503.51 GB
Epoch: [22]  [ 50/893]  eta: 0:24:19  lr: 0.001500  min_lr: 0.000002  loss: 0.2815 (0.3103)  class_acc: 0.8750 (0.8648)  loss_scale: 4096.0000 (4658.1961)  weight_decay: 0.0500 (0.0500)  time: 1.4997  data: 0.0006  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.41 GB / 503.51 GB
Epoch: [22]  [ 60/893]  eta: 0:23:26  lr: 0.001499  min_lr: 0.000002  loss: 0.2681 (0.3082)  class_acc: 0.8750 (0.8677)  loss_scale: 4096.0000 (4566.0328)  weight_decay: 0.0500 (0.0500)  time: 1.4818  data: 0.0004  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.91 GB / 503.51 GB
Epoch: [22]  [ 70/893]  eta: 0:22:44  lr: 0.001498  min_lr: 0.000002  loss: 0.2683 (0.3051)  class_acc: 0.8750 (0.8692)  loss_scale: 4096.0000 (4499.8310)  weight_decay: 0.0500 (0.0500)  time: 1.4678  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.11 GB / 503.51 GB
Epoch: [22]  [ 80/893]  eta: 0:22:07  lr: 0.001497  min_lr: 0.000002  loss: 0.2786 (0.3028)  class_acc: 0.8750 (0.8690)  loss_scale: 4096.0000 (4449.9753)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0002  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.04 GB / 503.51 GB
Epoch: [22]  [ 90/893]  eta: 0:21:36  lr: 0.001496  min_lr: 0.000002  loss: 0.2854 (0.3023)  class_acc: 0.8571 (0.8689)  loss_scale: 4096.0000 (4411.0769)  weight_decay: 0.0500 (0.0500)  time: 1.4602  data: 0.0002  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.39 GB / 503.51 GB
Epoch: [22]  [100/893]  eta: 0:21:07  lr: 0.001496  min_lr: 0.000002  loss: 0.2866 (0.2999)  class_acc: 0.8750 (0.8708)  loss_scale: 4096.0000 (4379.8812)  weight_decay: 0.0500 (0.0500)  time: 1.4589  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.34 GB / 503.51 GB
Epoch: [22]  [110/893]  eta: 0:20:42  lr: 0.001495  min_lr: 0.000002  loss: 0.2827 (0.3007)  class_acc: 0.8750 (0.8708)  loss_scale: 4096.0000 (4354.3063)  weight_decay: 0.0500 (0.0500)  time: 1.4633  data: 0.0004  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.80 GB / 503.51 GB
Epoch: [22]  [120/893]  eta: 0:20:18  lr: 0.001494  min_lr: 0.000002  loss: 0.2700 (0.2974)  class_acc: 0.8750 (0.8731)  loss_scale: 4096.0000 (4332.9587)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0004  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.00 GB / 503.51 GB
Epoch: [22]  [130/893]  eta: 0:19:56  lr: 0.001493  min_lr: 0.000002  loss: 0.2817 (0.2978)  class_acc: 0.8750 (0.8730)  loss_scale: 4096.0000 (4314.8702)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.55 GB / 503.51 GB
Epoch: [22]  [140/893]  eta: 0:19:35  lr: 0.001492  min_lr: 0.000002  loss: 0.2974 (0.3003)  class_acc: 0.8571 (0.8716)  loss_scale: 4096.0000 (4299.3475)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.11 GB / 503.51 GB
Epoch: [22]  [150/893]  eta: 0:19:14  lr: 0.001492  min_lr: 0.000002  loss: 0.3242 (0.3021)  class_acc: 0.8393 (0.8710)  loss_scale: 4096.0000 (4285.8808)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0003  max mem: 31081
[2025-03-11 07:31:13,917] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 07:31:13,917] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.15 GB / 503.51 GB
Epoch: [22]  [160/893]  eta: 0:18:55  lr: 0.001491  min_lr: 0.000002  loss: 0.3242 (0.3041)  class_acc: 0.8571 (0.8701)  loss_scale: 4096.0000 (4426.7329)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.31 GB / 503.51 GB
Epoch: [22]  [170/893]  eta: 0:18:35  lr: 0.001490  min_lr: 0.000002  loss: 0.3093 (0.3053)  class_acc: 0.8571 (0.8695)  loss_scale: 8192.0000 (4646.9240)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.50 GB / 503.51 GB
Epoch: [22]  [180/893]  eta: 0:18:17  lr: 0.001489  min_lr: 0.000002  loss: 0.3137 (0.3091)  class_acc: 0.8393 (0.8665)  loss_scale: 8192.0000 (4842.7845)  weight_decay: 0.0500 (0.0500)  time: 1.4596  data: 0.0002  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.13 GB / 503.51 GB
Epoch: [22]  [190/893]  eta: 0:17:59  lr: 0.001488  min_lr: 0.000002  loss: 0.3228 (0.3102)  class_acc: 0.8393 (0.8659)  loss_scale: 8192.0000 (5018.1361)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0002  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.28 GB / 503.51 GB
Epoch: [22]  [200/893]  eta: 0:17:41  lr: 0.001488  min_lr: 0.000002  loss: 0.3118 (0.3102)  class_acc: 0.8750 (0.8665)  loss_scale: 8192.0000 (5176.0398)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0002  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.22 GB / 503.51 GB
Epoch: [22]  [210/893]  eta: 0:17:23  lr: 0.001487  min_lr: 0.000002  loss: 0.2952 (0.3091)  class_acc: 0.8750 (0.8668)  loss_scale: 8192.0000 (5318.9763)  weight_decay: 0.0500 (0.0500)  time: 1.4663  data: 0.0003  max mem: 31081
[2025-03-11 07:32:40,336] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 19838
[2025-03-11 07:32:40,337] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 07:32:40,337] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.07 GB / 503.51 GB
Epoch: [22]  [220/893]  eta: 0:17:07  lr: 0.001486  min_lr: 0.000002  loss: 0.2590 (0.3063)  class_acc: 0.8929 (0.8682)  loss_scale: 8192.0000 (5319.2398)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.65 GB / 503.51 GB
Epoch: [22]  [230/893]  eta: 0:16:50  lr: 0.001485  min_lr: 0.000002  loss: 0.2805 (0.3053)  class_acc: 0.8750 (0.8686)  loss_scale: 4096.0000 (5266.2857)  weight_decay: 0.0500 (0.0500)  time: 1.4713  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.17 GB / 503.51 GB
Epoch: [22]  [240/893]  eta: 0:16:33  lr: 0.001484  min_lr: 0.000002  loss: 0.2988 (0.3047)  class_acc: 0.8750 (0.8685)  loss_scale: 4096.0000 (5217.7261)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.18 GB / 503.51 GB
Epoch: [22]  [250/893]  eta: 0:16:16  lr: 0.001484  min_lr: 0.000002  loss: 0.2947 (0.3046)  class_acc: 0.8750 (0.8687)  loss_scale: 4096.0000 (5173.0359)  weight_decay: 0.0500 (0.0500)  time: 1.4608  data: 0.0004  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.25 GB / 503.51 GB
Epoch: [22]  [260/893]  eta: 0:16:00  lr: 0.001483  min_lr: 0.000002  loss: 0.2974 (0.3040)  class_acc: 0.8571 (0.8682)  loss_scale: 4096.0000 (5131.7701)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0004  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.30 GB / 503.51 GB
Epoch: [22]  [270/893]  eta: 0:15:43  lr: 0.001482  min_lr: 0.000002  loss: 0.2917 (0.3033)  class_acc: 0.8571 (0.8682)  loss_scale: 4096.0000 (5093.5498)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.32 GB / 503.51 GB
Epoch: [22]  [280/893]  eta: 0:15:27  lr: 0.001481  min_lr: 0.000002  loss: 0.2695 (0.3030)  class_acc: 0.8750 (0.8683)  loss_scale: 4096.0000 (5058.0498)  weight_decay: 0.0500 (0.0500)  time: 1.4666  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.68 GB / 503.51 GB
Epoch: [22]  [290/893]  eta: 0:15:11  lr: 0.001480  min_lr: 0.000002  loss: 0.2449 (0.3021)  class_acc: 0.8929 (0.8687)  loss_scale: 4096.0000 (5024.9897)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.09 GB / 503.51 GB
Epoch: [22]  [300/893]  eta: 0:14:55  lr: 0.001480  min_lr: 0.000002  loss: 0.2551 (0.3021)  class_acc: 0.8750 (0.8688)  loss_scale: 4096.0000 (4994.1262)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0005  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.46 GB / 503.51 GB
Epoch: [22]  [310/893]  eta: 0:14:39  lr: 0.001479  min_lr: 0.000002  loss: 0.3149 (0.3031)  class_acc: 0.8571 (0.8682)  loss_scale: 4096.0000 (4965.2476)  weight_decay: 0.0500 (0.0500)  time: 1.4766  data: 0.0004  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.66 GB / 503.51 GB
Epoch: [22]  [320/893]  eta: 0:14:23  lr: 0.001478  min_lr: 0.000002  loss: 0.3159 (0.3039)  class_acc: 0.8393 (0.8676)  loss_scale: 4096.0000 (4938.1682)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0002  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.59 GB / 503.51 GB
Epoch: [22]  [330/893]  eta: 0:14:08  lr: 0.001477  min_lr: 0.000002  loss: 0.3313 (0.3051)  class_acc: 0.8393 (0.8671)  loss_scale: 4096.0000 (4912.7251)  weight_decay: 0.0500 (0.0500)  time: 1.4695  data: 0.0002  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.31 GB / 503.51 GB
Epoch: [22]  [340/893]  eta: 0:13:52  lr: 0.001476  min_lr: 0.000002  loss: 0.3464 (0.3059)  class_acc: 0.8393 (0.8666)  loss_scale: 4096.0000 (4888.7742)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0002  max mem: 31081
[2025-03-11 07:35:49,581] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 07:35:49,581] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.19 GB / 503.51 GB
Epoch: [22]  [350/893]  eta: 0:13:36  lr: 0.001476  min_lr: 0.000002  loss: 0.3347 (0.3063)  class_acc: 0.8393 (0.8664)  loss_scale: 4096.0000 (4959.5442)  weight_decay: 0.0500 (0.0500)  time: 1.4596  data: 0.0002  max mem: 31081
[2025-03-11 07:36:08,665] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 19980
[2025-03-11 07:36:08,666] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 07:36:08,666] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.85 GB / 503.51 GB
Epoch: [22]  [360/893]  eta: 0:13:21  lr: 0.001475  min_lr: 0.000002  loss: 0.3186 (0.3065)  class_acc: 0.8571 (0.8664)  loss_scale: 8192.0000 (4992.3546)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.87 GB / 503.51 GB
Epoch: [22]  [370/893]  eta: 0:13:05  lr: 0.001474  min_lr: 0.000002  loss: 0.2539 (0.3049)  class_acc: 0.8929 (0.8674)  loss_scale: 4096.0000 (4968.1941)  weight_decay: 0.0500 (0.0500)  time: 1.4636  data: 0.0003  max mem: 31081
[2025-03-11 07:36:36,458] [INFO] [logging.py:129:log_dist] [Rank 0] step=20000, skipped=118, lr=[1.9246387849903897e-06, 1.9246387849903897e-06, 3.2077313083173166e-06, 3.2077313083173166e-06, 5.346218847195527e-06, 5.346218847195527e-06, 8.91036474532588e-06, 8.91036474532588e-06, 1.4850607908876468e-05, 1.4850607908876468e-05, 2.475101318146078e-05, 2.475101318146078e-05, 4.1251688635767965e-05, 4.1251688635767965e-05, 6.875281439294662e-05, 6.875281439294662e-05, 0.00011458802398824436, 0.00011458802398824436, 0.0001909800399804073, 0.0001909800399804073, 0.00031830006663401215, 0.00031830006663401215, 0.0005305001110566869, 0.0005305001110566869, 0.0008841668517611449, 0.0008841668517611449, 0.0014736114196019082, 0.0014736114196019082], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 07:36:36,459] [INFO] [timer.py:264:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=60.994021454958315, CurrSamplesPerSec=59.03993964527012, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.87 GB / 503.51 GB
Epoch: [22]  [380/893]  eta: 0:12:49  lr: 0.001473  min_lr: 0.000002  loss: 0.2759 (0.3059)  class_acc: 0.8750 (0.8668)  loss_scale: 4096.0000 (4945.3018)  weight_decay: 0.0500 (0.0500)  time: 1.4651  data: 0.0003  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.35 GB / 503.51 GB
Epoch: [22]  [390/893]  eta: 0:12:34  lr: 0.001472  min_lr: 0.000002  loss: 0.3044 (0.3062)  class_acc: 0.8571 (0.8667)  loss_scale: 4096.0000 (4923.5806)  weight_decay: 0.0500 (0.0500)  time: 1.4628  data: 0.0003  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.40 GB / 503.51 GB
Epoch: [22]  [400/893]  eta: 0:12:18  lr: 0.001472  min_lr: 0.000002  loss: 0.2803 (0.3054)  class_acc: 0.8750 (0.8670)  loss_scale: 4096.0000 (4902.9426)  weight_decay: 0.0500 (0.0500)  time: 1.4569  data: 0.0002  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.78 GB / 503.51 GB
Epoch: [22]  [410/893]  eta: 0:12:03  lr: 0.001471  min_lr: 0.000002  loss: 0.2742 (0.3058)  class_acc: 0.8750 (0.8669)  loss_scale: 4096.0000 (4883.3090)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0002  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.56 GB / 503.51 GB
Epoch: [22]  [420/893]  eta: 0:11:48  lr: 0.001470  min_lr: 0.000002  loss: 0.3093 (0.3065)  class_acc: 0.8571 (0.8666)  loss_scale: 4096.0000 (4864.6081)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0003  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.46 GB / 503.51 GB
Epoch: [22]  [430/893]  eta: 0:11:32  lr: 0.001469  min_lr: 0.000002  loss: 0.3101 (0.3071)  class_acc: 0.8571 (0.8664)  loss_scale: 4096.0000 (4846.7749)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0002  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.03 GB / 503.51 GB
Epoch: [22]  [440/893]  eta: 0:11:17  lr: 0.001468  min_lr: 0.000002  loss: 0.3525 (0.3080)  class_acc: 0.8571 (0.8658)  loss_scale: 4096.0000 (4829.7506)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.54 GB / 503.51 GB
Epoch: [22]  [450/893]  eta: 0:11:02  lr: 0.001468  min_lr: 0.000002  loss: 0.3018 (0.3077)  class_acc: 0.8571 (0.8659)  loss_scale: 4096.0000 (4813.4812)  weight_decay: 0.0500 (0.0500)  time: 1.4709  data: 0.0003  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.56 GB / 503.51 GB
Epoch: [22]  [460/893]  eta: 0:10:47  lr: 0.001467  min_lr: 0.000002  loss: 0.3008 (0.3079)  class_acc: 0.8750 (0.8660)  loss_scale: 4096.0000 (4797.9176)  weight_decay: 0.0500 (0.0500)  time: 1.4729  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.84 GB / 503.51 GB
Epoch: [22]  [470/893]  eta: 0:10:31  lr: 0.001466  min_lr: 0.000002  loss: 0.2817 (0.3080)  class_acc: 0.8750 (0.8660)  loss_scale: 4096.0000 (4783.0149)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.97 GB / 503.51 GB
Epoch: [22]  [480/893]  eta: 0:10:16  lr: 0.001465  min_lr: 0.000002  loss: 0.2708 (0.3078)  class_acc: 0.8750 (0.8663)  loss_scale: 4096.0000 (4768.7318)  weight_decay: 0.0500 (0.0500)  time: 1.4658  data: 0.0002  max mem: 31081
[2025-03-11 07:39:17,613] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 07:39:17,613] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.68 GB / 503.51 GB
Epoch: [22]  [490/893]  eta: 0:10:01  lr: 0.001464  min_lr: 0.000002  loss: 0.2952 (0.3080)  class_acc: 0.8571 (0.8661)  loss_scale: 4096.0000 (4805.0835)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0002  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.47 GB / 503.51 GB
Epoch: [22]  [500/893]  eta: 0:09:46  lr: 0.001464  min_lr: 0.000002  loss: 0.3071 (0.3081)  class_acc: 0.8571 (0.8660)  loss_scale: 8192.0000 (4872.6866)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.16 GB / 503.51 GB
Epoch: [22]  [510/893]  eta: 0:09:31  lr: 0.001463  min_lr: 0.000002  loss: 0.2861 (0.3082)  class_acc: 0.8750 (0.8659)  loss_scale: 8192.0000 (4937.6438)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.52 GB / 503.51 GB
Epoch: [22]  [520/893]  eta: 0:09:16  lr: 0.001462  min_lr: 0.000002  loss: 0.2971 (0.3087)  class_acc: 0.8750 (0.8658)  loss_scale: 8192.0000 (5000.1075)  weight_decay: 0.0500 (0.0500)  time: 1.4728  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.84 GB / 503.51 GB
Epoch: [22]  [530/893]  eta: 0:09:01  lr: 0.001461  min_lr: 0.000002  loss: 0.3262 (0.3090)  class_acc: 0.8571 (0.8658)  loss_scale: 8192.0000 (5060.2185)  weight_decay: 0.0500 (0.0500)  time: 1.4658  data: 0.0003  max mem: 31081
[2025-03-11 07:40:27,986] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 20157
[2025-03-11 07:40:27,986] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 07:40:27,986] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.91 GB / 503.51 GB
Epoch: [22]  [540/893]  eta: 0:08:46  lr: 0.001460  min_lr: 0.000002  loss: 0.3262 (0.3089)  class_acc: 0.8750 (0.8658)  loss_scale: 8192.0000 (5057.5379)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.65 GB / 503.51 GB
Epoch: [22]  [550/893]  eta: 0:08:31  lr: 0.001460  min_lr: 0.000002  loss: 0.3020 (0.3091)  class_acc: 0.8750 (0.8658)  loss_scale: 4096.0000 (5040.0871)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0002  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.15 GB / 503.51 GB
Epoch: [22]  [560/893]  eta: 0:08:16  lr: 0.001459  min_lr: 0.000002  loss: 0.2957 (0.3089)  class_acc: 0.8750 (0.8659)  loss_scale: 4096.0000 (5023.2585)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0002  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.80 GB / 503.51 GB
Epoch: [22]  [570/893]  eta: 0:08:01  lr: 0.001458  min_lr: 0.000002  loss: 0.2817 (0.3082)  class_acc: 0.8750 (0.8658)  loss_scale: 4096.0000 (5007.0193)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0002  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.85 GB / 503.51 GB
Epoch: [22]  [580/893]  eta: 0:07:45  lr: 0.001457  min_lr: 0.000002  loss: 0.2773 (0.3081)  class_acc: 0.8571 (0.8658)  loss_scale: 4096.0000 (4991.3391)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.15 GB / 503.51 GB
Epoch: [22]  [590/893]  eta: 0:07:31  lr: 0.001456  min_lr: 0.000002  loss: 0.2993 (0.3089)  class_acc: 0.8393 (0.8656)  loss_scale: 4096.0000 (4976.1895)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0002  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.81 GB / 503.51 GB
Epoch: [22]  [600/893]  eta: 0:07:16  lr: 0.001455  min_lr: 0.000002  loss: 0.3000 (0.3091)  class_acc: 0.8571 (0.8655)  loss_scale: 4096.0000 (4961.5441)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0003  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.68 GB / 503.51 GB
Epoch: [22]  [610/893]  eta: 0:07:01  lr: 0.001455  min_lr: 0.000002  loss: 0.3030 (0.3088)  class_acc: 0.8750 (0.8656)  loss_scale: 4096.0000 (4947.3781)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0002  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.05 GB / 503.51 GB
Epoch: [22]  [620/893]  eta: 0:06:46  lr: 0.001454  min_lr: 0.000002  loss: 0.2786 (0.3081)  class_acc: 0.8750 (0.8659)  loss_scale: 4096.0000 (4933.6683)  weight_decay: 0.0500 (0.0500)  time: 1.4730  data: 0.0002  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.73 GB / 503.51 GB
Epoch: [22]  [630/893]  eta: 0:06:31  lr: 0.001453  min_lr: 0.000002  loss: 0.2954 (0.3091)  class_acc: 0.8571 (0.8654)  loss_scale: 4096.0000 (4920.3930)  weight_decay: 0.0500 (0.0500)  time: 1.4741  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.65 GB / 503.51 GB
Epoch: [22]  [640/893]  eta: 0:06:16  lr: 0.001452  min_lr: 0.000002  loss: 0.3462 (0.3092)  class_acc: 0.8571 (0.8654)  loss_scale: 4096.0000 (4907.5320)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.98 GB / 503.51 GB
Epoch: [22]  [650/893]  eta: 0:06:01  lr: 0.001451  min_lr: 0.000002  loss: 0.2922 (0.3089)  class_acc: 0.8750 (0.8657)  loss_scale: 4096.0000 (4895.0661)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.09 GB / 503.51 GB
Epoch: [22]  [660/893]  eta: 0:05:46  lr: 0.001451  min_lr: 0.000002  loss: 0.2915 (0.3088)  class_acc: 0.8750 (0.8656)  loss_scale: 4096.0000 (4882.9773)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0003  max mem: 31081
[2025-03-11 07:43:37,248] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 07:43:37,248] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.19 GB / 503.51 GB
Epoch: [22]  [670/893]  eta: 0:05:31  lr: 0.001450  min_lr: 0.000002  loss: 0.3003 (0.3090)  class_acc: 0.8571 (0.8654)  loss_scale: 4096.0000 (4926.1878)  weight_decay: 0.0500 (0.0500)  time: 1.4554  data: 0.0002  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.68 GB / 503.51 GB
Epoch: [22]  [680/893]  eta: 0:05:16  lr: 0.001449  min_lr: 0.000002  loss: 0.2949 (0.3087)  class_acc: 0.8571 (0.8656)  loss_scale: 8192.0000 (4974.1439)  weight_decay: 0.0500 (0.0500)  time: 1.4588  data: 0.0002  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.22 GB / 503.51 GB
Epoch: [22]  [690/893]  eta: 0:05:01  lr: 0.001448  min_lr: 0.000002  loss: 0.2681 (0.3086)  class_acc: 0.8571 (0.8657)  loss_scale: 8192.0000 (5020.7120)  weight_decay: 0.0500 (0.0500)  time: 1.4619  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.92 GB / 503.51 GB
Epoch: [22]  [700/893]  eta: 0:04:46  lr: 0.001447  min_lr: 0.000002  loss: 0.2444 (0.3084)  class_acc: 0.8750 (0.8658)  loss_scale: 8192.0000 (5065.9515)  weight_decay: 0.0500 (0.0500)  time: 1.4694  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.35 GB / 503.51 GB
Epoch: [22]  [710/893]  eta: 0:04:31  lr: 0.001447  min_lr: 0.000002  loss: 0.3428 (0.3088)  class_acc: 0.8571 (0.8656)  loss_scale: 8192.0000 (5109.9184)  weight_decay: 0.0500 (0.0500)  time: 1.4736  data: 0.0004  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.93 GB / 503.51 GB
Epoch: [22]  [720/893]  eta: 0:04:16  lr: 0.001446  min_lr: 0.000002  loss: 0.2766 (0.3082)  class_acc: 0.8571 (0.8658)  loss_scale: 8192.0000 (5152.6657)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.45 GB / 503.51 GB
Epoch: [22]  [730/893]  eta: 0:04:01  lr: 0.001445  min_lr: 0.000002  loss: 0.2454 (0.3080)  class_acc: 0.8750 (0.8658)  loss_scale: 8192.0000 (5194.2435)  weight_decay: 0.0500 (0.0500)  time: 1.4632  data: 0.0002  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.62 GB / 503.51 GB
Epoch: [22]  [740/893]  eta: 0:03:47  lr: 0.001444  min_lr: 0.000002  loss: 0.2993 (0.3080)  class_acc: 0.8750 (0.8659)  loss_scale: 8192.0000 (5234.6991)  weight_decay: 0.0500 (0.0500)  time: 1.4649  data: 0.0002  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.81 GB / 503.51 GB
Epoch: [22]  [750/893]  eta: 0:03:32  lr: 0.001443  min_lr: 0.000002  loss: 0.3054 (0.3081)  class_acc: 0.8571 (0.8658)  loss_scale: 8192.0000 (5274.0772)  weight_decay: 0.0500 (0.0500)  time: 1.4687  data: 0.0002  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.95 GB / 503.51 GB
Epoch: [22]  [760/893]  eta: 0:03:17  lr: 0.001443  min_lr: 0.000002  loss: 0.2917 (0.3079)  class_acc: 0.8571 (0.8660)  loss_scale: 8192.0000 (5312.4205)  weight_decay: 0.0500 (0.0500)  time: 1.4743  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.03 GB / 503.51 GB
Epoch: [22]  [770/893]  eta: 0:03:02  lr: 0.001442  min_lr: 0.000002  loss: 0.2922 (0.3078)  class_acc: 0.8750 (0.8659)  loss_scale: 8192.0000 (5349.7691)  weight_decay: 0.0500 (0.0500)  time: 1.4785  data: 0.0003  max mem: 31081
[2025-03-11 07:46:20,219] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 20397
[2025-03-11 07:46:20,219] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 07:46:20,219] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.29 GB / 503.51 GB
Epoch: [22]  [780/893]  eta: 0:02:47  lr: 0.001441  min_lr: 0.000002  loss: 0.2922 (0.3079)  class_acc: 0.8750 (0.8659)  loss_scale: 8192.0000 (5344.2049)  weight_decay: 0.0500 (0.0500)  time: 1.4689  data: 0.0003  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.76 GB / 503.51 GB
Epoch: [22]  [790/893]  eta: 0:02:32  lr: 0.001440  min_lr: 0.000002  loss: 0.2930 (0.3080)  class_acc: 0.8750 (0.8660)  loss_scale: 4096.0000 (5328.4248)  weight_decay: 0.0500 (0.0500)  time: 1.4612  data: 0.0002  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.04 GB / 503.51 GB
Epoch: [22]  [800/893]  eta: 0:02:17  lr: 0.001439  min_lr: 0.000002  loss: 0.3037 (0.3082)  class_acc: 0.8571 (0.8656)  loss_scale: 4096.0000 (5313.0387)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.75 GB / 503.51 GB
Epoch: [22]  [810/893]  eta: 0:02:03  lr: 0.001438  min_lr: 0.000002  loss: 0.2795 (0.3078)  class_acc: 0.8571 (0.8656)  loss_scale: 4096.0000 (5298.0321)  weight_decay: 0.0500 (0.0500)  time: 1.4677  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.85 GB / 503.51 GB
Epoch: [22]  [820/893]  eta: 0:01:48  lr: 0.001438  min_lr: 0.000002  loss: 0.2773 (0.3077)  class_acc: 0.8750 (0.8656)  loss_scale: 4096.0000 (5283.3910)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0002  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.04 GB / 503.51 GB
Epoch: [22]  [830/893]  eta: 0:01:33  lr: 0.001437  min_lr: 0.000002  loss: 0.3000 (0.3079)  class_acc: 0.8750 (0.8655)  loss_scale: 4096.0000 (5269.1023)  weight_decay: 0.0500 (0.0500)  time: 1.4585  data: 0.0004  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.91 GB / 503.51 GB
Epoch: [22]  [840/893]  eta: 0:01:18  lr: 0.001436  min_lr: 0.000002  loss: 0.3000 (0.3079)  class_acc: 0.8571 (0.8656)  loss_scale: 4096.0000 (5255.1534)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0005  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.28 GB / 503.51 GB
Epoch: [22]  [850/893]  eta: 0:01:03  lr: 0.001435  min_lr: 0.000002  loss: 0.3198 (0.3086)  class_acc: 0.8571 (0.8653)  loss_scale: 4096.0000 (5241.5323)  weight_decay: 0.0500 (0.0500)  time: 1.4582  data: 0.0004  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.37 GB / 503.51 GB
Epoch: [22]  [860/893]  eta: 0:00:48  lr: 0.001434  min_lr: 0.000002  loss: 0.3330 (0.3089)  class_acc: 0.8393 (0.8652)  loss_scale: 4096.0000 (5228.2276)  weight_decay: 0.0500 (0.0500)  time: 1.4439  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.35 GB / 503.51 GB
Epoch: [22]  [870/893]  eta: 0:00:34  lr: 0.001434  min_lr: 0.000002  loss: 0.3184 (0.3090)  class_acc: 0.8393 (0.8650)  loss_scale: 4096.0000 (5215.2285)  weight_decay: 0.0500 (0.0500)  time: 1.4372  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.34 GB / 503.51 GB
Epoch: [22]  [880/893]  eta: 0:00:19  lr: 0.001433  min_lr: 0.000002  loss: 0.2849 (0.3085)  class_acc: 0.8750 (0.8653)  loss_scale: 4096.0000 (5202.5244)  weight_decay: 0.0500 (0.0500)  time: 1.4352  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.37 GB / 503.51 GB
Epoch: [22]  [890/893]  eta: 0:00:04  lr: 0.001432  min_lr: 0.000002  loss: 0.2671 (0.3082)  class_acc: 0.8750 (0.8653)  loss_scale: 4096.0000 (5190.1055)  weight_decay: 0.0500 (0.0500)  time: 1.4351  data: 0.0001  max mem: 31081
Epoch: [22]  [892/893]  eta: 0:00:01  lr: 0.001432  min_lr: 0.000002  loss: 0.2671 (0.3082)  class_acc: 0.8750 (0.8653)  loss_scale: 4096.0000 (5188.8789)  weight_decay: 0.0500 (0.0500)  time: 1.3862  data: 0.0001  max mem: 31081
Epoch: [22] Total time: 0:22:00 (1.4785 s / it)
Averaged stats: lr: 0.001432  min_lr: 0.000002  loss: 0.2671 (0.3082)  class_acc: 0.8750 (0.8653)  loss_scale: 4096.0000 (5188.8789)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:17:28  loss: 1.3938 (1.3938)  acc: 69.0476 (69.0476)  time: 11.3299  data: 10.7872  max mem: 31081
Val:  [ 10/728]  eta: 0:18:16  loss: 0.4517 (0.6217)  acc: 78.5714 (78.2468)  time: 1.5265  data: 1.0031  max mem: 31081
Val:  [ 20/728]  eta: 0:14:07  loss: 0.4279 (0.6123)  acc: 78.5714 (77.9478)  time: 0.6900  data: 0.1697  max mem: 31081
Val:  [ 30/728]  eta: 0:12:35  loss: 0.3890 (0.5614)  acc: 77.3810 (78.2642)  time: 0.8384  data: 0.3178  max mem: 31081
Val:  [ 40/728]  eta: 0:11:47  loss: 0.3982 (0.5698)  acc: 78.5714 (78.0197)  time: 0.8522  data: 0.3299  max mem: 31081
Val:  [ 50/728]  eta: 0:11:12  loss: 0.4195 (0.5488)  acc: 80.9524 (78.6648)  time: 0.8496  data: 0.3268  max mem: 31081
Val:  [ 60/728]  eta: 0:10:18  loss: 0.4653 (0.5656)  acc: 78.5714 (78.0055)  time: 0.7130  data: 0.1904  max mem: 31081
Val:  [ 70/728]  eta: 0:09:48  loss: 0.4671 (0.5727)  acc: 76.1905 (77.3977)  time: 0.6469  data: 0.1252  max mem: 31081
Val:  [ 80/728]  eta: 0:09:36  loss: 0.4848 (0.5914)  acc: 75.0000 (76.8959)  time: 0.7789  data: 0.2584  max mem: 31081
Val:  [ 90/728]  eta: 0:09:19  loss: 0.6600 (0.6134)  acc: 75.0000 (76.2428)  time: 0.8182  data: 0.2981  max mem: 31081
Val:  [100/728]  eta: 0:09:08  loss: 0.5847 (0.5954)  acc: 76.1905 (76.8623)  time: 0.8115  data: 0.2902  max mem: 31081
Val:  [110/728]  eta: 0:08:58  loss: 0.4085 (0.5907)  acc: 80.9524 (77.0270)  time: 0.8444  data: 0.3205  max mem: 31081
Val:  [120/728]  eta: 0:08:48  loss: 0.3765 (0.5741)  acc: 83.3333 (77.7155)  time: 0.8425  data: 0.3201  max mem: 31081
Val:  [130/728]  eta: 0:08:23  loss: 0.3700 (0.5799)  acc: 83.3333 (77.7172)  time: 0.6774  data: 0.1585  max mem: 31081
Val:  [140/728]  eta: 0:08:13  loss: 0.4340 (0.5774)  acc: 79.7619 (77.6511)  time: 0.6589  data: 0.1394  max mem: 31081
Val:  [150/728]  eta: 0:08:00  loss: 0.4919 (0.5918)  acc: 75.0000 (77.3573)  time: 0.7624  data: 0.2418  max mem: 31081
Val:  [160/728]  eta: 0:07:52  loss: 0.4746 (0.5949)  acc: 77.3810 (77.2996)  time: 0.7848  data: 0.2613  max mem: 31081
Val:  [170/728]  eta: 0:07:42  loss: 0.4534 (0.5894)  acc: 78.5714 (77.5063)  time: 0.8123  data: 0.2884  max mem: 31081
Val:  [180/728]  eta: 0:07:32  loss: 0.4784 (0.5887)  acc: 78.5714 (77.5914)  time: 0.7664  data: 0.2446  max mem: 31081
Val:  [190/728]  eta: 0:07:17  loss: 0.4784 (0.5851)  acc: 77.3810 (77.6677)  time: 0.6679  data: 0.1453  max mem: 31081
Val:  [200/728]  eta: 0:07:10  loss: 0.4448 (0.5789)  acc: 78.5714 (77.7659)  time: 0.7379  data: 0.2156  max mem: 31081
Val:  [210/728]  eta: 0:07:03  loss: 0.3989 (0.5884)  acc: 78.5714 (77.6292)  time: 0.8617  data: 0.3397  max mem: 31081
Val:  [220/728]  eta: 0:06:55  loss: 0.4112 (0.5862)  acc: 79.7619 (77.6772)  time: 0.8424  data: 0.3184  max mem: 31081
Val:  [230/728]  eta: 0:06:47  loss: 0.4935 (0.5876)  acc: 79.7619 (77.6335)  time: 0.8321  data: 0.3081  max mem: 31081
Val:  [240/728]  eta: 0:06:37  loss: 0.5884 (0.5890)  acc: 79.7619 (77.7663)  time: 0.7630  data: 0.2420  max mem: 31081
Val:  [250/728]  eta: 0:06:25  loss: 0.5017 (0.5856)  acc: 77.3810 (77.9216)  time: 0.6663  data: 0.1450  max mem: 31081
Val:  [260/728]  eta: 0:06:17  loss: 0.3439 (0.5842)  acc: 82.1429 (77.9876)  time: 0.7195  data: 0.1948  max mem: 31081
Val:  [270/728]  eta: 0:06:09  loss: 0.3547 (0.5820)  acc: 80.9524 (78.0004)  time: 0.8263  data: 0.3018  max mem: 31081
Val:  [280/728]  eta: 0:06:04  loss: 0.4935 (0.5820)  acc: 79.7619 (78.0927)  time: 0.9065  data: 0.3851  max mem: 31081
Val:  [290/728]  eta: 0:05:56  loss: 0.4204 (0.5801)  acc: 80.9524 (78.1951)  time: 0.8955  data: 0.3758  max mem: 31081
Val:  [300/728]  eta: 0:05:44  loss: 0.4411 (0.5784)  acc: 80.9524 (78.2352)  time: 0.6649  data: 0.1456  max mem: 31081
Val:  [310/728]  eta: 0:05:36  loss: 0.5405 (0.5795)  acc: 77.3810 (78.1657)  time: 0.6702  data: 0.1526  max mem: 31081
Val:  [320/728]  eta: 0:05:28  loss: 0.4873 (0.5763)  acc: 80.9524 (78.2599)  time: 0.8048  data: 0.2857  max mem: 31081
Val:  [330/728]  eta: 0:05:21  loss: 0.3991 (0.5706)  acc: 82.1429 (78.4168)  time: 0.8392  data: 0.3171  max mem: 31081
Val:  [340/728]  eta: 0:05:14  loss: 0.3716 (0.5636)  acc: 84.5238 (78.6797)  time: 0.8955  data: 0.3715  max mem: 31081
Val:  [350/728]  eta: 0:05:05  loss: 0.3614 (0.5629)  acc: 86.9048 (78.7885)  time: 0.8442  data: 0.3216  max mem: 31081
Val:  [360/728]  eta: 0:04:54  loss: 0.3697 (0.5589)  acc: 82.1429 (78.8254)  time: 0.6548  data: 0.1342  max mem: 31081
Val:  [370/728]  eta: 0:04:47  loss: 0.4162 (0.5618)  acc: 80.9524 (78.7415)  time: 0.6878  data: 0.1659  max mem: 31081
Val:  [380/728]  eta: 0:04:39  loss: 0.4772 (0.5655)  acc: 79.7619 (78.6495)  time: 0.8485  data: 0.3262  max mem: 31081
Val:  [390/728]  eta: 0:04:31  loss: 0.3831 (0.5614)  acc: 82.1429 (78.8059)  time: 0.8330  data: 0.3101  max mem: 31081
Val:  [400/728]  eta: 0:04:24  loss: 0.3831 (0.5625)  acc: 80.9524 (78.8416)  time: 0.8537  data: 0.3331  max mem: 31081
Val:  [410/728]  eta: 0:04:16  loss: 0.4286 (0.5640)  acc: 79.7619 (78.8263)  time: 0.8729  data: 0.3540  max mem: 31081
Val:  [420/728]  eta: 0:04:06  loss: 0.4557 (0.5627)  acc: 79.7619 (78.8910)  time: 0.6912  data: 0.1706  max mem: 31081
Val:  [430/728]  eta: 0:03:58  loss: 0.4558 (0.5612)  acc: 79.7619 (78.9001)  time: 0.6969  data: 0.1742  max mem: 31081
Val:  [440/728]  eta: 0:03:50  loss: 0.4571 (0.5598)  acc: 79.7619 (78.9089)  time: 0.8304  data: 0.3069  max mem: 31081
Val:  [450/728]  eta: 0:03:43  loss: 0.4255 (0.5584)  acc: 82.1429 (78.9753)  time: 0.8266  data: 0.3009  max mem: 31081
Val:  [460/728]  eta: 0:03:35  loss: 0.4327 (0.5640)  acc: 80.9524 (78.8503)  time: 0.8372  data: 0.3139  max mem: 31081
Val:  [470/728]  eta: 0:03:27  loss: 0.4385 (0.5622)  acc: 75.0000 (78.8848)  time: 0.7948  data: 0.2752  max mem: 31081
Val:  [480/728]  eta: 0:03:17  loss: 0.4278 (0.5606)  acc: 79.7619 (78.9278)  time: 0.6519  data: 0.1294  max mem: 31081
Val:  [490/728]  eta: 0:03:09  loss: 0.3725 (0.5596)  acc: 79.7619 (78.9715)  time: 0.6827  data: 0.1605  max mem: 31081
Val:  [500/728]  eta: 0:03:02  loss: 0.3636 (0.5606)  acc: 80.9524 (78.9920)  time: 0.8401  data: 0.3194  max mem: 31081
Val:  [510/728]  eta: 0:02:54  loss: 0.3624 (0.5573)  acc: 83.3333 (79.0513)  time: 0.8856  data: 0.3630  max mem: 31081
Val:  [520/728]  eta: 0:02:46  loss: 0.3698 (0.5599)  acc: 82.1429 (78.9713)  time: 0.8574  data: 0.3342  max mem: 31081
Val:  [530/728]  eta: 0:02:38  loss: 0.5702 (0.5595)  acc: 78.5714 (78.9929)  time: 0.7500  data: 0.2299  max mem: 31081
Val:  [540/728]  eta: 0:02:29  loss: 0.5444 (0.5627)  acc: 77.3810 (78.8619)  time: 0.6212  data: 0.0999  max mem: 31081
Val:  [550/728]  eta: 0:02:21  loss: 0.6000 (0.5674)  acc: 72.6190 (78.7183)  time: 0.6658  data: 0.1421  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.5518 (0.5676)  acc: 76.1905 (78.7157)  time: 0.7652  data: 0.2445  max mem: 31081
Val:  [570/728]  eta: 0:02:05  loss: 0.4707 (0.5743)  acc: 77.3810 (78.6548)  time: 0.8058  data: 0.2858  max mem: 31081
Val:  [580/728]  eta: 0:01:57  loss: 0.4707 (0.5747)  acc: 78.5714 (78.6780)  time: 0.8336  data: 0.3135  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.5758 (0.5780)  acc: 78.5714 (78.6157)  time: 0.7263  data: 0.2048  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.6047 (0.5785)  acc: 75.0000 (78.5635)  time: 0.6657  data: 0.1447  max mem: 31081
Val:  [610/728]  eta: 0:01:32  loss: 0.6047 (0.5799)  acc: 75.0000 (78.4799)  time: 0.6516  data: 0.1321  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.5304 (0.5845)  acc: 77.3810 (78.4583)  time: 0.7735  data: 0.2515  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4503 (0.5832)  acc: 79.7619 (78.4865)  time: 0.8650  data: 0.3422  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.4573 (0.5830)  acc: 80.9524 (78.4953)  time: 0.7911  data: 0.2679  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4972 (0.5832)  acc: 79.7619 (78.4745)  time: 0.7875  data: 0.2643  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4972 (0.5827)  acc: 79.7619 (78.4868)  time: 0.7683  data: 0.2471  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.5406 (0.5833)  acc: 78.5714 (78.4650)  time: 0.6796  data: 0.1593  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.5285 (0.5812)  acc: 78.5714 (78.5277)  time: 0.6952  data: 0.1738  max mem: 31081
Val:  [690/728]  eta: 0:00:29  loss: 0.3799 (0.5785)  acc: 82.1429 (78.6024)  time: 0.8276  data: 0.3055  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.3645 (0.5780)  acc: 82.1429 (78.6071)  time: 0.8532  data: 0.3299  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4896 (0.5784)  acc: 78.5714 (78.5664)  time: 0.8041  data: 0.2830  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.5234 (0.5776)  acc: 78.5714 (78.5747)  time: 0.7783  data: 0.2653  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.5234 (0.5804)  acc: 78.5714 (78.5321)  time: 0.7557  data: 0.2653  max mem: 31081
Val: Total time: 0:09:31 (0.7851 s / it)
* Acc@1 78.532 AP 0.7949004769325256 loss 0.580
Accuracy of the network on the 61096 val videos: 78.5%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.07 GB / 503.51 GB
Epoch: [23]  [  0/893]  eta: 3:19:58  lr: 0.001432  min_lr: 0.000002  loss: 0.2529 (0.2529)  class_acc: 0.8929 (0.8929)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 13.4358  data: 12.1110  max mem: 31081
[2025-03-11 07:59:13,494] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 07:59:13,495] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.21 GB / 503.51 GB
Epoch: [23]  [ 10/893]  eta: 0:38:01  lr: 0.001431  min_lr: 0.000002  loss: 0.2983 (0.3039)  class_acc: 0.8750 (0.8750)  loss_scale: 4096.0000 (4468.3636)  weight_decay: 0.0500 (0.0500)  time: 2.5841  data: 1.1013  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 88.30 GB / 503.51 GB
Epoch: [23]  [ 20/893]  eta: 0:30:06  lr: 0.001430  min_lr: 0.000002  loss: 0.3040 (0.3039)  class_acc: 0.8750 (0.8690)  loss_scale: 8192.0000 (6241.5238)  weight_decay: 0.0500 (0.0500)  time: 1.5008  data: 0.0004  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.00 GB / 503.51 GB
Epoch: [23]  [ 30/893]  eta: 0:27:07  lr: 0.001429  min_lr: 0.000002  loss: 0.2825 (0.2968)  class_acc: 0.8571 (0.8715)  loss_scale: 8192.0000 (6870.7097)  weight_decay: 0.0500 (0.0500)  time: 1.5027  data: 0.0005  max mem: 31081
[2025-03-11 07:59:55,633] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 20554
[2025-03-11 07:59:55,633] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 07:59:55,633] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.37 GB / 503.51 GB
Epoch: [23]  [ 40/893]  eta: 0:25:30  lr: 0.001429  min_lr: 0.000002  loss: 0.2825 (0.2983)  class_acc: 0.8750 (0.8733)  loss_scale: 8192.0000 (6893.2683)  weight_decay: 0.0500 (0.0500)  time: 1.5068  data: 0.0006  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.05 GB / 503.51 GB
Epoch: [23]  [ 50/893]  eta: 0:24:23  lr: 0.001428  min_lr: 0.000002  loss: 0.3066 (0.3044)  class_acc: 0.8750 (0.8711)  loss_scale: 4096.0000 (6344.7843)  weight_decay: 0.0500 (0.0500)  time: 1.5024  data: 0.0006  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.64 GB / 503.51 GB
Epoch: [23]  [ 60/893]  eta: 0:23:28  lr: 0.001427  min_lr: 0.000002  loss: 0.3120 (0.3047)  class_acc: 0.8571 (0.8700)  loss_scale: 4096.0000 (5976.1311)  weight_decay: 0.0500 (0.0500)  time: 1.4788  data: 0.0004  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.89 GB / 503.51 GB
Epoch: [23]  [ 70/893]  eta: 0:22:45  lr: 0.001426  min_lr: 0.000002  loss: 0.2974 (0.3036)  class_acc: 0.8571 (0.8700)  loss_scale: 4096.0000 (5711.3239)  weight_decay: 0.0500 (0.0500)  time: 1.4638  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.51 GB / 503.51 GB
Epoch: [23]  [ 80/893]  eta: 0:22:09  lr: 0.001425  min_lr: 0.000002  loss: 0.2974 (0.3061)  class_acc: 0.8571 (0.8695)  loss_scale: 4096.0000 (5511.9012)  weight_decay: 0.0500 (0.0500)  time: 1.4644  data: 0.0004  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.04 GB / 503.51 GB
Epoch: [23]  [ 90/893]  eta: 0:21:39  lr: 0.001424  min_lr: 0.000002  loss: 0.3013 (0.3067)  class_acc: 0.8571 (0.8689)  loss_scale: 4096.0000 (5356.3077)  weight_decay: 0.0500 (0.0500)  time: 1.4713  data: 0.0002  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.71 GB / 503.51 GB
Epoch: [23]  [100/893]  eta: 0:21:11  lr: 0.001424  min_lr: 0.000002  loss: 0.2837 (0.3058)  class_acc: 0.8571 (0.8697)  loss_scale: 4096.0000 (5231.5248)  weight_decay: 0.0500 (0.0500)  time: 1.4740  data: 0.0002  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.08 GB / 503.51 GB
Epoch: [23]  [110/893]  eta: 0:20:45  lr: 0.001423  min_lr: 0.000002  loss: 0.2725 (0.3015)  class_acc: 0.8750 (0.8715)  loss_scale: 4096.0000 (5129.2252)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.62 GB / 503.51 GB
Epoch: [23]  [120/893]  eta: 0:20:21  lr: 0.001422  min_lr: 0.000002  loss: 0.2773 (0.3033)  class_acc: 0.8750 (0.8691)  loss_scale: 4096.0000 (5043.8347)  weight_decay: 0.0500 (0.0500)  time: 1.4628  data: 0.0004  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.49 GB / 503.51 GB
Epoch: [23]  [130/893]  eta: 0:19:58  lr: 0.001421  min_lr: 0.000002  loss: 0.3115 (0.3036)  class_acc: 0.8571 (0.8685)  loss_scale: 4096.0000 (4971.4809)  weight_decay: 0.0500 (0.0500)  time: 1.4608  data: 0.0003  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.79 GB / 503.51 GB
Epoch: [23]  [140/893]  eta: 0:19:37  lr: 0.001420  min_lr: 0.000002  loss: 0.3206 (0.3037)  class_acc: 0.8571 (0.8685)  loss_scale: 4096.0000 (4909.3901)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0002  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.66 GB / 503.51 GB
Epoch: [23]  [150/893]  eta: 0:19:16  lr: 0.001420  min_lr: 0.000002  loss: 0.3298 (0.3034)  class_acc: 0.8571 (0.8683)  loss_scale: 4096.0000 (4855.5232)  weight_decay: 0.0500 (0.0500)  time: 1.4649  data: 0.0002  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.29 GB / 503.51 GB
Epoch: [23]  [160/893]  eta: 0:18:56  lr: 0.001419  min_lr: 0.000002  loss: 0.2644 (0.3013)  class_acc: 0.8571 (0.8693)  loss_scale: 4096.0000 (4808.3478)  weight_decay: 0.0500 (0.0500)  time: 1.4587  data: 0.0002  max mem: 31081
[2025-03-11 08:03:04,961] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 08:03:04,961] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.85 GB / 503.51 GB
Epoch: [23]  [170/893]  eta: 0:18:37  lr: 0.001418  min_lr: 0.000002  loss: 0.2705 (0.3012)  class_acc: 0.8750 (0.8688)  loss_scale: 4096.0000 (4862.5029)  weight_decay: 0.0500 (0.0500)  time: 1.4605  data: 0.0002  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.41 GB / 503.51 GB
Epoch: [23]  [180/893]  eta: 0:18:18  lr: 0.001417  min_lr: 0.000002  loss: 0.2957 (0.3022)  class_acc: 0.8571 (0.8689)  loss_scale: 8192.0000 (5046.4530)  weight_decay: 0.0500 (0.0500)  time: 1.4630  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.13 GB / 503.51 GB
Epoch: [23]  [190/893]  eta: 0:18:00  lr: 0.001416  min_lr: 0.000002  loss: 0.2891 (0.3031)  class_acc: 0.8571 (0.8676)  loss_scale: 8192.0000 (5211.1414)  weight_decay: 0.0500 (0.0500)  time: 1.4632  data: 0.0003  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.28 GB / 503.51 GB
Epoch: [23]  [200/893]  eta: 0:17:42  lr: 0.001415  min_lr: 0.000002  loss: 0.2820 (0.3026)  class_acc: 0.8571 (0.8679)  loss_scale: 8192.0000 (5359.4428)  weight_decay: 0.0500 (0.0500)  time: 1.4574  data: 0.0003  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.33 GB / 503.51 GB
Epoch: [23]  [210/893]  eta: 0:17:24  lr: 0.001415  min_lr: 0.000002  loss: 0.2898 (0.3032)  class_acc: 0.8750 (0.8679)  loss_scale: 8192.0000 (5493.6872)  weight_decay: 0.0500 (0.0500)  time: 1.4566  data: 0.0003  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.41 GB / 503.51 GB
Epoch: [23]  [220/893]  eta: 0:17:07  lr: 0.001414  min_lr: 0.000002  loss: 0.2717 (0.3013)  class_acc: 0.8750 (0.8683)  loss_scale: 8192.0000 (5615.7828)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.97 GB / 503.51 GB
Epoch: [23]  [230/893]  eta: 0:16:50  lr: 0.001413  min_lr: 0.000002  loss: 0.2717 (0.3029)  class_acc: 0.8750 (0.8677)  loss_scale: 8192.0000 (5727.3074)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.88 GB / 503.51 GB
Epoch: [23]  [240/893]  eta: 0:16:33  lr: 0.001412  min_lr: 0.000002  loss: 0.3081 (0.3030)  class_acc: 0.8571 (0.8672)  loss_scale: 8192.0000 (5829.5768)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.64 GB / 503.51 GB
Epoch: [23]  [250/893]  eta: 0:16:16  lr: 0.001411  min_lr: 0.000002  loss: 0.2715 (0.3007)  class_acc: 0.8571 (0.8680)  loss_scale: 8192.0000 (5923.6972)  weight_decay: 0.0500 (0.0500)  time: 1.4599  data: 0.0002  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.98 GB / 503.51 GB
Epoch: [23]  [260/893]  eta: 0:15:59  lr: 0.001411  min_lr: 0.000002  loss: 0.2483 (0.2997)  class_acc: 0.8929 (0.8689)  loss_scale: 8192.0000 (6010.6054)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0002  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.30 GB / 503.51 GB
Epoch: [23]  [270/893]  eta: 0:15:43  lr: 0.001410  min_lr: 0.000002  loss: 0.2539 (0.2986)  class_acc: 0.8929 (0.8697)  loss_scale: 8192.0000 (6091.0996)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.03 GB / 503.51 GB
Epoch: [23]  [280/893]  eta: 0:15:27  lr: 0.001409  min_lr: 0.000002  loss: 0.2646 (0.2980)  class_acc: 0.8929 (0.8701)  loss_scale: 8192.0000 (6165.8648)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.19 GB / 503.51 GB
Epoch: [23]  [290/893]  eta: 0:15:11  lr: 0.001408  min_lr: 0.000002  loss: 0.2947 (0.2990)  class_acc: 0.8750 (0.8695)  loss_scale: 8192.0000 (6235.4914)  weight_decay: 0.0500 (0.0500)  time: 1.4641  data: 0.0004  max mem: 31081
[2025-03-11 08:06:12,142] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 08:06:12,142] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-03-11 08:06:13,618] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 20812
[2025-03-11 08:06:13,618] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 08:06:13,618] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2025-03-11 08:06:15,047] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 20813
[2025-03-11 08:06:15,048] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 08:06:15,048] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.22 GB / 503.51 GB
Epoch: [23]  [300/893]  eta: 0:14:54  lr: 0.001407  min_lr: 0.000002  loss: 0.2947 (0.2991)  class_acc: 0.8750 (0.8695)  loss_scale: 8192.0000 (6273.2757)  weight_decay: 0.0500 (0.0500)  time: 1.4610  data: 0.0004  max mem: 31081
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.25 GB / 503.51 GB
Epoch: [23]  [310/893]  eta: 0:14:38  lr: 0.001406  min_lr: 0.000002  loss: 0.2878 (0.2997)  class_acc: 0.8750 (0.8693)  loss_scale: 4096.0000 (6203.2669)  weight_decay: 0.0500 (0.0500)  time: 1.4550  data: 0.0003  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.88 GB / 503.51 GB
Epoch: [23]  [320/893]  eta: 0:14:23  lr: 0.001406  min_lr: 0.000002  loss: 0.3259 (0.2998)  class_acc: 0.8571 (0.8694)  loss_scale: 4096.0000 (6137.6199)  weight_decay: 0.0500 (0.0500)  time: 1.4604  data: 0.0003  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.00 GB / 503.51 GB
Epoch: [23]  [330/893]  eta: 0:14:07  lr: 0.001405  min_lr: 0.000002  loss: 0.3208 (0.3006)  class_acc: 0.8571 (0.8692)  loss_scale: 4096.0000 (6075.9396)  weight_decay: 0.0500 (0.0500)  time: 1.4582  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.06 GB / 503.51 GB
Epoch: [23]  [340/893]  eta: 0:13:51  lr: 0.001404  min_lr: 0.000002  loss: 0.3179 (0.3009)  class_acc: 0.8571 (0.8688)  loss_scale: 4096.0000 (6017.8768)  weight_decay: 0.0500 (0.0500)  time: 1.4603  data: 0.0002  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.17 GB / 503.51 GB
Epoch: [23]  [350/893]  eta: 0:13:35  lr: 0.001403  min_lr: 0.000002  loss: 0.3108 (0.3007)  class_acc: 0.8571 (0.8687)  loss_scale: 4096.0000 (5963.1225)  weight_decay: 0.0500 (0.0500)  time: 1.4704  data: 0.0002  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.14 GB / 503.51 GB
Epoch: [23]  [360/893]  eta: 0:13:20  lr: 0.001402  min_lr: 0.000002  loss: 0.3179 (0.3017)  class_acc: 0.8571 (0.8678)  loss_scale: 4096.0000 (5911.4017)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.58 GB / 503.51 GB
Epoch: [23]  [370/893]  eta: 0:13:04  lr: 0.001402  min_lr: 0.000002  loss: 0.3179 (0.3016)  class_acc: 0.8571 (0.8677)  loss_scale: 4096.0000 (5862.4690)  weight_decay: 0.0500 (0.0500)  time: 1.4646  data: 0.0003  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.87 GB / 503.51 GB
Epoch: [23]  [380/893]  eta: 0:12:49  lr: 0.001401  min_lr: 0.000002  loss: 0.2654 (0.3010)  class_acc: 0.8750 (0.8683)  loss_scale: 4096.0000 (5816.1050)  weight_decay: 0.0500 (0.0500)  time: 1.4623  data: 0.0002  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.95 GB / 503.51 GB
Epoch: [23]  [390/893]  eta: 0:12:33  lr: 0.001400  min_lr: 0.000002  loss: 0.2693 (0.3006)  class_acc: 0.8750 (0.8681)  loss_scale: 4096.0000 (5772.1125)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0004  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.19 GB / 503.51 GB
Epoch: [23]  [400/893]  eta: 0:12:18  lr: 0.001399  min_lr: 0.000002  loss: 0.3013 (0.3004)  class_acc: 0.8571 (0.8682)  loss_scale: 4096.0000 (5730.3142)  weight_decay: 0.0500 (0.0500)  time: 1.4671  data: 0.0004  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.56 GB / 503.51 GB
Epoch: [23]  [410/893]  eta: 0:12:03  lr: 0.001398  min_lr: 0.000002  loss: 0.3025 (0.3005)  class_acc: 0.8571 (0.8682)  loss_scale: 4096.0000 (5690.5499)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.24 GB / 503.51 GB
Epoch: [23]  [420/893]  eta: 0:11:47  lr: 0.001397  min_lr: 0.000002  loss: 0.3047 (0.3005)  class_acc: 0.8571 (0.8684)  loss_scale: 4096.0000 (5652.6746)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
[2025-03-11 08:09:23,792] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 08:09:23,792] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.11 GB / 503.51 GB
Epoch: [23]  [430/893]  eta: 0:11:32  lr: 0.001397  min_lr: 0.000002  loss: 0.3301 (0.3009)  class_acc: 0.8750 (0.8685)  loss_scale: 4096.0000 (5664.0742)  weight_decay: 0.0500 (0.0500)  time: 1.4617  data: 0.0002  max mem: 31081
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.52 GB / 503.51 GB
Epoch: [23]  [440/893]  eta: 0:11:17  lr: 0.001396  min_lr: 0.000002  loss: 0.3242 (0.3014)  class_acc: 0.8750 (0.8684)  loss_scale: 8192.0000 (5721.3968)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0003  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.12 GB / 503.51 GB
Epoch: [23]  [450/893]  eta: 0:11:01  lr: 0.001395  min_lr: 0.000002  loss: 0.3242 (0.3022)  class_acc: 0.8571 (0.8679)  loss_scale: 8192.0000 (5776.1774)  weight_decay: 0.0500 (0.0500)  time: 1.4719  data: 0.0004  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.35 GB / 503.51 GB
Epoch: [23]  [460/893]  eta: 0:10:46  lr: 0.001394  min_lr: 0.000002  loss: 0.3274 (0.3028)  class_acc: 0.8571 (0.8674)  loss_scale: 8192.0000 (5828.5813)  weight_decay: 0.0500 (0.0500)  time: 1.4721  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.40 GB / 503.51 GB
Epoch: [23]  [470/893]  eta: 0:10:31  lr: 0.001393  min_lr: 0.000002  loss: 0.3206 (0.3027)  class_acc: 0.8571 (0.8675)  loss_scale: 8192.0000 (5878.7601)  weight_decay: 0.0500 (0.0500)  time: 1.4731  data: 0.0002  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.44 GB / 503.51 GB
Epoch: [23]  [480/893]  eta: 0:10:16  lr: 0.001393  min_lr: 0.000002  loss: 0.2764 (0.3032)  class_acc: 0.8571 (0.8671)  loss_scale: 8192.0000 (5926.8524)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0002  max mem: 31081
[2025-03-11 08:10:47,483] [INFO] [logging.py:129:log_dist] [Rank 0] step=21000, skipped=123, lr=[1.8183908024277564e-06, 1.8183908024277564e-06, 3.030651337379594e-06, 3.030651337379594e-06, 5.051085562299324e-06, 5.051085562299324e-06, 8.41847593716554e-06, 8.41847593716554e-06, 1.4030793228609234e-05, 1.4030793228609234e-05, 2.338465538101539e-05, 2.338465538101539e-05, 3.8974425635025655e-05, 3.8974425635025655e-05, 6.495737605837608e-05, 6.495737605837608e-05, 0.00010826229343062682, 0.00010826229343062682, 0.00018043715571771138, 0.00018043715571771138, 0.0003007285928628523, 0.0003007285928628523, 0.0005012143214380872, 0.0005012143214380872, 0.000835357202396812, 0.000835357202396812, 0.0013922620039946868, 0.0013922620039946868], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 08:10:47,484] [INFO] [timer.py:264:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=60.99388784876149, CurrSamplesPerSec=61.38197494326784, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.23 GB / 503.51 GB
Epoch: [23]  [490/893]  eta: 0:10:01  lr: 0.001392  min_lr: 0.000002  loss: 0.3130 (0.3036)  class_acc: 0.8571 (0.8669)  loss_scale: 8192.0000 (5972.9857)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.80 GB / 503.51 GB
Epoch: [23]  [500/893]  eta: 0:09:46  lr: 0.001391  min_lr: 0.000002  loss: 0.3257 (0.3042)  class_acc: 0.8571 (0.8665)  loss_scale: 8192.0000 (6017.2774)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.68 GB / 503.51 GB
Epoch: [23]  [510/893]  eta: 0:09:31  lr: 0.001390  min_lr: 0.000002  loss: 0.3064 (0.3044)  class_acc: 0.8571 (0.8665)  loss_scale: 8192.0000 (6059.8356)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0002  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.31 GB / 503.51 GB
Epoch: [23]  [520/893]  eta: 0:09:16  lr: 0.001389  min_lr: 0.000002  loss: 0.3196 (0.3050)  class_acc: 0.8571 (0.8663)  loss_scale: 8192.0000 (6100.7601)  weight_decay: 0.0500 (0.0500)  time: 1.4705  data: 0.0002  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.53 GB / 503.51 GB
Epoch: [23]  [530/893]  eta: 0:09:00  lr: 0.001388  min_lr: 0.000002  loss: 0.3276 (0.3051)  class_acc: 0.8571 (0.8661)  loss_scale: 8192.0000 (6140.1431)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0003  max mem: 31081
[2025-03-11 08:12:09,507] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 21055
[2025-03-11 08:12:09,507] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 08:12:09,507] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.39 GB / 503.51 GB
Epoch: [23]  [540/893]  eta: 0:08:45  lr: 0.001388  min_lr: 0.000002  loss: 0.3293 (0.3054)  class_acc: 0.8571 (0.8660)  loss_scale: 8192.0000 (6162.9279)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0003  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.95 GB / 503.51 GB
Epoch: [23]  [550/893]  eta: 0:08:30  lr: 0.001387  min_lr: 0.000002  loss: 0.2927 (0.3049)  class_acc: 0.8750 (0.8664)  loss_scale: 4096.0000 (6125.4156)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0003  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.53 GB / 503.51 GB
Epoch: [23]  [560/893]  eta: 0:08:15  lr: 0.001386  min_lr: 0.000002  loss: 0.2764 (0.3047)  class_acc: 0.8750 (0.8667)  loss_scale: 4096.0000 (6089.2406)  weight_decay: 0.0500 (0.0500)  time: 1.4695  data: 0.0003  max mem: 31081
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.54 GB / 503.51 GB
Epoch: [23]  [570/893]  eta: 0:08:00  lr: 0.001385  min_lr: 0.000002  loss: 0.2939 (0.3049)  class_acc: 0.8571 (0.8665)  loss_scale: 4096.0000 (6054.3327)  weight_decay: 0.0500 (0.0500)  time: 1.4747  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.43 GB / 503.51 GB
Epoch: [23]  [580/893]  eta: 0:07:45  lr: 0.001384  min_lr: 0.000002  loss: 0.2905 (0.3046)  class_acc: 0.8750 (0.8666)  loss_scale: 4096.0000 (6020.6265)  weight_decay: 0.0500 (0.0500)  time: 1.4692  data: 0.0003  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.58 GB / 503.51 GB
Epoch: [23]  [590/893]  eta: 0:07:30  lr: 0.001383  min_lr: 0.000002  loss: 0.2825 (0.3043)  class_acc: 0.8929 (0.8670)  loss_scale: 4096.0000 (5988.0609)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0002  max mem: 31081
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.37 GB / 503.51 GB
Epoch: [23]  [600/893]  eta: 0:07:15  lr: 0.001383  min_lr: 0.000002  loss: 0.2969 (0.3042)  class_acc: 0.8929 (0.8670)  loss_scale: 4096.0000 (5956.5790)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0002  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.97 GB / 503.51 GB
Epoch: [23]  [610/893]  eta: 0:07:00  lr: 0.001382  min_lr: 0.000002  loss: 0.2891 (0.3037)  class_acc: 0.8929 (0.8672)  loss_scale: 4096.0000 (5926.1277)  weight_decay: 0.0500 (0.0500)  time: 1.4590  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.64 GB / 503.51 GB
Epoch: [23]  [620/893]  eta: 0:06:45  lr: 0.001381  min_lr: 0.000002  loss: 0.2717 (0.3034)  class_acc: 0.8750 (0.8674)  loss_scale: 4096.0000 (5896.6570)  weight_decay: 0.0500 (0.0500)  time: 1.4665  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.55 GB / 503.51 GB
Epoch: [23]  [630/893]  eta: 0:06:30  lr: 0.001380  min_lr: 0.000002  loss: 0.2771 (0.3032)  class_acc: 0.8750 (0.8676)  loss_scale: 4096.0000 (5868.1204)  weight_decay: 0.0500 (0.0500)  time: 1.4751  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.38 GB / 503.51 GB
Epoch: [23]  [640/893]  eta: 0:06:16  lr: 0.001379  min_lr: 0.000002  loss: 0.2754 (0.3028)  class_acc: 0.8571 (0.8677)  loss_scale: 4096.0000 (5840.4743)  weight_decay: 0.0500 (0.0500)  time: 1.4768  data: 0.0002  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.75 GB / 503.51 GB
Epoch: [23]  [650/893]  eta: 0:06:01  lr: 0.001378  min_lr: 0.000002  loss: 0.2754 (0.3023)  class_acc: 0.8750 (0.8679)  loss_scale: 4096.0000 (5813.6774)  weight_decay: 0.0500 (0.0500)  time: 1.4765  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.41 GB / 503.51 GB
Epoch: [23]  [660/893]  eta: 0:05:46  lr: 0.001378  min_lr: 0.000002  loss: 0.2642 (0.3018)  class_acc: 0.8750 (0.8681)  loss_scale: 4096.0000 (5787.6914)  weight_decay: 0.0500 (0.0500)  time: 1.4693  data: 0.0003  max mem: 31081
[2025-03-11 08:15:19,068] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 08:15:19,068] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.89 GB / 503.51 GB
Epoch: [23]  [670/893]  eta: 0:05:31  lr: 0.001377  min_lr: 0.000002  loss: 0.3123 (0.3025)  class_acc: 0.8393 (0.8676)  loss_scale: 4096.0000 (5780.7928)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.61 GB / 503.51 GB
Epoch: [23]  [680/893]  eta: 0:05:16  lr: 0.001376  min_lr: 0.000002  loss: 0.3123 (0.3024)  class_acc: 0.8571 (0.8675)  loss_scale: 8192.0000 (5816.1997)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0002  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.70 GB / 503.51 GB
Epoch: [23]  [690/893]  eta: 0:05:01  lr: 0.001375  min_lr: 0.000002  loss: 0.2720 (0.3019)  class_acc: 0.8750 (0.8677)  loss_scale: 8192.0000 (5850.5818)  weight_decay: 0.0500 (0.0500)  time: 1.4695  data: 0.0002  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.54 GB / 503.51 GB
Epoch: [23]  [700/893]  eta: 0:04:46  lr: 0.001374  min_lr: 0.000002  loss: 0.2864 (0.3019)  class_acc: 0.8750 (0.8676)  loss_scale: 8192.0000 (5883.9829)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0002  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.54 GB / 503.51 GB
Epoch: [23]  [710/893]  eta: 0:04:31  lr: 0.001374  min_lr: 0.000002  loss: 0.2964 (0.3018)  class_acc: 0.8571 (0.8676)  loss_scale: 8192.0000 (5916.4444)  weight_decay: 0.0500 (0.0500)  time: 1.4746  data: 0.0002  max mem: 31081
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.63 GB / 503.51 GB
Epoch: [23]  [720/893]  eta: 0:04:16  lr: 0.001373  min_lr: 0.000002  loss: 0.2878 (0.3016)  class_acc: 0.8750 (0.8677)  loss_scale: 8192.0000 (5948.0055)  weight_decay: 0.0500 (0.0500)  time: 1.4702  data: 0.0002  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.66 GB / 503.51 GB
Epoch: [23]  [730/893]  eta: 0:04:01  lr: 0.001372  min_lr: 0.000002  loss: 0.2993 (0.3020)  class_acc: 0.8571 (0.8676)  loss_scale: 8192.0000 (5978.7031)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0002  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.46 GB / 503.51 GB
Epoch: [23]  [740/893]  eta: 0:03:47  lr: 0.001371  min_lr: 0.000002  loss: 0.3076 (0.3022)  class_acc: 0.8571 (0.8675)  loss_scale: 8192.0000 (6008.5722)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0003  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.51 GB / 503.51 GB
Epoch: [23]  [750/893]  eta: 0:03:32  lr: 0.001370  min_lr: 0.000002  loss: 0.3096 (0.3024)  class_acc: 0.8571 (0.8674)  loss_scale: 8192.0000 (6037.6458)  weight_decay: 0.0500 (0.0500)  time: 1.4761  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.09 GB / 503.51 GB
Epoch: [23]  [760/893]  eta: 0:03:17  lr: 0.001369  min_lr: 0.000002  loss: 0.3289 (0.3027)  class_acc: 0.8571 (0.8672)  loss_scale: 8192.0000 (6065.9553)  weight_decay: 0.0500 (0.0500)  time: 1.4727  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.94 GB / 503.51 GB
Epoch: [23]  [770/893]  eta: 0:03:02  lr: 0.001369  min_lr: 0.000002  loss: 0.2954 (0.3022)  class_acc: 0.8571 (0.8673)  loss_scale: 8192.0000 (6093.5305)  weight_decay: 0.0500 (0.0500)  time: 1.4613  data: 0.0002  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.27 GB / 503.51 GB
Epoch: [23]  [780/893]  eta: 0:02:47  lr: 0.001368  min_lr: 0.000002  loss: 0.2737 (0.3026)  class_acc: 0.8750 (0.8671)  loss_scale: 8192.0000 (6120.3995)  weight_decay: 0.0500 (0.0500)  time: 1.4584  data: 0.0004  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.23 GB / 503.51 GB
Epoch: [23]  [790/893]  eta: 0:02:32  lr: 0.001367  min_lr: 0.000002  loss: 0.2969 (0.3027)  class_acc: 0.8571 (0.8671)  loss_scale: 8192.0000 (6146.5891)  weight_decay: 0.0500 (0.0500)  time: 1.4696  data: 0.0004  max mem: 31081
[2025-03-11 08:18:27,123] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 08:18:27,123] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-03-11 08:18:28,560] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 21313
[2025-03-11 08:18:28,560] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 08:18:28,560] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.25 GB / 503.51 GB
Epoch: [23]  [800/893]  eta: 0:02:17  lr: 0.001366  min_lr: 0.000002  loss: 0.3110 (0.3032)  class_acc: 0.8571 (0.8670)  loss_scale: 8192.0000 (6182.3521)  weight_decay: 0.0500 (0.0500)  time: 1.4759  data: 0.0003  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.57 GB / 503.51 GB
Epoch: [23]  [810/893]  eta: 0:02:03  lr: 0.001365  min_lr: 0.000002  loss: 0.3174 (0.3033)  class_acc: 0.8571 (0.8670)  loss_scale: 8192.0000 (6207.1319)  weight_decay: 0.0500 (0.0500)  time: 1.4720  data: 0.0003  max mem: 31081
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.01 GB / 503.51 GB
Epoch: [23]  [820/893]  eta: 0:01:48  lr: 0.001364  min_lr: 0.000002  loss: 0.3174 (0.3038)  class_acc: 0.8571 (0.8665)  loss_scale: 8192.0000 (6231.3082)  weight_decay: 0.0500 (0.0500)  time: 1.4756  data: 0.0003  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.41 GB / 503.51 GB
Epoch: [23]  [830/893]  eta: 0:01:33  lr: 0.001364  min_lr: 0.000002  loss: 0.3091 (0.3039)  class_acc: 0.8393 (0.8666)  loss_scale: 8192.0000 (6254.9025)  weight_decay: 0.0500 (0.0500)  time: 1.4697  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.19 GB / 503.51 GB
Epoch: [23]  [840/893]  eta: 0:01:18  lr: 0.001363  min_lr: 0.000002  loss: 0.2903 (0.3037)  class_acc: 0.8750 (0.8667)  loss_scale: 8192.0000 (6277.9358)  weight_decay: 0.0500 (0.0500)  time: 1.4668  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.97 GB / 503.51 GB
Epoch: [23]  [850/893]  eta: 0:01:03  lr: 0.001362  min_lr: 0.000002  loss: 0.2996 (0.3041)  class_acc: 0.8571 (0.8666)  loss_scale: 8192.0000 (6300.4277)  weight_decay: 0.0500 (0.0500)  time: 1.4662  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.98 GB / 503.51 GB
Epoch: [23]  [860/893]  eta: 0:00:48  lr: 0.001361  min_lr: 0.000002  loss: 0.2815 (0.3038)  class_acc: 0.8750 (0.8668)  loss_scale: 8192.0000 (6322.3972)  weight_decay: 0.0500 (0.0500)  time: 1.4491  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.00 GB / 503.51 GB
Epoch: [23]  [870/893]  eta: 0:00:34  lr: 0.001360  min_lr: 0.000002  loss: 0.2448 (0.3035)  class_acc: 0.8929 (0.8670)  loss_scale: 8192.0000 (6343.8622)  weight_decay: 0.0500 (0.0500)  time: 1.4356  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.93 GB / 503.51 GB
Epoch: [23]  [880/893]  eta: 0:00:19  lr: 0.001359  min_lr: 0.000002  loss: 0.2642 (0.3032)  class_acc: 0.8929 (0.8669)  loss_scale: 8192.0000 (6364.8400)  weight_decay: 0.0500 (0.0500)  time: 1.4374  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 81.92 GB / 503.51 GB
Epoch: [23]  [890/893]  eta: 0:00:04  lr: 0.001359  min_lr: 0.000002  loss: 0.2825 (0.3032)  class_acc: 0.8571 (0.8668)  loss_scale: 8192.0000 (6385.3468)  weight_decay: 0.0500 (0.0500)  time: 1.4386  data: 0.0001  max mem: 31081
Epoch: [23]  [892/893]  eta: 0:00:01  lr: 0.001359  min_lr: 0.000002  loss: 0.2825 (0.3031)  class_acc: 0.8571 (0.8669)  loss_scale: 8192.0000 (6387.3722)  weight_decay: 0.0500 (0.0500)  time: 1.3880  data: 0.0001  max mem: 31081
Epoch: [23] Total time: 0:22:00 (1.4791 s / it)
Averaged stats: lr: 0.001359  min_lr: 0.000002  loss: 0.2825 (0.3031)  class_acc: 0.8571 (0.8669)  loss_scale: 8192.0000 (6387.3722)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:18:05  loss: 0.5248 (0.5248)  acc: 85.7143 (85.7143)  time: 11.3813  data: 10.8328  max mem: 31081
Val:  [ 10/728]  eta: 0:18:18  loss: 0.4502 (0.5312)  acc: 84.5238 (83.3333)  time: 1.5302  data: 1.0083  max mem: 31081
Val:  [ 20/728]  eta: 0:14:05  loss: 0.4487 (0.5175)  acc: 82.1429 (81.5760)  time: 0.6851  data: 0.1645  max mem: 31081
Val:  [ 30/728]  eta: 0:12:30  loss: 0.4487 (0.5132)  acc: 79.7619 (80.1843)  time: 0.8240  data: 0.3016  max mem: 31081
Val:  [ 40/728]  eta: 0:11:43  loss: 0.4690 (0.5134)  acc: 78.5714 (79.5877)  time: 0.8406  data: 0.3176  max mem: 31081
Val:  [ 50/728]  eta: 0:11:07  loss: 0.4690 (0.4932)  acc: 78.5714 (80.4388)  time: 0.8440  data: 0.3202  max mem: 31081
Val:  [ 60/728]  eta: 0:10:18  loss: 0.5292 (0.5150)  acc: 78.5714 (79.6643)  time: 0.7280  data: 0.2045  max mem: 31081
Val:  [ 70/728]  eta: 0:09:46  loss: 0.5997 (0.5353)  acc: 77.3810 (79.0409)  time: 0.6541  data: 0.1311  max mem: 31081
Val:  [ 80/728]  eta: 0:09:32  loss: 0.5608 (0.5562)  acc: 76.1905 (78.4832)  time: 0.7558  data: 0.2323  max mem: 31081
Val:  [ 90/728]  eta: 0:09:19  loss: 0.5648 (0.5769)  acc: 76.1905 (77.7734)  time: 0.8256  data: 0.3012  max mem: 31081
Val:  [100/728]  eta: 0:09:06  loss: 0.4777 (0.5606)  acc: 82.1429 (78.3946)  time: 0.8138  data: 0.2884  max mem: 31081
Val:  [110/728]  eta: 0:08:56  loss: 0.3584 (0.5587)  acc: 84.5238 (78.3355)  time: 0.8303  data: 0.3073  max mem: 31081
Val:  [120/728]  eta: 0:08:43  loss: 0.3321 (0.5446)  acc: 84.5238 (78.8371)  time: 0.8158  data: 0.2947  max mem: 31081
Val:  [130/728]  eta: 0:08:21  loss: 0.3658 (0.5453)  acc: 86.9048 (78.9440)  time: 0.6726  data: 0.1518  max mem: 31081
Val:  [140/728]  eta: 0:08:10  loss: 0.4134 (0.5478)  acc: 79.7619 (78.7825)  time: 0.6701  data: 0.1488  max mem: 31081
Val:  [150/728]  eta: 0:07:56  loss: 0.4722 (0.5584)  acc: 77.3810 (78.6897)  time: 0.7286  data: 0.2054  max mem: 31081
Val:  [160/728]  eta: 0:07:48  loss: 0.4691 (0.5630)  acc: 79.7619 (78.5640)  time: 0.7566  data: 0.2333  max mem: 31081
Val:  [170/728]  eta: 0:07:41  loss: 0.4530 (0.5608)  acc: 80.9524 (78.6967)  time: 0.8476  data: 0.3229  max mem: 31081
Val:  [180/728]  eta: 0:07:30  loss: 0.4530 (0.5593)  acc: 82.1429 (78.6898)  time: 0.8040  data: 0.2798  max mem: 31081
Val:  [190/728]  eta: 0:07:16  loss: 0.4869 (0.5535)  acc: 78.5714 (78.8270)  time: 0.6743  data: 0.1520  max mem: 31081
Val:  [200/728]  eta: 0:07:07  loss: 0.3159 (0.5428)  acc: 83.3333 (79.1222)  time: 0.6964  data: 0.1708  max mem: 31081
Val:  [210/728]  eta: 0:07:01  loss: 0.3159 (0.5491)  acc: 83.3333 (79.1187)  time: 0.8378  data: 0.3113  max mem: 31081
Val:  [220/728]  eta: 0:06:55  loss: 0.4081 (0.5482)  acc: 82.1429 (79.1424)  time: 0.8967  data: 0.3734  max mem: 31081
Val:  [230/728]  eta: 0:06:45  loss: 0.4870 (0.5541)  acc: 83.3333 (79.0146)  time: 0.8258  data: 0.3047  max mem: 31081
Val:  [240/728]  eta: 0:06:35  loss: 0.5381 (0.5547)  acc: 79.7619 (78.9765)  time: 0.7217  data: 0.1994  max mem: 31081
Val:  [250/728]  eta: 0:06:23  loss: 0.5381 (0.5543)  acc: 76.1905 (78.9461)  time: 0.6737  data: 0.1494  max mem: 31081
Val:  [260/728]  eta: 0:06:16  loss: 0.3549 (0.5517)  acc: 84.5238 (79.0914)  time: 0.7375  data: 0.2145  max mem: 31081
Val:  [270/728]  eta: 0:06:08  loss: 0.4060 (0.5527)  acc: 83.3333 (78.9931)  time: 0.8245  data: 0.3032  max mem: 31081
Val:  [280/728]  eta: 0:06:03  loss: 0.5377 (0.5548)  acc: 79.7619 (79.0205)  time: 0.9220  data: 0.4005  max mem: 31081
Val:  [290/728]  eta: 0:05:55  loss: 0.3870 (0.5492)  acc: 80.9524 (79.1278)  time: 0.9132  data: 0.3895  max mem: 31081
Val:  [300/728]  eta: 0:05:43  loss: 0.3870 (0.5471)  acc: 83.3333 (79.2201)  time: 0.6659  data: 0.1423  max mem: 31081
Val:  [310/728]  eta: 0:05:35  loss: 0.5166 (0.5512)  acc: 76.1905 (79.0193)  time: 0.6773  data: 0.1556  max mem: 31081
Val:  [320/728]  eta: 0:05:27  loss: 0.4647 (0.5476)  acc: 75.0000 (79.1129)  time: 0.8034  data: 0.2818  max mem: 31081
Val:  [330/728]  eta: 0:05:20  loss: 0.3425 (0.5420)  acc: 84.5238 (79.2584)  time: 0.8335  data: 0.3108  max mem: 31081
Val:  [340/728]  eta: 0:05:13  loss: 0.3254 (0.5369)  acc: 83.3333 (79.4337)  time: 0.8897  data: 0.3669  max mem: 31081
Val:  [350/728]  eta: 0:05:05  loss: 0.3739 (0.5376)  acc: 84.5238 (79.4770)  time: 0.8379  data: 0.3152  max mem: 31081
Val:  [360/728]  eta: 0:04:54  loss: 0.4349 (0.5344)  acc: 80.9524 (79.5179)  time: 0.6540  data: 0.1329  max mem: 31081
Val:  [370/728]  eta: 0:04:46  loss: 0.4130 (0.5374)  acc: 79.7619 (79.4474)  time: 0.6932  data: 0.1720  max mem: 31081
Val:  [380/728]  eta: 0:04:39  loss: 0.4437 (0.5405)  acc: 79.7619 (79.3682)  time: 0.8510  data: 0.3262  max mem: 31081
Val:  [390/728]  eta: 0:04:31  loss: 0.4023 (0.5363)  acc: 80.9524 (79.5244)  time: 0.8287  data: 0.3007  max mem: 31081
Val:  [400/728]  eta: 0:04:23  loss: 0.3492 (0.5365)  acc: 84.5238 (79.5867)  time: 0.8530  data: 0.3263  max mem: 31081
Val:  [410/728]  eta: 0:04:16  loss: 0.4549 (0.5375)  acc: 80.9524 (79.5563)  time: 0.8706  data: 0.3491  max mem: 31081
Val:  [420/728]  eta: 0:04:06  loss: 0.4866 (0.5364)  acc: 79.7619 (79.5781)  time: 0.6888  data: 0.1692  max mem: 31081
Val:  [430/728]  eta: 0:03:58  loss: 0.4866 (0.5357)  acc: 80.9524 (79.5741)  time: 0.6902  data: 0.1683  max mem: 31081
Val:  [440/728]  eta: 0:03:50  loss: 0.4159 (0.5333)  acc: 80.9524 (79.6323)  time: 0.8259  data: 0.3039  max mem: 31081
Val:  [450/728]  eta: 0:03:42  loss: 0.4331 (0.5352)  acc: 83.3333 (79.5982)  time: 0.8176  data: 0.2980  max mem: 31081
Val:  [460/728]  eta: 0:03:34  loss: 0.5875 (0.5378)  acc: 77.3810 (79.4727)  time: 0.8217  data: 0.3007  max mem: 31081
Val:  [470/728]  eta: 0:03:26  loss: 0.3775 (0.5352)  acc: 78.5714 (79.5597)  time: 0.7871  data: 0.2636  max mem: 31081
Val:  [480/728]  eta: 0:03:17  loss: 0.3775 (0.5344)  acc: 78.5714 (79.5738)  time: 0.6500  data: 0.1253  max mem: 31081
Val:  [490/728]  eta: 0:03:09  loss: 0.4730 (0.5345)  acc: 78.5714 (79.5704)  time: 0.6865  data: 0.1636  max mem: 31081
Val:  [500/728]  eta: 0:03:01  loss: 0.4332 (0.5348)  acc: 79.7619 (79.5837)  time: 0.8355  data: 0.3138  max mem: 31081
Val:  [510/728]  eta: 0:02:54  loss: 0.3581 (0.5315)  acc: 82.1429 (79.6547)  time: 0.8643  data: 0.3431  max mem: 31081
Val:  [520/728]  eta: 0:02:46  loss: 0.3833 (0.5328)  acc: 79.7619 (79.5448)  time: 0.8484  data: 0.3239  max mem: 31081
Val:  [530/728]  eta: 0:02:37  loss: 0.5275 (0.5323)  acc: 77.3810 (79.5669)  time: 0.7479  data: 0.2230  max mem: 31081
Val:  [540/728]  eta: 0:02:28  loss: 0.5849 (0.5356)  acc: 77.3810 (79.4318)  time: 0.6153  data: 0.0920  max mem: 31081
Val:  [550/728]  eta: 0:02:20  loss: 0.6126 (0.5383)  acc: 71.4286 (79.3190)  time: 0.6638  data: 0.1382  max mem: 31081
Val:  [560/728]  eta: 0:02:12  loss: 0.5160 (0.5379)  acc: 78.5714 (79.3396)  time: 0.7462  data: 0.2214  max mem: 31081
Val:  [570/728]  eta: 0:02:05  loss: 0.4938 (0.5455)  acc: 78.5714 (79.2886)  time: 0.7914  data: 0.2688  max mem: 31081
Val:  [580/728]  eta: 0:01:57  loss: 0.3682 (0.5448)  acc: 82.1429 (79.3746)  time: 0.8325  data: 0.3118  max mem: 31081
Val:  [590/728]  eta: 0:01:48  loss: 0.4534 (0.5460)  acc: 83.3333 (79.3570)  time: 0.7283  data: 0.2058  max mem: 31081
Val:  [600/728]  eta: 0:01:40  loss: 0.5032 (0.5463)  acc: 82.1429 (79.3420)  time: 0.6327  data: 0.1085  max mem: 31081
Val:  [610/728]  eta: 0:01:32  loss: 0.6174 (0.5492)  acc: 76.1905 (79.2241)  time: 0.6511  data: 0.1261  max mem: 31081
Val:  [620/728]  eta: 0:01:24  loss: 0.5117 (0.5514)  acc: 76.1905 (79.2021)  time: 0.7978  data: 0.2745  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4385 (0.5506)  acc: 83.3333 (79.2450)  time: 0.8442  data: 0.3223  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.4061 (0.5497)  acc: 83.3333 (79.2605)  time: 0.7907  data: 0.2660  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4850 (0.5494)  acc: 82.1429 (79.2974)  time: 0.7931  data: 0.2687  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4628 (0.5492)  acc: 82.1429 (79.3207)  time: 0.7486  data: 0.2267  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4631 (0.5499)  acc: 82.1429 (79.3219)  time: 0.6777  data: 0.1547  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4631 (0.5488)  acc: 84.5238 (79.3668)  time: 0.6997  data: 0.1771  max mem: 31081
Val:  [690/728]  eta: 0:00:29  loss: 0.3737 (0.5478)  acc: 85.7143 (79.4070)  time: 0.8247  data: 0.3029  max mem: 31081
Val:  [700/728]  eta: 0:00:21  loss: 0.4626 (0.5473)  acc: 80.9524 (79.3951)  time: 0.8503  data: 0.3274  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4626 (0.5478)  acc: 80.9524 (79.3835)  time: 0.7890  data: 0.2692  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.5191 (0.5473)  acc: 79.7619 (79.3673)  time: 0.7524  data: 0.2404  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.5270 (0.5490)  acc: 79.7619 (79.3358)  time: 0.7304  data: 0.2404  max mem: 31081
Val: Total time: 0:09:28 (0.7812 s / it)
* Acc@1 79.336 AP 0.800227701663971 loss 0.549
Accuracy of the network on the 61096 val videos: 79.3%
[2025-03-11 08:30:15,521] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-bestacc is about to be saved!
[2025-03-11 08:30:15,524] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt
[2025-03-11 08:30:15,524] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt...
[2025-03-11 08:30:15,775] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/sorlova/repos/AITHENA/NewStage/VideoMAE/logs/rein/bl2/dota_fuse-no_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-bestacc/mp_rank_00_model_states.pt.
[2025-03-11 08:30:15,775] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-bestacc is ready now!
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 87.35 GB / 503.51 GB
Epoch: [24]  [  0/893]  eta: 3:02:57  lr: 0.001358  min_lr: 0.000002  loss: 0.3428 (0.3428)  class_acc: 0.8214 (0.8214)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 12.2924  data: 10.9629  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.15 GB / 503.51 GB
Epoch: [24]  [ 10/893]  eta: 0:36:50  lr: 0.001358  min_lr: 0.000002  loss: 0.2708 (0.2800)  class_acc: 0.8750 (0.8831)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5030  data: 1.0237  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.85 GB / 503.51 GB
Epoch: [24]  [ 20/893]  eta: 0:29:29  lr: 0.001357  min_lr: 0.000002  loss: 0.2754 (0.2935)  class_acc: 0.8750 (0.8724)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5135  data: 0.0151  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.31 GB / 503.51 GB
Epoch: [24]  [ 30/893]  eta: 0:26:43  lr: 0.001356  min_lr: 0.000002  loss: 0.2869 (0.2919)  class_acc: 0.8929 (0.8790)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5033  data: 0.0005  max mem: 31081
[2025-03-11 08:31:19,774] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 08:31:19,775] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-03-11 08:31:21,283] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 21443
[2025-03-11 08:31:21,284] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 08:31:21,284] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.39 GB / 503.51 GB
Epoch: [24]  [ 40/893]  eta: 0:25:10  lr: 0.001355  min_lr: 0.000002  loss: 0.2896 (0.2902)  class_acc: 0.8929 (0.8785)  loss_scale: 8192.0000 (8391.8049)  weight_decay: 0.0500 (0.0500)  time: 1.5025  data: 0.0008  max mem: 31081
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.38 GB / 503.51 GB
Epoch: [24]  [ 50/893]  eta: 0:24:08  lr: 0.001354  min_lr: 0.000002  loss: 0.2920 (0.2911)  class_acc: 0.8750 (0.8792)  loss_scale: 8192.0000 (8352.6275)  weight_decay: 0.0500 (0.0500)  time: 1.5004  data: 0.0008  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.37 GB / 503.51 GB
Epoch: [24]  [ 60/893]  eta: 0:23:17  lr: 0.001353  min_lr: 0.000002  loss: 0.2986 (0.2923)  class_acc: 0.8750 (0.8779)  loss_scale: 8192.0000 (8326.2951)  weight_decay: 0.0500 (0.0500)  time: 1.4878  data: 0.0005  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.59 GB / 503.51 GB
Epoch: [24]  [ 70/893]  eta: 0:22:35  lr: 0.001353  min_lr: 0.000002  loss: 0.3035 (0.2954)  class_acc: 0.8571 (0.8737)  loss_scale: 8192.0000 (8307.3803)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.86 GB / 503.51 GB
Epoch: [24]  [ 80/893]  eta: 0:22:00  lr: 0.001352  min_lr: 0.000002  loss: 0.3284 (0.3019)  class_acc: 0.8393 (0.8721)  loss_scale: 8192.0000 (8293.1358)  weight_decay: 0.0500 (0.0500)  time: 1.4599  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.09 GB / 503.51 GB
Epoch: [24]  [ 90/893]  eta: 0:21:29  lr: 0.001351  min_lr: 0.000002  loss: 0.3284 (0.3045)  class_acc: 0.8571 (0.8709)  loss_scale: 8192.0000 (8282.0220)  weight_decay: 0.0500 (0.0500)  time: 1.4598  data: 0.0002  max mem: 31081
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.98 GB / 503.51 GB
Epoch: [24]  [100/893]  eta: 0:21:01  lr: 0.001350  min_lr: 0.000002  loss: 0.2896 (0.3036)  class_acc: 0.8750 (0.8720)  loss_scale: 8192.0000 (8273.1089)  weight_decay: 0.0500 (0.0500)  time: 1.4563  data: 0.0004  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.75 GB / 503.51 GB
Epoch: [24]  [110/893]  eta: 0:20:37  lr: 0.001349  min_lr: 0.000002  loss: 0.3083 (0.3056)  class_acc: 0.8750 (0.8703)  loss_scale: 8192.0000 (8265.8018)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0004  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.44 GB / 503.51 GB
Epoch: [24]  [120/893]  eta: 0:20:14  lr: 0.001348  min_lr: 0.000002  loss: 0.2803 (0.3042)  class_acc: 0.8571 (0.8716)  loss_scale: 8192.0000 (8259.7025)  weight_decay: 0.0500 (0.0500)  time: 1.4676  data: 0.0003  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.92 GB / 503.51 GB
Epoch: [24]  [130/893]  eta: 0:19:52  lr: 0.001348  min_lr: 0.000002  loss: 0.2715 (0.3041)  class_acc: 0.8571 (0.8710)  loss_scale: 8192.0000 (8254.5344)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0003  max mem: 31081
[2025-03-11 08:33:42,383] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 21539
[2025-03-11 08:33:42,383] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 08:33:42,383] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.96 GB / 503.51 GB
Epoch: [24]  [140/893]  eta: 0:19:31  lr: 0.001347  min_lr: 0.000002  loss: 0.3069 (0.3061)  class_acc: 0.8571 (0.8692)  loss_scale: 4096.0000 (7959.6028)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.17 GB / 503.51 GB
Epoch: [24]  [150/893]  eta: 0:19:11  lr: 0.001346  min_lr: 0.000002  loss: 0.2854 (0.3020)  class_acc: 0.8571 (0.8710)  loss_scale: 4096.0000 (7703.7351)  weight_decay: 0.0500 (0.0500)  time: 1.4620  data: 0.0003  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.85 GB / 503.51 GB
Epoch: [24]  [160/893]  eta: 0:18:52  lr: 0.001345  min_lr: 0.000002  loss: 0.2827 (0.3052)  class_acc: 0.8750 (0.8693)  loss_scale: 4096.0000 (7479.6522)  weight_decay: 0.0500 (0.0500)  time: 1.4683  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.13 GB / 503.51 GB
Epoch: [24]  [170/893]  eta: 0:18:33  lr: 0.001344  min_lr: 0.000002  loss: 0.3237 (0.3054)  class_acc: 0.8571 (0.8695)  loss_scale: 4096.0000 (7281.7778)  weight_decay: 0.0500 (0.0500)  time: 1.4707  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.03 GB / 503.51 GB
Epoch: [24]  [180/893]  eta: 0:18:15  lr: 0.001343  min_lr: 0.000002  loss: 0.3252 (0.3071)  class_acc: 0.8571 (0.8693)  loss_scale: 4096.0000 (7105.7680)  weight_decay: 0.0500 (0.0500)  time: 1.4663  data: 0.0002  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.27 GB / 503.51 GB
Epoch: [24]  [190/893]  eta: 0:17:57  lr: 0.001343  min_lr: 0.000002  loss: 0.3137 (0.3070)  class_acc: 0.8571 (0.8689)  loss_scale: 4096.0000 (6948.1885)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0002  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.40 GB / 503.51 GB
Epoch: [24]  [200/893]  eta: 0:17:39  lr: 0.001342  min_lr: 0.000002  loss: 0.3137 (0.3072)  class_acc: 0.8750 (0.8683)  loss_scale: 4096.0000 (6806.2886)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0002  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.67 GB / 503.51 GB
Epoch: [24]  [210/893]  eta: 0:17:21  lr: 0.001341  min_lr: 0.000002  loss: 0.2930 (0.3057)  class_acc: 0.8750 (0.8689)  loss_scale: 4096.0000 (6677.8389)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0002  max mem: 31081
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.49 GB / 503.51 GB
Epoch: [24]  [220/893]  eta: 0:17:04  lr: 0.001340  min_lr: 0.000002  loss: 0.2854 (0.3067)  class_acc: 0.8571 (0.8685)  loss_scale: 4096.0000 (6561.0136)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.54 GB / 503.51 GB
Epoch: [24]  [230/893]  eta: 0:16:47  lr: 0.001339  min_lr: 0.000002  loss: 0.2854 (0.3061)  class_acc: 0.8571 (0.8680)  loss_scale: 4096.0000 (6454.3030)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.19 GB / 503.51 GB
Epoch: [24]  [240/893]  eta: 0:16:31  lr: 0.001338  min_lr: 0.000002  loss: 0.2769 (0.3053)  class_acc: 0.8750 (0.8683)  loss_scale: 4096.0000 (6356.4481)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0003  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.51 GB / 503.51 GB
Epoch: [24]  [250/893]  eta: 0:16:14  lr: 0.001338  min_lr: 0.000002  loss: 0.2803 (0.3065)  class_acc: 0.8750 (0.8678)  loss_scale: 4096.0000 (6266.3904)  weight_decay: 0.0500 (0.0500)  time: 1.4615  data: 0.0003  max mem: 31081
[2025-03-11 08:36:51,209] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 08:36:51,209] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.64 GB / 503.51 GB
Epoch: [24]  [260/893]  eta: 0:15:58  lr: 0.001337  min_lr: 0.000002  loss: 0.2856 (0.3062)  class_acc: 0.8750 (0.8680)  loss_scale: 4096.0000 (6198.9272)  weight_decay: 0.0500 (0.0500)  time: 1.4598  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.54 GB / 503.51 GB
Epoch: [24]  [270/893]  eta: 0:15:41  lr: 0.001336  min_lr: 0.000002  loss: 0.2927 (0.3065)  class_acc: 0.8750 (0.8678)  loss_scale: 8192.0000 (6272.4723)  weight_decay: 0.0500 (0.0500)  time: 1.4635  data: 0.0004  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.61 GB / 503.51 GB
Epoch: [24]  [280/893]  eta: 0:15:26  lr: 0.001335  min_lr: 0.000002  loss: 0.2812 (0.3048)  class_acc: 0.8750 (0.8687)  loss_scale: 8192.0000 (6340.7829)  weight_decay: 0.0500 (0.0500)  time: 1.4732  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.64 GB / 503.51 GB
Epoch: [24]  [290/893]  eta: 0:15:10  lr: 0.001334  min_lr: 0.000002  loss: 0.2812 (0.3046)  class_acc: 0.8929 (0.8694)  loss_scale: 8192.0000 (6404.3986)  weight_decay: 0.0500 (0.0500)  time: 1.4730  data: 0.0003  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.57 GB / 503.51 GB
Epoch: [24]  [300/893]  eta: 0:14:53  lr: 0.001333  min_lr: 0.000002  loss: 0.2686 (0.3035)  class_acc: 0.8929 (0.8702)  loss_scale: 8192.0000 (6463.7874)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0003  max mem: 31081
[2025-03-11 08:37:57,270] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 21713
[2025-03-11 08:37:57,270] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 08:37:57,270] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 310, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.51 GB / 503.51 GB
Epoch: [24]  [310/893]  eta: 0:14:38  lr: 0.001333  min_lr: 0.000002  loss: 0.3213 (0.3063)  class_acc: 0.8571 (0.8689)  loss_scale: 8192.0000 (6440.3344)  weight_decay: 0.0500 (0.0500)  time: 1.4617  data: 0.0004  max mem: 31081
AFTER Batch: 320, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.06 GB / 503.51 GB
Epoch: [24]  [320/893]  eta: 0:14:22  lr: 0.001332  min_lr: 0.000002  loss: 0.3350 (0.3068)  class_acc: 0.8393 (0.8689)  loss_scale: 4096.0000 (6367.3022)  weight_decay: 0.0500 (0.0500)  time: 1.4679  data: 0.0003  max mem: 31081
AFTER Batch: 330, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.53 GB / 503.51 GB
Epoch: [24]  [330/893]  eta: 0:14:06  lr: 0.001331  min_lr: 0.000002  loss: 0.3174 (0.3062)  class_acc: 0.8750 (0.8688)  loss_scale: 4096.0000 (6298.6828)  weight_decay: 0.0500 (0.0500)  time: 1.4626  data: 0.0003  max mem: 31081
AFTER Batch: 340, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.68 GB / 503.51 GB
Epoch: [24]  [340/893]  eta: 0:13:50  lr: 0.001330  min_lr: 0.000002  loss: 0.2878 (0.3061)  class_acc: 0.8750 (0.8686)  loss_scale: 4096.0000 (6234.0880)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0003  max mem: 31081
AFTER Batch: 350, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.03 GB / 503.51 GB
Epoch: [24]  [350/893]  eta: 0:13:35  lr: 0.001329  min_lr: 0.000002  loss: 0.2847 (0.3064)  class_acc: 0.8750 (0.8683)  loss_scale: 4096.0000 (6173.1738)  weight_decay: 0.0500 (0.0500)  time: 1.4686  data: 0.0003  max mem: 31081
AFTER Batch: 360, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.55 GB / 503.51 GB
Epoch: [24]  [360/893]  eta: 0:13:19  lr: 0.001328  min_lr: 0.000002  loss: 0.2913 (0.3061)  class_acc: 0.8750 (0.8685)  loss_scale: 4096.0000 (6115.6343)  weight_decay: 0.0500 (0.0500)  time: 1.4672  data: 0.0003  max mem: 31081
AFTER Batch: 370, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.84 GB / 503.51 GB
Epoch: [24]  [370/893]  eta: 0:13:04  lr: 0.001328  min_lr: 0.000002  loss: 0.3049 (0.3064)  class_acc: 0.8750 (0.8683)  loss_scale: 4096.0000 (6061.1968)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0004  max mem: 31081
AFTER Batch: 380, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.57 GB / 503.51 GB
Epoch: [24]  [380/893]  eta: 0:12:48  lr: 0.001327  min_lr: 0.000002  loss: 0.3079 (0.3066)  class_acc: 0.8393 (0.8679)  loss_scale: 4096.0000 (6009.6168)  weight_decay: 0.0500 (0.0500)  time: 1.4659  data: 0.0004  max mem: 31081
AFTER Batch: 390, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.56 GB / 503.51 GB
Epoch: [24]  [390/893]  eta: 0:12:33  lr: 0.001326  min_lr: 0.000002  loss: 0.2720 (0.3061)  class_acc: 0.8571 (0.8681)  loss_scale: 4096.0000 (5960.6752)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0003  max mem: 31081
AFTER Batch: 400, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.56 GB / 503.51 GB
Epoch: [24]  [400/893]  eta: 0:12:18  lr: 0.001325  min_lr: 0.000002  loss: 0.2737 (0.3065)  class_acc: 0.8571 (0.8679)  loss_scale: 4096.0000 (5914.1746)  weight_decay: 0.0500 (0.0500)  time: 1.4647  data: 0.0003  max mem: 31081
AFTER Batch: 410, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.48 GB / 503.51 GB
Epoch: [24]  [410/893]  eta: 0:12:02  lr: 0.001324  min_lr: 0.000002  loss: 0.2854 (0.3061)  class_acc: 0.8750 (0.8682)  loss_scale: 4096.0000 (5869.9367)  weight_decay: 0.0500 (0.0500)  time: 1.4703  data: 0.0003  max mem: 31081
AFTER Batch: 420, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.27 GB / 503.51 GB
Epoch: [24]  [420/893]  eta: 0:11:47  lr: 0.001323  min_lr: 0.000002  loss: 0.2937 (0.3062)  class_acc: 0.8750 (0.8680)  loss_scale: 4096.0000 (5827.8005)  weight_decay: 0.0500 (0.0500)  time: 1.4656  data: 0.0002  max mem: 31081
AFTER Batch: 430, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.41 GB / 503.51 GB
Epoch: [24]  [430/893]  eta: 0:11:32  lr: 0.001323  min_lr: 0.000002  loss: 0.3157 (0.3063)  class_acc: 0.8571 (0.8682)  loss_scale: 4096.0000 (5787.6195)  weight_decay: 0.0500 (0.0500)  time: 1.4639  data: 0.0002  max mem: 31081
[2025-03-11 08:41:06,305] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 08:41:06,305] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 440, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.00 GB / 503.51 GB
Epoch: [24]  [440/893]  eta: 0:11:16  lr: 0.001322  min_lr: 0.000002  loss: 0.3401 (0.3070)  class_acc: 0.8571 (0.8680)  loss_scale: 4096.0000 (5814.2766)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0002  max mem: 31081
AFTER Batch: 450, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.20 GB / 503.51 GB
Epoch: [24]  [450/893]  eta: 0:11:01  lr: 0.001321  min_lr: 0.000002  loss: 0.3015 (0.3067)  class_acc: 0.8571 (0.8680)  loss_scale: 8192.0000 (5866.9978)  weight_decay: 0.0500 (0.0500)  time: 1.4616  data: 0.0002  max mem: 31081
AFTER Batch: 460, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.80 GB / 503.51 GB
Epoch: [24]  [460/893]  eta: 0:10:46  lr: 0.001320  min_lr: 0.000002  loss: 0.2903 (0.3063)  class_acc: 0.8571 (0.8681)  loss_scale: 8192.0000 (5917.4317)  weight_decay: 0.0500 (0.0500)  time: 1.4612  data: 0.0003  max mem: 31081
AFTER Batch: 470, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.44 GB / 503.51 GB
Epoch: [24]  [470/893]  eta: 0:10:31  lr: 0.001319  min_lr: 0.000002  loss: 0.2435 (0.3054)  class_acc: 0.8750 (0.8687)  loss_scale: 8192.0000 (5965.7240)  weight_decay: 0.0500 (0.0500)  time: 1.4572  data: 0.0004  max mem: 31081
AFTER Batch: 480, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.69 GB / 503.51 GB
Epoch: [24]  [480/893]  eta: 0:10:15  lr: 0.001318  min_lr: 0.000002  loss: 0.2456 (0.3051)  class_acc: 0.8929 (0.8691)  loss_scale: 8192.0000 (6012.0083)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0003  max mem: 31081
AFTER Batch: 490, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.71 GB / 503.51 GB
Epoch: [24]  [490/893]  eta: 0:10:00  lr: 0.001318  min_lr: 0.000002  loss: 0.2837 (0.3053)  class_acc: 0.8571 (0.8689)  loss_scale: 8192.0000 (6056.4073)  weight_decay: 0.0500 (0.0500)  time: 1.4685  data: 0.0003  max mem: 31081
AFTER Batch: 500, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.05 GB / 503.51 GB
Epoch: [24]  [500/893]  eta: 0:09:45  lr: 0.001317  min_lr: 0.000002  loss: 0.3228 (0.3056)  class_acc: 0.8571 (0.8688)  loss_scale: 8192.0000 (6099.0339)  weight_decay: 0.0500 (0.0500)  time: 1.4698  data: 0.0003  max mem: 31081
AFTER Batch: 510, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.40 GB / 503.51 GB
Epoch: [24]  [510/893]  eta: 0:09:30  lr: 0.001316  min_lr: 0.000002  loss: 0.3037 (0.3055)  class_acc: 0.8571 (0.8687)  loss_scale: 8192.0000 (6139.9922)  weight_decay: 0.0500 (0.0500)  time: 1.4615  data: 0.0003  max mem: 31081
AFTER Batch: 520, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.70 GB / 503.51 GB
Epoch: [24]  [520/893]  eta: 0:09:15  lr: 0.001315  min_lr: 0.000002  loss: 0.2825 (0.3046)  class_acc: 0.8571 (0.8688)  loss_scale: 8192.0000 (6179.3781)  weight_decay: 0.0500 (0.0500)  time: 1.4567  data: 0.0003  max mem: 31081
AFTER Batch: 530, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.58 GB / 503.51 GB
Epoch: [24]  [530/893]  eta: 0:09:00  lr: 0.001314  min_lr: 0.000002  loss: 0.2489 (0.3041)  class_acc: 0.8750 (0.8691)  loss_scale: 8192.0000 (6217.2806)  weight_decay: 0.0500 (0.0500)  time: 1.4603  data: 0.0003  max mem: 31081
AFTER Batch: 540, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.12 GB / 503.51 GB
Epoch: [24]  [540/893]  eta: 0:08:45  lr: 0.001313  min_lr: 0.000002  loss: 0.2489 (0.3035)  class_acc: 0.8929 (0.8697)  loss_scale: 8192.0000 (6253.7819)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0002  max mem: 31081
AFTER Batch: 550, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.62 GB / 503.51 GB
Epoch: [24]  [550/893]  eta: 0:08:30  lr: 0.001313  min_lr: 0.000002  loss: 0.2739 (0.3035)  class_acc: 0.8750 (0.8693)  loss_scale: 8192.0000 (6288.9583)  weight_decay: 0.0500 (0.0500)  time: 1.4674  data: 0.0002  max mem: 31081
AFTER Batch: 560, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.66 GB / 503.51 GB
Epoch: [24]  [560/893]  eta: 0:08:15  lr: 0.001312  min_lr: 0.000002  loss: 0.2871 (0.3034)  class_acc: 0.8571 (0.8696)  loss_scale: 8192.0000 (6322.8806)  weight_decay: 0.0500 (0.0500)  time: 1.4722  data: 0.0002  max mem: 31081
[2025-03-11 08:44:13,794] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 08:44:13,794] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-03-11 08:44:19,554] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 21974
[2025-03-11 08:44:19,554] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-03-11 08:44:19,554] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
AFTER Batch: 570, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.90 GB / 503.51 GB
Epoch: [24]  [570/893]  eta: 0:08:00  lr: 0.001311  min_lr: 0.000002  loss: 0.2844 (0.3032)  class_acc: 0.8750 (0.8698)  loss_scale: 8192.0000 (6413.0018)  weight_decay: 0.0500 (0.0500)  time: 1.4661  data: 0.0003  max mem: 31081
AFTER Batch: 580, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.24 GB / 503.51 GB
Epoch: [24]  [580/893]  eta: 0:07:45  lr: 0.001310  min_lr: 0.000002  loss: 0.2839 (0.3030)  class_acc: 0.8750 (0.8699)  loss_scale: 8192.0000 (6443.6213)  weight_decay: 0.0500 (0.0500)  time: 1.4602  data: 0.0002  max mem: 31081
AFTER Batch: 590, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.60 GB / 503.51 GB
Epoch: [24]  [590/893]  eta: 0:07:30  lr: 0.001309  min_lr: 0.000002  loss: 0.2893 (0.3027)  class_acc: 0.8571 (0.8698)  loss_scale: 8192.0000 (6473.2047)  weight_decay: 0.0500 (0.0500)  time: 1.4696  data: 0.0002  max mem: 31081
[2025-03-11 08:44:56,229] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 21999
[2025-03-11 08:44:56,229] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 08:44:56,229] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2025-03-11 08:44:56,229] [INFO] [logging.py:129:log_dist] [Rank 0] step=22000, skipped=130, lr=[1.7097567002723693e-06, 1.7097567002723693e-06, 2.849594500453949e-06, 2.849594500453949e-06, 4.7493241674232485e-06, 4.7493241674232485e-06, 7.915540279038747e-06, 7.915540279038747e-06, 1.3192567131731248e-05, 1.3192567131731248e-05, 2.1987611886218744e-05, 2.1987611886218744e-05, 3.664601981036458e-05, 3.664601981036458e-05, 6.107669968394096e-05, 6.107669968394096e-05, 0.00010179449947323494, 0.00010179449947323494, 0.00016965749912205828, 0.00016965749912205828, 0.0002827624985367637, 0.0002827624985367637, 0.0004712708308946063, 0.0004712708308946063, 0.0007854513848243439, 0.0007854513848243439, 0.0013090856413739064, 0.0013090856413739064], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-03-11 08:44:56,230] [INFO] [timer.py:264:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=60.98876610538706, CurrSamplesPerSec=62.232268836525805, MemAllocated=0.57GB, MaxMemAllocated=30.35GB
AFTER Batch: 600, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.11 GB / 503.51 GB
Epoch: [24]  [600/893]  eta: 0:07:15  lr: 0.001308  min_lr: 0.000002  loss: 0.2725 (0.3022)  class_acc: 0.8571 (0.8700)  loss_scale: 4096.0000 (6433.6506)  weight_decay: 0.0500 (0.0500)  time: 1.4675  data: 0.0002  max mem: 31081
AFTER Batch: 610, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.21 GB / 503.51 GB
Epoch: [24]  [610/893]  eta: 0:07:00  lr: 0.001307  min_lr: 0.000002  loss: 0.2761 (0.3027)  class_acc: 0.8571 (0.8698)  loss_scale: 4096.0000 (6395.3912)  weight_decay: 0.0500 (0.0500)  time: 1.4649  data: 0.0003  max mem: 31081
AFTER Batch: 620, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.09 GB / 503.51 GB
Epoch: [24]  [620/893]  eta: 0:06:45  lr: 0.001307  min_lr: 0.000002  loss: 0.2766 (0.3026)  class_acc: 0.8750 (0.8698)  loss_scale: 4096.0000 (6358.3639)  weight_decay: 0.0500 (0.0500)  time: 1.4667  data: 0.0003  max mem: 31081
AFTER Batch: 630, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.93 GB / 503.51 GB
Epoch: [24]  [630/893]  eta: 0:06:30  lr: 0.001306  min_lr: 0.000002  loss: 0.2578 (0.3022)  class_acc: 0.8929 (0.8700)  loss_scale: 4096.0000 (6322.5103)  weight_decay: 0.0500 (0.0500)  time: 1.4653  data: 0.0003  max mem: 31081
AFTER Batch: 640, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.37 GB / 503.51 GB
Epoch: [24]  [640/893]  eta: 0:06:15  lr: 0.001305  min_lr: 0.000002  loss: 0.2822 (0.3023)  class_acc: 0.8929 (0.8700)  loss_scale: 4096.0000 (6287.7754)  weight_decay: 0.0500 (0.0500)  time: 1.4655  data: 0.0004  max mem: 31081
AFTER Batch: 650, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.33 GB / 503.51 GB
Epoch: [24]  [650/893]  eta: 0:06:00  lr: 0.001304  min_lr: 0.000002  loss: 0.2915 (0.3022)  class_acc: 0.8571 (0.8697)  loss_scale: 4096.0000 (6254.1075)  weight_decay: 0.0500 (0.0500)  time: 1.4680  data: 0.0003  max mem: 31081
AFTER Batch: 660, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.85 GB / 503.51 GB
Epoch: [24]  [660/893]  eta: 0:05:45  lr: 0.001303  min_lr: 0.000002  loss: 0.2837 (0.3018)  class_acc: 0.8571 (0.8698)  loss_scale: 4096.0000 (6221.4584)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0002  max mem: 31081
AFTER Batch: 670, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.85 GB / 503.51 GB
Epoch: [24]  [670/893]  eta: 0:05:30  lr: 0.001302  min_lr: 0.000002  loss: 0.2878 (0.3022)  class_acc: 0.8750 (0.8697)  loss_scale: 4096.0000 (6189.7824)  weight_decay: 0.0500 (0.0500)  time: 1.4693  data: 0.0003  max mem: 31081
AFTER Batch: 680, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.15 GB / 503.51 GB
Epoch: [24]  [680/893]  eta: 0:05:16  lr: 0.001302  min_lr: 0.000002  loss: 0.2991 (0.3020)  class_acc: 0.8750 (0.8699)  loss_scale: 4096.0000 (6159.0367)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0003  max mem: 31081
AFTER Batch: 690, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.94 GB / 503.51 GB
Epoch: [24]  [690/893]  eta: 0:05:01  lr: 0.001301  min_lr: 0.000002  loss: 0.2820 (0.3018)  class_acc: 0.8571 (0.8699)  loss_scale: 4096.0000 (6129.1809)  weight_decay: 0.0500 (0.0500)  time: 1.4704  data: 0.0003  max mem: 31081
AFTER Batch: 700, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.91 GB / 503.51 GB
Epoch: [24]  [700/893]  eta: 0:04:46  lr: 0.001300  min_lr: 0.000002  loss: 0.2739 (0.3015)  class_acc: 0.8571 (0.8700)  loss_scale: 4096.0000 (6100.1769)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0003  max mem: 31081
AFTER Batch: 710, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.18 GB / 503.51 GB
Epoch: [24]  [710/893]  eta: 0:04:31  lr: 0.001299  min_lr: 0.000002  loss: 0.2747 (0.3013)  class_acc: 0.8750 (0.8701)  loss_scale: 4096.0000 (6071.9887)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0002  max mem: 31081
[2025-03-11 08:48:05,572] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 08:48:05,572] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 720, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.65 GB / 503.51 GB
Epoch: [24]  [720/893]  eta: 0:04:16  lr: 0.001298  min_lr: 0.000002  loss: 0.3015 (0.3018)  class_acc: 0.8571 (0.8698)  loss_scale: 4096.0000 (6050.2635)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
AFTER Batch: 730, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.03 GB / 503.51 GB
Epoch: [24]  [730/893]  eta: 0:04:01  lr: 0.001297  min_lr: 0.000002  loss: 0.2913 (0.3014)  class_acc: 0.8571 (0.8699)  loss_scale: 8192.0000 (6079.5622)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 740, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.41 GB / 503.51 GB
Epoch: [24]  [740/893]  eta: 0:03:46  lr: 0.001297  min_lr: 0.000002  loss: 0.2874 (0.3010)  class_acc: 0.8571 (0.8701)  loss_scale: 8192.0000 (6108.0702)  weight_decay: 0.0500 (0.0500)  time: 1.4624  data: 0.0002  max mem: 31081
AFTER Batch: 750, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.84 GB / 503.51 GB
Epoch: [24]  [750/893]  eta: 0:03:31  lr: 0.001296  min_lr: 0.000002  loss: 0.2920 (0.3013)  class_acc: 0.8750 (0.8701)  loss_scale: 8192.0000 (6135.8189)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 760, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.49 GB / 503.51 GB
Epoch: [24]  [760/893]  eta: 0:03:17  lr: 0.001295  min_lr: 0.000002  loss: 0.3281 (0.3017)  class_acc: 0.8571 (0.8698)  loss_scale: 8192.0000 (6162.8384)  weight_decay: 0.0500 (0.0500)  time: 1.4609  data: 0.0003  max mem: 31081
AFTER Batch: 770, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.55 GB / 503.51 GB
Epoch: [24]  [770/893]  eta: 0:03:02  lr: 0.001294  min_lr: 0.000002  loss: 0.3220 (0.3021)  class_acc: 0.8571 (0.8696)  loss_scale: 8192.0000 (6189.1569)  weight_decay: 0.0500 (0.0500)  time: 1.4621  data: 0.0002  max mem: 31081
AFTER Batch: 780, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.93 GB / 503.51 GB
Epoch: [24]  [780/893]  eta: 0:02:47  lr: 0.001293  min_lr: 0.000002  loss: 0.3030 (0.3018)  class_acc: 0.8571 (0.8696)  loss_scale: 8192.0000 (6214.8015)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0002  max mem: 31081
AFTER Batch: 790, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.79 GB / 503.51 GB
Epoch: [24]  [790/893]  eta: 0:02:32  lr: 0.001292  min_lr: 0.000002  loss: 0.2795 (0.3018)  class_acc: 0.8571 (0.8695)  loss_scale: 8192.0000 (6239.7977)  weight_decay: 0.0500 (0.0500)  time: 1.4601  data: 0.0002  max mem: 31081
AFTER Batch: 800, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 85.03 GB / 503.51 GB
Epoch: [24]  [800/893]  eta: 0:02:17  lr: 0.001292  min_lr: 0.000002  loss: 0.2698 (0.3014)  class_acc: 0.8750 (0.8696)  loss_scale: 8192.0000 (6264.1698)  weight_decay: 0.0500 (0.0500)  time: 1.4600  data: 0.0002  max mem: 31081
AFTER Batch: 810, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.65 GB / 503.51 GB
Epoch: [24]  [810/893]  eta: 0:02:02  lr: 0.001291  min_lr: 0.000002  loss: 0.2788 (0.3015)  class_acc: 0.8750 (0.8696)  loss_scale: 8192.0000 (6287.9408)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0004  max mem: 31081
[2025-03-11 08:50:21,591] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 22221
[2025-03-11 08:50:21,591] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 08:50:21,591] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 820, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.61 GB / 503.51 GB
Epoch: [24]  [820/893]  eta: 0:01:48  lr: 0.001290  min_lr: 0.000002  loss: 0.3022 (0.3017)  class_acc: 0.8571 (0.8695)  loss_scale: 8192.0000 (6271.2205)  weight_decay: 0.0500 (0.0500)  time: 1.4670  data: 0.0004  max mem: 31081
AFTER Batch: 830, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.56 GB / 503.51 GB
Epoch: [24]  [830/893]  eta: 0:01:33  lr: 0.001289  min_lr: 0.000002  loss: 0.2820 (0.3016)  class_acc: 0.8750 (0.8696)  loss_scale: 4096.0000 (6245.0445)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 840, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.05 GB / 503.51 GB
Epoch: [24]  [840/893]  eta: 0:01:18  lr: 0.001288  min_lr: 0.000002  loss: 0.2988 (0.3019)  class_acc: 0.8750 (0.8695)  loss_scale: 4096.0000 (6219.4911)  weight_decay: 0.0500 (0.0500)  time: 1.4654  data: 0.0003  max mem: 31081
AFTER Batch: 850, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.98 GB / 503.51 GB
Epoch: [24]  [850/893]  eta: 0:01:03  lr: 0.001287  min_lr: 0.000002  loss: 0.3020 (0.3018)  class_acc: 0.8750 (0.8696)  loss_scale: 4096.0000 (6194.5382)  weight_decay: 0.0500 (0.0500)  time: 1.4625  data: 0.0003  max mem: 31081
AFTER Batch: 860, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.98 GB / 503.51 GB
Epoch: [24]  [860/893]  eta: 0:00:48  lr: 0.001286  min_lr: 0.000002  loss: 0.3113 (0.3022)  class_acc: 0.8571 (0.8692)  loss_scale: 4096.0000 (6170.1649)  weight_decay: 0.0500 (0.0500)  time: 1.4510  data: 0.0002  max mem: 31081
AFTER Batch: 870, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.96 GB / 503.51 GB
Epoch: [24]  [870/893]  eta: 0:00:34  lr: 0.001286  min_lr: 0.000002  loss: 0.2986 (0.3022)  class_acc: 0.8571 (0.8693)  loss_scale: 4096.0000 (6146.3513)  weight_decay: 0.0500 (0.0500)  time: 1.4356  data: 0.0001  max mem: 31081
AFTER Batch: 880, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.99 GB / 503.51 GB
Epoch: [24]  [880/893]  eta: 0:00:19  lr: 0.001285  min_lr: 0.000002  loss: 0.2712 (0.3019)  class_acc: 0.8929 (0.8695)  loss_scale: 4096.0000 (6123.0783)  weight_decay: 0.0500 (0.0500)  time: 1.4335  data: 0.0001  max mem: 31081
AFTER Batch: 890, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.99 GB / 503.51 GB
Epoch: [24]  [890/893]  eta: 0:00:04  lr: 0.001284  min_lr: 0.000002  loss: 0.2712 (0.3018)  class_acc: 0.8750 (0.8696)  loss_scale: 4096.0000 (6100.3277)  weight_decay: 0.0500 (0.0500)  time: 1.4335  data: 0.0001  max mem: 31081
Epoch: [24]  [892/893]  eta: 0:00:01  lr: 0.001284  min_lr: 0.000002  loss: 0.2769 (0.3018)  class_acc: 0.8750 (0.8696)  loss_scale: 4096.0000 (6098.0807)  weight_decay: 0.0500 (0.0500)  time: 1.3838  data: 0.0001  max mem: 31081
Epoch: [24] Total time: 0:21:58 (1.4770 s / it)
Averaged stats: lr: 0.001284  min_lr: 0.000002  loss: 0.2769 (0.3018)  class_acc: 0.8750 (0.8696)  loss_scale: 4096.0000 (6098.0807)  weight_decay: 0.0500 (0.0500)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/engine_for_frame_finetuning.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Val:  [  0/728]  eta: 2:18:27  loss: 0.9654 (0.9654)  acc: 72.6190 (72.6190)  time: 11.4116  data: 10.8921  max mem: 31081
Val:  [ 10/728]  eta: 0:18:39  loss: 0.4141 (0.5026)  acc: 83.3333 (81.9264)  time: 1.5591  data: 1.0381  max mem: 31081
Val:  [ 20/728]  eta: 0:14:06  loss: 0.4134 (0.4759)  acc: 80.9524 (81.4059)  time: 0.6854  data: 0.1638  max mem: 31081
Val:  [ 30/728]  eta: 0:12:36  loss: 0.4821 (0.4929)  acc: 78.5714 (79.9539)  time: 0.8221  data: 0.2981  max mem: 31081
Val:  [ 40/728]  eta: 0:11:44  loss: 0.5133 (0.5256)  acc: 75.0000 (78.3391)  time: 0.8432  data: 0.3186  max mem: 31081
Val:  [ 50/728]  eta: 0:11:08  loss: 0.4788 (0.5144)  acc: 76.1905 (79.2717)  time: 0.8339  data: 0.3114  max mem: 31081
Val:  [ 60/728]  eta: 0:10:21  loss: 0.5869 (0.5297)  acc: 79.7619 (78.8447)  time: 0.7363  data: 0.2130  max mem: 31081
Val:  [ 70/728]  eta: 0:09:48  loss: 0.6035 (0.5528)  acc: 75.0000 (78.2696)  time: 0.6639  data: 0.1413  max mem: 31081
Val:  [ 80/728]  eta: 0:09:33  loss: 0.5775 (0.5725)  acc: 73.8095 (77.6455)  time: 0.7495  data: 0.2257  max mem: 31081
Val:  [ 90/728]  eta: 0:09:18  loss: 0.6192 (0.6054)  acc: 71.4286 (76.6876)  time: 0.8073  data: 0.2836  max mem: 31081
Val:  [100/728]  eta: 0:09:07  loss: 0.5141 (0.5848)  acc: 78.5714 (77.2513)  time: 0.8212  data: 0.3010  max mem: 31081
Val:  [110/728]  eta: 0:08:57  loss: 0.3559 (0.5874)  acc: 80.9524 (77.1665)  time: 0.8462  data: 0.3248  max mem: 31081
Val:  [120/728]  eta: 0:08:44  loss: 0.3556 (0.5697)  acc: 83.3333 (77.7843)  time: 0.8147  data: 0.2918  max mem: 31081
Val:  [130/728]  eta: 0:08:23  loss: 0.3347 (0.5745)  acc: 83.3333 (77.9262)  time: 0.6816  data: 0.1577  max mem: 31081
Val:  [140/728]  eta: 0:08:13  loss: 0.4228 (0.5753)  acc: 80.9524 (77.9129)  time: 0.6906  data: 0.1638  max mem: 31081
Val:  [150/728]  eta: 0:07:59  loss: 0.4382 (0.5742)  acc: 83.3333 (77.9959)  time: 0.7448  data: 0.2184  max mem: 31081
Val:  [160/728]  eta: 0:07:50  loss: 0.5131 (0.5787)  acc: 77.3810 (77.8394)  time: 0.7587  data: 0.2355  max mem: 31081
Val:  [170/728]  eta: 0:07:41  loss: 0.5131 (0.5804)  acc: 77.3810 (77.8474)  time: 0.8111  data: 0.2868  max mem: 31081
Val:  [180/728]  eta: 0:07:31  loss: 0.4665 (0.5809)  acc: 77.3810 (77.8150)  time: 0.7830  data: 0.2586  max mem: 31081
Val:  [190/728]  eta: 0:07:17  loss: 0.5615 (0.5771)  acc: 77.3810 (77.9606)  time: 0.6941  data: 0.1740  max mem: 31081
Val:  [200/728]  eta: 0:07:08  loss: 0.4667 (0.5707)  acc: 80.9524 (78.0798)  time: 0.7024  data: 0.1816  max mem: 31081
Val:  [210/728]  eta: 0:07:03  loss: 0.4667 (0.5769)  acc: 76.1905 (77.9508)  time: 0.8545  data: 0.3334  max mem: 31081
Val:  [220/728]  eta: 0:06:56  loss: 0.4792 (0.5755)  acc: 78.5714 (78.0112)  time: 0.8982  data: 0.3775  max mem: 31081
Val:  [230/728]  eta: 0:06:47  loss: 0.4807 (0.5797)  acc: 80.9524 (77.8809)  time: 0.8280  data: 0.3056  max mem: 31081
Val:  [240/728]  eta: 0:06:35  loss: 0.6717 (0.5876)  acc: 77.3810 (77.7465)  time: 0.7234  data: 0.2018  max mem: 31081
Val:  [250/728]  eta: 0:06:24  loss: 0.5651 (0.5874)  acc: 77.3810 (77.8126)  time: 0.6464  data: 0.1270  max mem: 31081
Val:  [260/728]  eta: 0:06:17  loss: 0.3963 (0.5862)  acc: 79.7619 (77.9101)  time: 0.7563  data: 0.2356  max mem: 31081
Val:  [270/728]  eta: 0:06:09  loss: 0.3818 (0.5870)  acc: 79.7619 (77.8686)  time: 0.8459  data: 0.3227  max mem: 31081
Val:  [280/728]  eta: 0:06:04  loss: 0.4339 (0.5868)  acc: 78.5714 (77.8554)  time: 0.8990  data: 0.3772  max mem: 31081
Val:  [290/728]  eta: 0:05:56  loss: 0.4387 (0.5819)  acc: 79.7619 (78.0028)  time: 0.8961  data: 0.3709  max mem: 31081
Val:  [300/728]  eta: 0:05:43  loss: 0.4721 (0.5809)  acc: 79.7619 (78.1008)  time: 0.6629  data: 0.1368  max mem: 31081
Val:  [310/728]  eta: 0:05:36  loss: 0.6675 (0.5854)  acc: 77.3810 (77.9207)  time: 0.6750  data: 0.1514  max mem: 31081
Val:  [320/728]  eta: 0:05:27  loss: 0.6675 (0.5840)  acc: 73.8095 (77.9113)  time: 0.8044  data: 0.2812  max mem: 31081
Val:  [330/728]  eta: 0:05:20  loss: 0.4065 (0.5792)  acc: 79.7619 (78.0247)  time: 0.8355  data: 0.3115  max mem: 31081
Val:  [340/728]  eta: 0:05:13  loss: 0.4065 (0.5739)  acc: 80.9524 (78.1525)  time: 0.8886  data: 0.3672  max mem: 31081
Val:  [350/728]  eta: 0:05:05  loss: 0.4177 (0.5746)  acc: 80.9524 (78.2017)  time: 0.8442  data: 0.3258  max mem: 31081
Val:  [360/728]  eta: 0:04:54  loss: 0.4357 (0.5691)  acc: 80.9524 (78.3505)  time: 0.6618  data: 0.1402  max mem: 31081
Val:  [370/728]  eta: 0:04:47  loss: 0.4357 (0.5718)  acc: 80.9524 (78.2794)  time: 0.6925  data: 0.1697  max mem: 31081
Val:  [380/728]  eta: 0:04:39  loss: 0.4915 (0.5765)  acc: 76.1905 (78.2059)  time: 0.8485  data: 0.3265  max mem: 31081
Val:  [390/728]  eta: 0:04:31  loss: 0.3745 (0.5700)  acc: 80.9524 (78.3705)  time: 0.8339  data: 0.3111  max mem: 31081
Val:  [400/728]  eta: 0:04:24  loss: 0.3745 (0.5696)  acc: 83.3333 (78.4171)  time: 0.8552  data: 0.3329  max mem: 31081
Val:  [410/728]  eta: 0:04:16  loss: 0.4226 (0.5688)  acc: 80.9524 (78.4266)  time: 0.8658  data: 0.3425  max mem: 31081
Val:  [420/728]  eta: 0:04:06  loss: 0.4837 (0.5684)  acc: 80.9524 (78.4696)  time: 0.6894  data: 0.1638  max mem: 31081
Val:  [430/728]  eta: 0:03:58  loss: 0.4455 (0.5683)  acc: 79.7619 (78.4637)  time: 0.6985  data: 0.1723  max mem: 31081
Val:  [440/728]  eta: 0:03:50  loss: 0.3938 (0.5651)  acc: 82.1429 (78.5066)  time: 0.8338  data: 0.3098  max mem: 31081
Val:  [450/728]  eta: 0:03:43  loss: 0.4117 (0.5656)  acc: 82.1429 (78.5107)  time: 0.8281  data: 0.3073  max mem: 31081
Val:  [460/728]  eta: 0:03:35  loss: 0.6053 (0.5680)  acc: 79.7619 (78.4242)  time: 0.8388  data: 0.3174  max mem: 31081
Val:  [470/728]  eta: 0:03:27  loss: 0.4714 (0.5649)  acc: 78.5714 (78.5108)  time: 0.7938  data: 0.2714  max mem: 31081
Val:  [480/728]  eta: 0:03:17  loss: 0.4434 (0.5638)  acc: 78.5714 (78.5615)  time: 0.6434  data: 0.1237  max mem: 31081
Val:  [490/728]  eta: 0:03:09  loss: 0.4949 (0.5637)  acc: 76.1905 (78.5302)  time: 0.6905  data: 0.1698  max mem: 31081
Val:  [500/728]  eta: 0:03:02  loss: 0.4630 (0.5654)  acc: 80.9524 (78.5500)  time: 0.8440  data: 0.3211  max mem: 31081
Val:  [510/728]  eta: 0:02:54  loss: 0.3632 (0.5622)  acc: 83.3333 (78.6157)  time: 0.8653  data: 0.3438  max mem: 31081
Val:  [520/728]  eta: 0:02:46  loss: 0.4381 (0.5642)  acc: 78.5714 (78.4915)  time: 0.8481  data: 0.3268  max mem: 31081
Val:  [530/728]  eta: 0:02:38  loss: 0.5443 (0.5638)  acc: 76.1905 (78.5109)  time: 0.7471  data: 0.2239  max mem: 31081
Val:  [540/728]  eta: 0:02:29  loss: 0.5848 (0.5666)  acc: 75.0000 (78.3514)  time: 0.6131  data: 0.0896  max mem: 31081
Val:  [550/728]  eta: 0:02:21  loss: 0.5848 (0.5678)  acc: 70.2381 (78.2819)  time: 0.6694  data: 0.1479  max mem: 31081
Val:  [560/728]  eta: 0:02:13  loss: 0.4867 (0.5672)  acc: 77.3810 (78.3168)  time: 0.7586  data: 0.2353  max mem: 31081
Val:  [570/728]  eta: 0:02:05  loss: 0.5202 (0.5722)  acc: 77.3810 (78.2399)  time: 0.7950  data: 0.2704  max mem: 31081
Val:  [580/728]  eta: 0:01:57  loss: 0.5263 (0.5729)  acc: 78.5714 (78.2682)  time: 0.8394  data: 0.3166  max mem: 31081
Val:  [590/728]  eta: 0:01:49  loss: 0.5778 (0.5742)  acc: 79.7619 (78.2471)  time: 0.7504  data: 0.2290  max mem: 31081
Val:  [600/728]  eta: 0:01:41  loss: 0.5585 (0.5743)  acc: 80.9524 (78.2486)  time: 0.6592  data: 0.1402  max mem: 31081
Val:  [610/728]  eta: 0:01:32  loss: 0.5508 (0.5771)  acc: 79.7619 (78.1525)  time: 0.6493  data: 0.1282  max mem: 31081
Val:  [620/728]  eta: 0:01:25  loss: 0.4823 (0.5776)  acc: 78.5714 (78.1535)  time: 0.7747  data: 0.2525  max mem: 31081
Val:  [630/728]  eta: 0:01:17  loss: 0.4014 (0.5757)  acc: 79.7619 (78.2224)  time: 0.8520  data: 0.3302  max mem: 31081
Val:  [640/728]  eta: 0:01:09  loss: 0.3762 (0.5742)  acc: 83.3333 (78.2798)  time: 0.7944  data: 0.2709  max mem: 31081
Val:  [650/728]  eta: 0:01:01  loss: 0.4490 (0.5736)  acc: 80.9524 (78.3026)  time: 0.7845  data: 0.2619  max mem: 31081
Val:  [660/728]  eta: 0:00:53  loss: 0.4549 (0.5737)  acc: 79.7619 (78.3121)  time: 0.7762  data: 0.2542  max mem: 31081
Val:  [670/728]  eta: 0:00:45  loss: 0.4602 (0.5749)  acc: 79.7619 (78.3053)  time: 0.6829  data: 0.1594  max mem: 31081
Val:  [680/728]  eta: 0:00:37  loss: 0.4602 (0.5737)  acc: 79.7619 (78.3424)  time: 0.6789  data: 0.1560  max mem: 31081
Val:  [690/728]  eta: 0:00:29  loss: 0.3730 (0.5729)  acc: 83.3333 (78.3578)  time: 0.8201  data: 0.2990  max mem: 31081
Val:  [700/728]  eta: 0:00:22  loss: 0.4204 (0.5721)  acc: 79.7619 (78.3693)  time: 0.8477  data: 0.3246  max mem: 31081
Val:  [710/728]  eta: 0:00:14  loss: 0.4204 (0.5720)  acc: 79.7619 (78.3789)  time: 0.8060  data: 0.2855  max mem: 31081
Val:  [720/728]  eta: 0:00:06  loss: 0.5064 (0.5715)  acc: 78.5714 (78.3948)  time: 0.7927  data: 0.2801  max mem: 31081
Val:  [727/728]  eta: 0:00:00  loss: 0.5810 (0.5754)  acc: 77.3810 (78.3259)  time: 0.7700  data: 0.2801  max mem: 31081
Val: Total time: 0:09:31 (0.7845 s / it)
* Acc@1 78.326 AP 0.7894619107246399 loss 0.575
Accuracy of the network on the 61096 val videos: 78.3%
Max accuracy: 0.86%
	training another epoch...
AFTER Batch: 0, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.28 GB / 503.51 GB
Epoch: [25]  [  0/893]  eta: 3:24:43  lr: 0.001284  min_lr: 0.000002  loss: 0.3193 (0.3193)  class_acc: 0.8036 (0.8036)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 13.7548  data: 12.4507  max mem: 31081
AFTER Batch: 10, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.33 GB / 503.51 GB
Epoch: [25]  [ 10/893]  eta: 0:38:29  lr: 0.001283  min_lr: 0.000002  loss: 0.3184 (0.3101)  class_acc: 0.8393 (0.8539)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6153  data: 1.1323  max mem: 31081
AFTER Batch: 20, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.28 GB / 503.51 GB
Epoch: [25]  [ 20/893]  eta: 0:30:24  lr: 0.001282  min_lr: 0.000002  loss: 0.3137 (0.3120)  class_acc: 0.8571 (0.8520)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5064  data: 0.0004  max mem: 31081
AFTER Batch: 30, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.20 GB / 503.51 GB
Epoch: [25]  [ 30/893]  eta: 0:27:21  lr: 0.001281  min_lr: 0.000002  loss: 0.2905 (0.3045)  class_acc: 0.8571 (0.8594)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5092  data: 0.0010  max mem: 31081
AFTER Batch: 40, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 86.24 GB / 503.51 GB
Epoch: [25]  [ 40/893]  eta: 0:25:43  lr: 0.001280  min_lr: 0.000002  loss: 0.2837 (0.3013)  class_acc: 0.8750 (0.8619)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5153  data: 0.0011  max mem: 31081
[2025-03-11 09:03:16,584] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 09:03:16,584] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 50, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.62 GB / 503.51 GB
Epoch: [25]  [ 50/893]  eta: 0:24:31  lr: 0.001280  min_lr: 0.000002  loss: 0.2634 (0.2939)  class_acc: 0.8750 (0.8680)  loss_scale: 4096.0000 (4176.3137)  weight_decay: 0.0500 (0.0500)  time: 1.5049  data: 0.0005  max mem: 31081
AFTER Batch: 60, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.08 GB / 503.51 GB
Epoch: [25]  [ 60/893]  eta: 0:23:35  lr: 0.001279  min_lr: 0.000002  loss: 0.2216 (0.2853)  class_acc: 0.8929 (0.8747)  loss_scale: 8192.0000 (4834.6230)  weight_decay: 0.0500 (0.0500)  time: 1.4737  data: 0.0003  max mem: 31081
AFTER Batch: 70, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.54 GB / 503.51 GB
Epoch: [25]  [ 70/893]  eta: 0:22:50  lr: 0.001278  min_lr: 0.000002  loss: 0.2546 (0.2815)  class_acc: 0.8929 (0.8768)  loss_scale: 8192.0000 (5307.4930)  weight_decay: 0.0500 (0.0500)  time: 1.4603  data: 0.0003  max mem: 31081
AFTER Batch: 80, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 82.97 GB / 503.51 GB
Epoch: [25]  [ 80/893]  eta: 0:22:14  lr: 0.001277  min_lr: 0.000002  loss: 0.2583 (0.2828)  class_acc: 0.8929 (0.8781)  loss_scale: 8192.0000 (5663.6049)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 90, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.08 GB / 503.51 GB
Epoch: [25]  [ 90/893]  eta: 0:21:43  lr: 0.001276  min_lr: 0.000002  loss: 0.3120 (0.2904)  class_acc: 0.8750 (0.8740)  loss_scale: 8192.0000 (5941.4505)  weight_decay: 0.0500 (0.0500)  time: 1.4703  data: 0.0003  max mem: 31081
[2025-03-11 09:04:16,687] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 22391
[2025-03-11 09:04:16,688] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 09:04:16,688] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 100, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.28 GB / 503.51 GB
Epoch: [25]  [100/893]  eta: 0:21:14  lr: 0.001275  min_lr: 0.000002  loss: 0.3481 (0.2974)  class_acc: 0.8571 (0.8731)  loss_scale: 4096.0000 (5758.7327)  weight_decay: 0.0500 (0.0500)  time: 1.4717  data: 0.0003  max mem: 31081
AFTER Batch: 110, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.57 GB / 503.51 GB
Epoch: [25]  [110/893]  eta: 0:20:48  lr: 0.001274  min_lr: 0.000002  loss: 0.3264 (0.2973)  class_acc: 0.8750 (0.8724)  loss_scale: 4096.0000 (5608.9369)  weight_decay: 0.0500 (0.0500)  time: 1.4690  data: 0.0003  max mem: 31081
AFTER Batch: 120, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.03 GB / 503.51 GB
Epoch: [25]  [120/893]  eta: 0:20:25  lr: 0.001274  min_lr: 0.000002  loss: 0.2996 (0.2977)  class_acc: 0.8571 (0.8725)  loss_scale: 4096.0000 (5483.9008)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0004  max mem: 31081
AFTER Batch: 130, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.03 GB / 503.51 GB
Epoch: [25]  [130/893]  eta: 0:20:02  lr: 0.001273  min_lr: 0.000002  loss: 0.2986 (0.2977)  class_acc: 0.8571 (0.8713)  loss_scale: 4096.0000 (5377.9542)  weight_decay: 0.0500 (0.0500)  time: 1.4732  data: 0.0004  max mem: 31081
AFTER Batch: 140, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.41 GB / 503.51 GB
Epoch: [25]  [140/893]  eta: 0:19:41  lr: 0.001272  min_lr: 0.000002  loss: 0.2986 (0.2967)  class_acc: 0.8750 (0.8718)  loss_scale: 4096.0000 (5287.0355)  weight_decay: 0.0500 (0.0500)  time: 1.4730  data: 0.0003  max mem: 31081
AFTER Batch: 150, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.13 GB / 503.51 GB
Epoch: [25]  [150/893]  eta: 0:19:20  lr: 0.001271  min_lr: 0.000002  loss: 0.2727 (0.2975)  class_acc: 0.8750 (0.8705)  loss_scale: 4096.0000 (5208.1589)  weight_decay: 0.0500 (0.0500)  time: 1.4669  data: 0.0002  max mem: 31081
AFTER Batch: 160, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.07 GB / 503.51 GB
Epoch: [25]  [160/893]  eta: 0:19:00  lr: 0.001270  min_lr: 0.000002  loss: 0.2917 (0.2982)  class_acc: 0.8571 (0.8695)  loss_scale: 4096.0000 (5139.0807)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0003  max mem: 31081
AFTER Batch: 170, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.24 GB / 503.51 GB
Epoch: [25]  [170/893]  eta: 0:18:41  lr: 0.001269  min_lr: 0.000002  loss: 0.2917 (0.2954)  class_acc: 0.8571 (0.8704)  loss_scale: 4096.0000 (5078.0819)  weight_decay: 0.0500 (0.0500)  time: 1.4718  data: 0.0003  max mem: 31081
AFTER Batch: 180, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.23 GB / 503.51 GB
Epoch: [25]  [180/893]  eta: 0:18:22  lr: 0.001269  min_lr: 0.000002  loss: 0.2722 (0.2940)  class_acc: 0.8929 (0.8703)  loss_scale: 4096.0000 (5023.8232)  weight_decay: 0.0500 (0.0500)  time: 1.4700  data: 0.0003  max mem: 31081
AFTER Batch: 190, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.04 GB / 503.51 GB
Epoch: [25]  [190/893]  eta: 0:18:04  lr: 0.001268  min_lr: 0.000002  loss: 0.2830 (0.2944)  class_acc: 0.8571 (0.8708)  loss_scale: 4096.0000 (4975.2461)  weight_decay: 0.0500 (0.0500)  time: 1.4645  data: 0.0003  max mem: 31081
AFTER Batch: 200, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.71 GB / 503.51 GB
Epoch: [25]  [200/893]  eta: 0:17:46  lr: 0.001267  min_lr: 0.000002  loss: 0.2605 (0.2926)  class_acc: 0.8929 (0.8721)  loss_scale: 4096.0000 (4931.5025)  weight_decay: 0.0500 (0.0500)  time: 1.4684  data: 0.0002  max mem: 31081
AFTER Batch: 210, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.21 GB / 503.51 GB
Epoch: [25]  [210/893]  eta: 0:17:28  lr: 0.001266  min_lr: 0.000002  loss: 0.2605 (0.2924)  class_acc: 0.8929 (0.8720)  loss_scale: 4096.0000 (4891.9052)  weight_decay: 0.0500 (0.0500)  time: 1.4634  data: 0.0003  max mem: 31081
[2025-03-11 09:07:26,044] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 128 iterations
[2025-03-11 09:07:26,044] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
AFTER Batch: 220, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.68 GB / 503.51 GB
Epoch: [25]  [220/893]  eta: 0:17:10  lr: 0.001265  min_lr: 0.000002  loss: 0.2930 (0.2924)  class_acc: 0.8571 (0.8720)  loss_scale: 4096.0000 (4874.4253)  weight_decay: 0.0500 (0.0500)  time: 1.4607  data: 0.0004  max mem: 31081
AFTER Batch: 230, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.51 GB / 503.51 GB
Epoch: [25]  [230/893]  eta: 0:16:53  lr: 0.001264  min_lr: 0.000002  loss: 0.2974 (0.2925)  class_acc: 0.8571 (0.8719)  loss_scale: 8192.0000 (5018.0433)  weight_decay: 0.0500 (0.0500)  time: 1.4642  data: 0.0003  max mem: 31081
[2025-03-11 09:07:46,548] [INFO] [fused_optimizer.py:392:_update_scale] 
Grad overflow on iteration 22534
[2025-03-11 09:07:46,548] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2025-03-11 09:07:46,548] [INFO] [logging.py:129:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
AFTER Batch: 240, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.27 GB / 503.51 GB
Epoch: [25]  [240/893]  eta: 0:16:36  lr: 0.001264  min_lr: 0.000002  loss: 0.2986 (0.2929)  class_acc: 0.8571 (0.8717)  loss_scale: 8192.0000 (5030.7718)  weight_decay: 0.0500 (0.0500)  time: 1.4660  data: 0.0004  max mem: 31081
AFTER Batch: 250, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.58 GB / 503.51 GB
Epoch: [25]  [250/893]  eta: 0:16:19  lr: 0.001263  min_lr: 0.000002  loss: 0.2815 (0.2929)  class_acc: 0.8750 (0.8718)  loss_scale: 4096.0000 (4993.5299)  weight_decay: 0.0500 (0.0500)  time: 1.4648  data: 0.0004  max mem: 31081
AFTER Batch: 260, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.55 GB / 503.51 GB
Epoch: [25]  [260/893]  eta: 0:16:03  lr: 0.001262  min_lr: 0.000002  loss: 0.2815 (0.2926)  class_acc: 0.8571 (0.8718)  loss_scale: 4096.0000 (4959.1418)  weight_decay: 0.0500 (0.0500)  time: 1.4631  data: 0.0003  max mem: 31081
AFTER Batch: 270, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.53 GB / 503.51 GB
Epoch: [25]  [270/893]  eta: 0:15:46  lr: 0.001261  min_lr: 0.000002  loss: 0.3022 (0.2938)  class_acc: 0.8571 (0.8715)  loss_scale: 4096.0000 (4927.2915)  weight_decay: 0.0500 (0.0500)  time: 1.4643  data: 0.0003  max mem: 31081
AFTER Batch: 280, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.07 GB / 503.51 GB
Epoch: [25]  [280/893]  eta: 0:15:30  lr: 0.001260  min_lr: 0.000002  loss: 0.3184 (0.2960)  class_acc: 0.8571 (0.8709)  loss_scale: 4096.0000 (4897.7082)  weight_decay: 0.0500 (0.0500)  time: 1.4640  data: 0.0003  max mem: 31081
AFTER Batch: 290, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 83.83 GB / 503.51 GB
Epoch: [25]  [290/893]  eta: 0:15:13  lr: 0.001259  min_lr: 0.000002  loss: 0.3115 (0.2961)  class_acc: 0.8571 (0.8707)  loss_scale: 4096.0000 (4870.1581)  weight_decay: 0.0500 (0.0500)  time: 1.4622  data: 0.0004  max mem: 31081
AFTER Batch: 300, total batch size 56
------------GPU 0: Allocated: 0.32 GB, Reserved: 31.42 GB
------------CPU Memory Usage: 84.26 GB / 503.51 GB
Epoch: [25]  [300/893]  eta: 0:14:57  lr: 0.001258  min_lr: 0.000002  loss: 0.3022 (0.2964)  class_acc: 0.8571 (0.8704)  loss_scale: 4096.0000 (4844.4385)  weight_decay: 0.0500 (0.0500)  time: 1.4629  data: 0.0004  max mem: 31081
slurmstepd: error: *** JOB 10425261 ON gcn32 CANCELLED AT 2025-03-11T09:09:29 ***

JOB STATISTICS
==============
Job ID: 10425261
Cluster: snellius
User/Group: sorlova/sorlova
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:01
CPU Efficiency: 0.00% of 9-23:41:42 core-walltime
Job Wall-clock time: 13:18:59
Memory Utilized: 67.58 GB
Memory Efficiency: 56.32% of 120.00 GB
