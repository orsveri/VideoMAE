============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
/home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/flash_attn/ops/fused_dense.py:29: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/flash_attn/ops/fused_dense.py:70: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/flash_attn/ops/fused_dense.py:251: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/flash_attn/ops/fused_dense.py:348: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/InternVideo2_single_modality/models/internvl_clip_vision.py:145: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/InternVideo2_single_modality/models/internvideo2_teacher.py:153: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/InternVideo2_single_modality/models/internvideo2.py:140: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/InternVideo2_single_modality/models/internvideo2_cat.py:140: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/InternVideo2_single_modality/models/internvideo2_ap.py:140: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/InternVideo2_single_modality/models/internvideo2_pretrain.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/InternVideo2_single_modality/models/internvideo2_distill.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
[2025-02-06 23:43:47,337] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Distributed process initialized successfully.
	Rank 0, World Size: 1, Device: 0
Namespace(batch_size=550, epochs=20, update_freq=1, save_ckpt_freq=1, steps_per_print=1, use_ceph_checkpoint=False, ceph_checkpoint_prefix='', ckpt_path_split='/exp/', model='internvideo2_small_patch14_224', tubelet_size=1, input_size=224, layer_scale_init_value=1e-05, layerscale_no_force_fp32=False, sep_pos_embed=False, center_init=False, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, head_drop_path=0.0, qkv_bias=False, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, loss='crossentropy', opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.75, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=1, aa='rand-m3-n3-mstd0.5-inc1', smoothing=0.0, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=1, test_num_crop=1, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cutmix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='logs/baselines/bl3/9_dota_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-5.pth', delete_head=False, model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, checkpoint_num=0, use_mean_pooling=True, final_reduction='fc_norm', freeze_layers=None, prefix='', split=' ', filename_tmpl='img_{:05}.jpg', data_path='/gpfs/work3/0/tese0625/RiskNetData/DoTA_refined', eval_data_path=None, nb_classes=2, imagenet_default_mean_and_std=True, use_decord=False, num_segments=1, num_frames=16, sampling_rate=1, sampling_rate_val=-1, view_fps=10, data_set='DoTA', output_dir='logs/baselines/bl3/9_dota_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/eval_DoTA_ckpt_5', log_dir='logs/baselines/bl3/9_dota_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/eval_DoTA_ckpt_5', device='cuda', seed=0, resume='', auto_resume=True, nb_samples_per_epoch=0, save_ckpt=True, start_epoch=0, test_best=False, eval=True, dist_eval=True, num_workers=8, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, bf16=False, zero_stage=0, eval_option='9_5', deepspeed=False, deepspeed_config='logs/baselines/bl3/9_dota_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/eval_DoTA_ckpt_5/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')


===
[validation] | COUNT safe: 75084
COUNT risk: 46603
===
Number of the class = 2


===
[test] | COUNT safe: 75084
COUNT risk: 46603
===
Number of the class = 2
dset lengths: train <not used>, val <not used>
4
Use joint position embedding
Droppath rate: [0.0, 0.00909090880304575, 0.0181818176060915, 0.027272727340459824, 0.036363635212183, 0.045454543083906174, 0.054545458406209946, 0.06363636255264282, 0.0727272778749466, 0.08181818574666977, 0.09090909361839294, 0.10000000149011612]
Checkpoint list: [False, False, False, False, False, False, False, False, False, False, False, False]
Init pos_embed from sincos pos_embed
Patch size = (14, 14)
/gpfs/home1/sorlova/repos/AITHENA/NewStage/VideoMAE/iv2_sm_run_frame_finetuning.py:458: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.finetune, map_location='cpu')
Load ckpt from logs/baselines/bl3/9_dota_lr1e3_b56x1_dsampl1val2_ld06_aam6n3/checkpoint-5.pth
Temporal interpolate from 8 to 16
Model = InternVideo2(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(1, 14, 14), stride=(1, 14, 14))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): DropoutAddRMSNorm()
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (inner_attn): FlashAttention()
        (q_norm): DropoutAddRMSNorm()
        (k_norm): DropoutAddRMSNorm()
      )
      (ls1): LayerScale()
      (drop_path1): Identity()
      (norm2): DropoutAddRMSNorm()
      (mlp): FusedMLP(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
      )
      (ls2): LayerScale()
      (drop_path2): Identity()
    )
    (1-11): 11 x Block(
      (norm1): DropoutAddRMSNorm()
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (inner_attn): FlashAttention()
        (q_norm): DropoutAddRMSNorm()
        (k_norm): DropoutAddRMSNorm()
      )
      (ls1): LayerScale()
      (drop_path1): DropPath()
      (norm2): DropoutAddRMSNorm()
      (mlp): FusedMLP(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
      )
      (ls2): LayerScale()
      (drop_path2): DropPath()
    )
  )
  (clip_projector): AttentionPoolingBlock(
    (norm1_q): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (norm1_k): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (norm1_v): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (cross_attn): CrossAttention(
      (q): Linear(in_features=384, out_features=384, bias=False)
      (k): Linear(in_features=384, out_features=384, bias=False)
      (v): Linear(in_features=384, out_features=384, bias=False)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=384, out_features=768, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (drop_path): Identity()
  )
  (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=768, out_features=2, bias=True)
)
number of params: 23833346
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'pos_embed_cls', 'pos_embed_spatial', 'cls_token', 'pos_embed_temporal', 'pos_embed'}
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "pos_embed",
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.attn.proj.bias",
      "blocks.0.attn.q_norm.weight",
      "blocks.0.attn.k_norm.weight",
      "blocks.0.ls1.gamma",
      "blocks.0.norm2.weight",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias",
      "blocks.0.ls2.gamma"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.attn.proj.bias",
      "blocks.1.attn.q_norm.weight",
      "blocks.1.attn.k_norm.weight",
      "blocks.1.ls1.gamma",
      "blocks.1.norm2.weight",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias",
      "blocks.1.ls2.gamma"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.attn.proj.bias",
      "blocks.2.attn.q_norm.weight",
      "blocks.2.attn.k_norm.weight",
      "blocks.2.ls1.gamma",
      "blocks.2.norm2.weight",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias",
      "blocks.2.ls2.gamma"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.attn.proj.bias",
      "blocks.3.attn.q_norm.weight",
      "blocks.3.attn.k_norm.weight",
      "blocks.3.ls1.gamma",
      "blocks.3.norm2.weight",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias",
      "blocks.3.ls2.gamma"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.attn.proj.bias",
      "blocks.4.attn.q_norm.weight",
      "blocks.4.attn.k_norm.weight",
      "blocks.4.ls1.gamma",
      "blocks.4.norm2.weight",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias",
      "blocks.4.ls2.gamma"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.attn.proj.bias",
      "blocks.5.attn.q_norm.weight",
      "blocks.5.attn.k_norm.weight",
      "blocks.5.ls1.gamma",
      "blocks.5.norm2.weight",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias",
      "blocks.5.ls2.gamma"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.attn.proj.bias",
      "blocks.6.attn.q_norm.weight",
      "blocks.6.attn.k_norm.weight",
      "blocks.6.ls1.gamma",
      "blocks.6.norm2.weight",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias",
      "blocks.6.ls2.gamma"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.attn.proj.bias",
      "blocks.7.attn.q_norm.weight",
      "blocks.7.attn.k_norm.weight",
      "blocks.7.ls1.gamma",
      "blocks.7.norm2.weight",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias",
      "blocks.7.ls2.gamma"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.attn.proj.bias",
      "blocks.8.attn.q_norm.weight",
      "blocks.8.attn.k_norm.weight",
      "blocks.8.ls1.gamma",
      "blocks.8.norm2.weight",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias",
      "blocks.8.ls2.gamma"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.attn.proj.bias",
      "blocks.9.attn.q_norm.weight",
      "blocks.9.attn.k_norm.weight",
      "blocks.9.ls1.gamma",
      "blocks.9.norm2.weight",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias",
      "blocks.9.ls2.gamma"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.attn.proj.bias",
      "blocks.10.attn.q_norm.weight",
      "blocks.10.attn.k_norm.weight",
      "blocks.10.ls1.gamma",
      "blocks.10.norm2.weight",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias",
      "blocks.10.ls2.gamma"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.attn.proj.bias",
      "blocks.11.attn.q_norm.weight",
      "blocks.11.attn.k_norm.weight",
      "blocks.11.ls1.gamma",
      "blocks.11.norm2.weight",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias",
      "blocks.11.ls2.gamma"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "clip_projector.norm1_q.weight",
      "clip_projector.norm1_q.bias",
      "clip_projector.norm1_k.weight",
      "clip_projector.norm1_k.bias",
      "clip_projector.norm1_v.weight",
      "clip_projector.norm1_v.bias",
      "clip_projector.cross_attn.q_bias",
      "clip_projector.cross_attn.k_bias",
      "clip_projector.cross_attn.v_bias",
      "clip_projector.cross_attn.proj.bias",
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "clip_projector.cross_attn.q.weight",
      "clip_projector.cross_attn.k.weight",
      "clip_projector.cross_attn.v.weight",
      "clip_projector.cross_attn.proj.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-02-06 23:44:14,347] [INFO] [logging.py:129:log_dist] [Rank -1] DeepSpeed info: version=0.15.3, git-hash=unknown, git-branch=unknown
[2025-02-06 23:44:14,347] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-02-06 23:44:14,348] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1
[2025-02-06 23:44:14,477] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: True
Using /gpfs/home1/sorlova/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /gpfs/home1/sorlova/.cache/torch_extensions/py312_cu121/fused_adam/build.ninja...
/home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/2] /sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/deepspeed/ops/csrc/includes -I/home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/deepspeed/ops/csrc/adam -isystem /home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/torch/include -isystem /home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/torch/include/TH -isystem /home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/torch/include/THC -isystem /sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/include -isystem /home/sorlova/anaconda3/envs/video/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=compute_90 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[2/2] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/home/sorlova/anaconda3/envs/video/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/sw/arch/RHEL8/EB_production/2023/software/CUDA/12.1.1/lib64 -lcudart -o fused_adam.so
Loading extension module fused_adam...
Time to load fused_adam op: 24.946449995040894 seconds
[2025-02-06 23:44:39,430] [INFO] [logging.py:129:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-02-06 23:44:39,430] [INFO] [logging.py:129:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-02-06 23:44:39,433] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-02-06 23:44:39,433] [INFO] [logging.py:129:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-02-06 23:44:39,449] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed Final Optimizer = FP16_Optimizer
[2025-02-06 23:44:39,450] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-02-06 23:44:39,450] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-02-06 23:44:39,450] [INFO] [logging.py:129:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-02-06 23:44:39,450] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   amp_params ................... {'opt_level': 'O2'}
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x15024279dc70>
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": true, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": -1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   fp16_enabled ................. True
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-02-06 23:44:39,451] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   loss_scale ................... 0
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   optimizer_name ............... adam
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 0.0005, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   steps_per_print .............. 1
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   train_batch_size ............. 550
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  550
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... True
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   world_size ................... 1
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-02-06 23:44:39,452] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2025-02-06 23:44:39,452] [INFO] [config.py:989:print_user_config]   json = {
    "train_batch_size": 550, 
    "train_micro_batch_size_per_gpu": 550, 
    "steps_per_print": 1, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.0005, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": false
    }, 
    "amp": {
        "enabled": false, 
        "opt_level": "O2"
    }, 
    "flops_profiler": {
        "enabled": true, 
        "profile_step": -1, 
        "module_depth": -1, 
        "top_modules": 1, 
        "detailed": true
    }, 
    "zero_allow_untested_optimizer": true
}
model.gradient_accumulation_steps() = 1
criterion = CrossEntropyLoss()
Auto resume the latest checkpoint
No latest model
Auto resume the best checkpoint
No best model
No other models
Use bf16 False
Test:  [  0/222]  eta: 3:53:59  loss: 0.4700 (0.4700)  acc1: 77.4545 (77.4545)  time: 63.2429 (max: 63.2429)  data: 59.7004 (max: 59.7004)  max mem: 25989
Test:  [ 10/222]  eta: 0:26:36  loss: 0.5415 (0.5131)  acc1: 77.2727 (76.7107)  time: 7.5310 (max: 63.2429)  data: 5.9028 (max: 59.7004)  max mem: 25989
Test:  [ 20/222]  eta: 0:19:48  loss: 0.4260 (0.4787)  acc1: 77.8182 (78.1299)  time: 3.0173 (max: 27.8006)  data: 1.5799 (max: 26.3630)  max mem: 25989
Test:  [ 30/222]  eta: 0:17:03  loss: 0.4490 (0.4999)  acc1: 76.9091 (76.8504)  time: 4.1166 (max: 28.6354)  data: 2.6786 (max: 27.2001)  max mem: 25989
Test:  [ 40/222]  eta: 0:17:03  loss: 0.4893 (0.4993)  acc1: 74.5455 (76.9357)  time: 5.3455 (max: 28.6354)  data: 3.9073 (max: 27.2001)  max mem: 25989
Test:  [ 50/222]  eta: 0:15:08  loss: 0.4800 (0.5009)  acc1: 77.6364 (76.9911)  time: 5.2030 (max: 26.8679)  data: 3.7646 (max: 25.4313)  max mem: 25989
Test:  [ 60/222]  eta: 0:13:43  loss: 0.4915 (0.4998)  acc1: 77.2727 (77.0015)  time: 3.9805 (max: 26.3307)  data: 2.5421 (max: 24.8940)  max mem: 25989
Test:  [ 70/222]  eta: 0:12:32  loss: 0.5117 (0.5042)  acc1: 76.7273 (76.9808)  time: 4.1086 (max: 26.3307)  data: 2.6701 (max: 24.8940)  max mem: 25989
Test:  [ 80/222]  eta: 0:11:57  loss: 0.4956 (0.5023)  acc1: 78.3636 (77.1066)  time: 4.9469 (max: 20.6762)  data: 3.5082 (max: 19.2404)  max mem: 25990
Test:  [ 90/222]  eta: 0:10:55  loss: 0.4434 (0.4973)  acc1: 78.5455 (77.3367)  time: 5.0303 (max: 21.4793)  data: 3.5923 (max: 20.0428)  max mem: 25990
Test:  [100/222]  eta: 0:09:52  loss: 0.4653 (0.4992)  acc1: 77.6364 (77.2439)  time: 4.0750 (max: 21.4793)  data: 2.6374 (max: 20.0428)  max mem: 25990
Test:  [110/222]  eta: 0:09:01  loss: 0.4702 (0.4924)  acc1: 78.5455 (77.6003)  time: 4.2271 (max: 19.8149)  data: 2.7894 (max: 18.3800)  max mem: 25990
Test:  [120/222]  eta: 0:08:25  loss: 0.4634 (0.4895)  acc1: 80.1818 (77.8227)  time: 5.4663 (max: 24.2257)  data: 4.0286 (max: 22.7868)  max mem: 25990
Test:  [130/222]  eta: 0:07:29  loss: 0.4543 (0.4894)  acc1: 79.4545 (77.8321)  time: 5.1914 (max: 24.2257)  data: 3.7535 (max: 22.7868)  max mem: 25990
Test:  [140/222]  eta: 0:06:34  loss: 0.4543 (0.4878)  acc1: 78.5455 (77.8968)  time: 3.9148 (max: 22.3530)  data: 2.4770 (max: 20.9173)  max mem: 25990
Test:  [150/222]  eta: 0:05:46  loss: 0.4624 (0.4871)  acc1: 77.0909 (77.9109)  time: 4.3281 (max: 19.0269)  data: 2.8904 (max: 17.5884)  max mem: 25990
Test:  [160/222]  eta: 0:05:01  loss: 0.4792 (0.4882)  acc1: 76.0000 (77.8295)  time: 5.1955 (max: 18.7967)  data: 3.7580 (max: 17.3607)  max mem: 25990
Test:  [170/222]  eta: 0:04:11  loss: 0.5098 (0.4882)  acc1: 74.9091 (77.7416)  time: 5.0281 (max: 16.5087)  data: 3.5900 (max: 15.0732)  max mem: 25991
Test:  [180/222]  eta: 0:03:21  loss: 0.5332 (0.4951)  acc1: 74.9091 (77.4616)  time: 4.3225 (max: 12.7679)  data: 2.8844 (max: 11.3352)  max mem: 25991
Test:  [190/222]  eta: 0:02:33  loss: 0.5049 (0.4966)  acc1: 77.2727 (77.4698)  time: 4.5635 (max: 11.7842)  data: 3.1262 (max: 10.3452)  max mem: 25991
Test:  [200/222]  eta: 0:01:45  loss: 0.4609 (0.4958)  acc1: 78.9091 (77.4799)  time: 4.6922 (max: 11.4327)  data: 3.2551 (max: 9.9944)  max mem: 25991
Test:  [210/222]  eta: 0:00:57  loss: 0.4705 (0.4961)  acc1: 75.2727 (77.4054)  time: 4.8587 (max: 11.8449)  data: 3.4214 (max: 10.4102)  max mem: 25991
Test:  [220/222]  eta: 0:00:09  loss: 0.4558 (0.4954)  acc1: 76.9091 (77.4118)  time: 4.4860 (max: 12.6416)  data: 3.0483 (max: 11.2073)  max mem: 25991
Test:  [221/222]  eta: 0:00:04  loss: 0.4558 (0.4957)  acc1: 76.9091 (77.4019)  time: 4.1133 (max: 12.6416)  data: 2.7081 (max: 11.2073)  max mem: 25991
Test: Total time: 0:17:33 (4.7442 s / it)

===================================
mAP: 0.7856596112251282, auroc: 0.8364953994750977, acc: 0.7739939093589783
P@0.5: 0.726341962814331, R@0.5: 0.6576400399208069, F1@0.5: 0.6902858018875122
Confmat: 
	63537 | 11547 
	15955 | 30648
----------------------------
* Acc@1 77.402 loss 0.496
[rank0]:[W207 00:02:14.359241046 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

JOB STATISTICS
==============
Job ID: 9802467
Cluster: snellius
User/Group: sorlova/sorlova
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 03:43:55
CPU Efficiency: 73.59% of 05:04:16 core-walltime
Job Wall-clock time: 00:19:01
Memory Utilized: 120.24 GB
Memory Efficiency: 66.80% of 180.00 GB
